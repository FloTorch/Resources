{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5013b3d",
   "metadata": {},
   "source": [
    "# Integrating Flotorch Session Management with LangChain\n",
    "\n",
    "This notebook demonstrates how to integrate Flotorch's session management with a LangChain agent. By using `FlotorchLangChainSession` as the memory for the `AgentExecutor`, we can create a conversational agent that remembers the context of the current session.\n",
    "\n",
    "### Key Concepts:\n",
    "- **`FlotorchLangChainLLM`**: The language model powering the agent.\n",
    "- **`FlotorchLangChainSession`**: The session storage mechanism for persisting conversation history.\n",
    "- **LangChain Memory**: The `AgentExecutor` is configured to use the Flotorch session backend for conversational memory.\n",
    "- **`AgentExecutor` and `ChatPromptTemplate`**: The core LangChain components for building and orchestrating the AI workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af134a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "The following cells install the necessary packages, configure API credentials, and import all required components from the Flotorch and LangChain libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e45f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install flotorch langchain package\n",
    "%pip install  flotorch[langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa334c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOTORCH_API_KEY = \"<flotorch api key>\"\n",
    "FLOTORCH_BASE_URL = \"https://qa-gateway.flotorch.cloud\"\n",
    "FLOTORCH_MODEL = \"<flotorch model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain and related modules\n",
    "from flotorch.langchain.llm import FlotorchLangChainLLM\n",
    "from flotorch.langchain.session import FlotorchLangChainSession\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "print(\"Imported necessary libraries successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33d8a7",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Here, we initialize the `FlotorchLangChainLLM` with our model configuration. This will serve as the reasoning engine for our LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  =  FlotorchLangChainLLM(\n",
    "        model_id=FLOTORCH_MODEL,\n",
    "        api_key=FLOTORCH_API_KEY,\n",
    "        base_url=FLOTORCH_BASE_URL,\n",
    "    )\n",
    "print(\"FlotorchLLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27134aae",
   "metadata": {},
   "source": [
    "## 3. Session Storage Setup\n",
    "\n",
    "We configure Flotorch session storage and integrate it with LangChain's memory system. This enables the agent to maintain conversational context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98abcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_memory = FlotorchLangChainSession(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL\n",
    ")\n",
    "\n",
    "print(\"short_term_memory initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeec9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze text and provide comprehensive statistics.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted text analysis with statistics\n",
    "    \"\"\"\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    char_count_no_spaces = len(text.replace(' ', ''))\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    \n",
    "    return f\"\"\"Text Analysis:\n",
    "- Words: {word_count}\n",
    "- Characters (with spaces): {char_count}\n",
    "- Characters (without spaces): {char_count_no_spaces}\n",
    "- Sentences: {sentences}\"\"\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city. Available cities: New York, London, Tokyo, Paris.\"\"\"\n",
    "    city = city.lower().strip()\n",
    "\n",
    "    weather_info = {\n",
    "        \"new york\": \"Sunny, 72°F, Humidity: 45%\",\n",
    "        \"london\": \"Cloudy, 15°C, Humidity: 70%\",\n",
    "        \"tokyo\": \"Rainy, 25°C, Humidity: 80%\",\n",
    "        \"paris\": \"Partly cloudy, 18°C, Humidity: 60%\"\n",
    "    }\n",
    "\n",
    "    if city in weather_info:\n",
    "        return f\"Weather in {city.title()}: {weather_info[city]}\"\n",
    "    else:\n",
    "        return f\"I don't have weather data for '{city}'. Try: New York, London, Tokyo, or Paris.\"\n",
    "\n",
    "tools = [analyze_text, weather]\n",
    "\n",
    "print(\"Custom tool defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce0773",
   "metadata": {},
   "source": [
    "## 4. Agent and Prompt Configuration\n",
    "\n",
    "Next, we define the agent's logic using a `ChatPromptTemplate`. The prompt defines the agent's persona and includes a placeholder for `{history}` to inject the conversational context from our session memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Agent\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant with access to tools for calculations, weather, and information search.\n",
    "    Use the appropriate tool when needed, or answer directly if you can.\n",
    "    Keep your responses clear and concise.\n",
    "    session history: {history}\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\",\"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "agent = create_openai_functions_agent(model,tools,prompt)\n",
    "\n",
    "print(f\"Agent is created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf2d9f",
   "metadata": {},
   "source": [
    "## 5. Agent Executor Assembly\n",
    "\n",
    "We create an `AgentExecutor` to run the agent. Crucially, we pass the `session_memory` object to the `memory` parameter of the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=session_memory,\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95882b4",
   "metadata": {},
   "source": [
    "## 6. Interactive Chat\n",
    "\n",
    "This loop starts an interactive chat session. The agent will use the Flotorch-backed session memory to recall context from previous messages within this conversation. Type 'exit' to end the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe68212",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    user_query = input(\"user: \")\n",
    "    \n",
    "    if user_query.lower().strip() == \"exit\":\n",
    "        break\n",
    "    print(f\"User: {user_query}\")\n",
    "    response = agent_executor.invoke({\"input\":user_query})\n",
    "    print(f\"Assistant: {response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9413a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully demonstrated how to create a conversational agent with short-term memory.  \n",
    "By integrating **Flotorch's session management** with **LangChain**, we achieved a **stateful interaction** where the agent can recall information from earlier in the conversation.\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "- **Session Persistence** \n",
    "  Configured `FlotorchLangChainSession` as the memory component for the `AgentExecutor`, enabling the agent to maintain context across turns.\n",
    "\n",
    "- **Context-Aware Responses** \n",
    "  The agent successfully referenced previously mentioned details (e.g., the user's name) to provide more natural and relevant replies.\n",
    "\n",
    "- **Streamlined Integration** \n",
    "  Demonstrated a clear and effective method to connect Flotorch's robust session infrastructure with LangChain’s powerful agent framework.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
