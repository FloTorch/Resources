{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lbil2FwHB3Fcv8CL5F2sSi2nxXk_toQ5#scrollTo=HPrAc0uaVSrx)\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "This cell uses `pip` to install the necessary Python packages for the project.\n",
        "* `flotorch[adk]`: Installs the Flotorch Agent Development Kit (ADK), which provides the core framework for building and running the agent.\n",
        "* `PyPDF2`: A library used to read and extract text from PDF files.\n",
        "* `sentence-transformers`: A library used to generate vector embeddings (numerical representations) of text. This is essential for semantic search.\n",
        "* `faiss-cpu`: Facebook AI Similarity Search (FAISS) library. It's used to create an efficient, local vector index for storing and searching the text embeddings.\n",
        "* `requests`: A library for making HTTP requests, used here to download the PDF from a URL.\n",
        "* `numpy`: A fundamental package for numerical operations in Python, required by FAISS and sentence-transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGPTd0FtGfyd"
      },
      "outputs": [],
      "source": [
        "%pip install flotorch[adk]\n",
        "%pip install PyPDF2 sentence-transformers faiss-cpu requests numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-2"
      },
      "source": [
        "### Step 2: Configuration Variables\n",
        "\n",
        "This cell defines the constant variables required to configure and authenticate the Flotorch agent.\n",
        "\n",
        "* `FLOTORCH_API_KEY`: Replace the API key with your actual FloTorch API Key (Check [Prerequisites.ipynb](https://github.com/FloTorch/Resources/blob/main/examples/flotorch-rag-notebooks/faiss-example/01_Prerequisites.ipynb) on how to get the key)\n",
        "\n",
        "                        Using FloTorch helps you monitor the agentic flow through the FloTorch gateway.\n",
        "* `FLOTORCH_BASE_URL`: The API gateway endpoint.\n",
        "* `AGENT_NAME`: A unique name for this specific agent instance, indicating it uses MCP (Multi-turn Conversation Protocol) tools.\n",
        "* `APP_NAME`: A name for your application, used to group sessions.\n",
        "* `USER_ID`: A unique identifier for the end-user interacting with the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v48AGvIfSW16"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "FLOTORCH_API_KEY = getpass(\"FloTorch API Key\") # SET YOUR FLOTORCH API KEY HERE\n",
        "FLOTORCH_BASE_URL = \"https://gateway.flotorch.cloud\"\n",
        "AGENT_NAME = \"rag-agent\"\n",
        "APP_NAME = \"flotorch_tools_example\"\n",
        "USER_ID = \"flotorch_user_001\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-3"
      },
      "source": [
        "## Step 3: Import Core ADK Libraries\n",
        "\n",
        "This cell imports the primary classes needed from the Flotorch and Google ADK libraries to construct the agent.\n",
        "* `FlotorchADKAgent`: The main class for creating a Flotorch agent.\n",
        "* `FlotorchMemoryService`: (Imported but not used) A class for managing persistent memory.\n",
        "* `FlotorchADKSession`: The class used to manage conversation sessions.\n",
        "* `Runner`: The ADK class responsible for executing the agent's logic.\n",
        "* `types`: Google GenAI types, used for structuring messages (e.g., `Part`, `Content`).\n",
        "* `FunctionTool`: A class used to wrap a standard Python function so it can be used as a \"tool\" by the LLM agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ep7RxgcTBHo"
      },
      "outputs": [],
      "source": [
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.memory import FlotorchMemoryService\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "from google.adk import Runner\n",
        "from google.genai import types\n",
        "from google.adk.tools import FunctionTool\n",
        "\n",
        "print(\"Imported necessary libraries successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-4"
      },
      "source": [
        "## Step 4: Define the RAG Tool (Local Vector Store)\n",
        "\n",
        "This is the most complex cell and contains the core logic for the Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "1.  **Imports**: It imports necessary helper libraries (`os`, `json`, `tempfile`, `faiss`, etc.).\n",
        "2.  **Helper Functions**:\n",
        "    * `extract_text_from_pdf(file_path)`: Opens a PDF file, reads each page, and extracts the text.\n",
        "    * `split_text_into_sentences(text, max_len=300)`: A simple text chunker that splits the extracted text into smaller, manageable chunks (up to 300 characters).\n",
        "    * `create_faiss_index(sentences, embedder)`: Takes the text chunks, converts them into vector embeddings using the `SentenceTransformer` model, and stores them in a FAISS index.\n",
        "    * `vector_search(query, embedder, index, sentences, top_k=5)`: Takes a user query, embeds it, and searches the FAISS index for the top 5 most semantically similar text chunks.\n",
        "3.  **Main Tool Function**:\n",
        "    * `get_context_local(...)`: This is the main function that orchestrates the RAG process.\n",
        "    * It first checks if an index (`local_faiss.index`) already exists.\n",
        "    * If `force_rebuild=True` or no index exists and a PDF is provided (via `pdf_url` or `pdf_path`), it downloads/reads the PDF, extracts text, chunks it, and creates/saves a new FAISS index and a `sentences.json` file.\n",
        "    * If an index *does* exist (or was just built), it loads the index and sentences from the local files.\n",
        "    * It then performs a `vector_search` using the user's `query`.\n",
        "    * Finally, it returns the combined text of the top search results.\n",
        "4.  **Tool Creation**:\n",
        "    * `local_vector_tool = FunctionTool(func=get_context_local)`: This line wraps the entire `get_context_local` function into a `FunctionTool` object. This makes the function \"callable\" by the Flotorch agent, allowing the LLM to decide when to use it to retrieve context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1DEmp7-xcGD"
      },
      "outputs": [],
      "source": [
        "from google.adk.tools import FunctionTool\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import requests\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PyPDF2 import PdfReader\n",
        "from typing import Optional\n",
        "\n",
        "# PDF text extraction\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        reader = PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "# Split text into sentences/chunks\n",
        "def split_text_into_sentences(text, max_len=300):\n",
        "    import re\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_len:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Create FAISS index\n",
        "def create_faiss_index(sentences, embedder):\n",
        "    embeddings = embedder.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings\n",
        "\n",
        "# Vector search\n",
        "def vector_search(query, embedder, index, sentences, top_k=5):\n",
        "    query_vec = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    scores, indices = index.search(query_vec, top_k)\n",
        "    results = []\n",
        "    for i, score in zip(indices[0], scores[0]):\n",
        "        results.append({\"text\": sentences[i], \"score\": float(score)})\n",
        "    return results\n",
        "\n",
        "# Main function\n",
        "def get_context_local(query: str, pdf_url: str = \"\", pdf_path: str = \"\", force_rebuild: bool = False) -> str:\n",
        "    INDEX_PATH = \"local_faiss.index\"\n",
        "    SENTENCES_PATH = \"sentences.json\"\n",
        "    MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "    embedder = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "    index_exists = os.path.exists(INDEX_PATH) and os.path.exists(SENTENCES_PATH)\n",
        "\n",
        "    # Rebuild index if force_rebuild is True OR if PDF provided and index does not exist\n",
        "    if force_rebuild or ((pdf_path.strip() or pdf_url.strip()) and not index_exists):\n",
        "        if pdf_path.strip() and os.path.exists(pdf_path):\n",
        "            print(f\"üìÑ Using local PDF: {pdf_path}\")\n",
        "            text_content = extract_text_from_pdf(pdf_path)\n",
        "        elif pdf_url.strip():\n",
        "            print(f\"üåê Downloading PDF from: {pdf_url}\")\n",
        "            temp_pdf = tempfile.mktemp(\".pdf\")\n",
        "            open(temp_pdf, \"wb\").write(requests.get(pdf_url).content)\n",
        "            text_content = extract_text_from_pdf(temp_pdf)\n",
        "            os.remove(temp_pdf)\n",
        "        else:\n",
        "            return \"‚ùå Invalid PDF path or URL provided.\"\n",
        "\n",
        "        if not text_content.strip():\n",
        "            return \"‚ùå No text extracted from PDF.\"\n",
        "\n",
        "        sentences = split_text_into_sentences(text_content)\n",
        "        index, embeddings = create_faiss_index(sentences, embedder)\n",
        "\n",
        "        faiss.write_index(index, INDEX_PATH)\n",
        "        with open(SENTENCES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(sentences, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"‚úÖ Index created successfully.\")\n",
        "\n",
        "    # Load existing index\n",
        "    if not os.path.exists(INDEX_PATH) or not os.path.exists(SENTENCES_PATH):\n",
        "        return \"‚ùå No local index found. Provide a PDF first (local or URL).\"\n",
        "\n",
        "    index = faiss.read_index(INDEX_PATH)\n",
        "    with open(SENTENCES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentences = json.load(f)\n",
        "\n",
        "    results = vector_search(query, embedder, index, sentences, top_k=5)\n",
        "    if not results:\n",
        "        return \"‚ö†Ô∏è No relevant results found.\"\n",
        "\n",
        "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return \"\\n\\n\".join([r[\"text\"] for r in results])\n",
        "\n",
        "# Wrap as ADK tool\n",
        "local_vector_tool = FunctionTool(func=get_context_local)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-5"
      },
      "source": [
        "## Step 5:  Initialize Session Service\n",
        "\n",
        "This cell initializes the `FlotorchADKSession` service. This service is responsible for handling the creation and retrieval of conversation sessions, using the API key and base URL defined in Cell 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKZWF4MPU1Y2"
      },
      "outputs": [],
      "source": [
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY, base_url=FLOTORCH_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"Initialized Memory and Session\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-6"
      },
      "source": [
        "## Step 6:  Initialize the Flotorch Agent\n",
        "\n",
        "This cell creates the agent instance.\n",
        "* It initializes `FlotorchADKAgent`, passing the agent's name, API key, and base URL.\n",
        "* Crucially, it passes `custom_tools=[local_vector_tool]`. This tells the agent that it has access to the RAG function we defined in Cell 4. The agent's underlying LLM will be aware of this tool and can choose to call it when it needs to answer a question about a PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNiNUAdlVHPi"
      },
      "outputs": [],
      "source": [
        "flotorch_client = FlotorchADKAgent(\n",
        "    agent_name=AGENT_NAME,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    custom_tools=[local_vector_tool]\n",
        ")\n",
        "\n",
        "agent = flotorch_client.get_agent()\n",
        "print(f\"Advanced FlotorchADKAgent '{agent.name}' created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-7"
      },
      "source": [
        "## Step 7:  Configure the Agent Runner and Chat Function\n",
        "\n",
        "This cell sets up the components needed to run the agent and interact with it.\n",
        "1.  **Initialize `Runner`**: The `Runner` is initialized, binding the `agent` (from Cell 6) and the `session_service` (from Cell 5) together.\n",
        "2.  **Define `chat_with_agent`**: This `async` function is a helper to simplify sending messages to the agent.\n",
        "    * It takes a `query`, `session_id`, and optional `pdf_url` and `force_rebuild` flags.\n",
        "    * It constructs a message `content` for the agent.\n",
        "    * It calls `runner.run(...)`, which processes the message. This may involve multiple steps (e.g., LLM calls, tool calls).\n",
        "    * It iterates through the `events` generated by the runner and returns the text from the `final_response`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81jpWVVxVLsM"
      },
      "outputs": [],
      "source": [
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=APP_NAME,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "async def chat_with_agent(query, session_id, pdf_url=None, force_rebuild=False):\n",
        "    \"\"\"\n",
        "    Send query to agent; optionally provide PDF for indexing.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "        session_id (str): Session ID for the agent.\n",
        "        pdf_url (str, optional): PDF URL to build/update index.\n",
        "        force_rebuild (bool, optional): Force rebuilding the FAISS index.\n",
        "    \"\"\"\n",
        "    parts = [types.Part(text=query)]\n",
        "\n",
        "    if pdf_url:\n",
        "        # Include PDF URL and force rebuild flag in the message so the agent can rebuild index\n",
        "        parts.append(types.Part(text=json.dumps({\"pdf_url\": pdf_url, \"force_rebuild\": force_rebuild})))\n",
        "\n",
        "    content = types.Content(role=\"user\", parts=parts)\n",
        "\n",
        "    events = runner.run(user_id=USER_ID, session_id=session_id, new_message=content)\n",
        "    for event in events:\n",
        "        if event.is_final_response():\n",
        "            if event.content and event.content.parts:\n",
        "                return event.content.parts[0].text\n",
        "\n",
        "    return \"Sorry, I couldn't process that request.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-8"
      },
      "source": [
        "## Step 8:  Run Test: Index PDF and Ask First Question\n",
        "\n",
        "This cell executes the first end-to-end test of the RAG agent.\n",
        "1.  **Create Session**: It creates a new conversation session using the `session_service`.\n",
        "2.  **Build Index**: It calls `chat_with_agent` with the query \"Build index\", a `pdf_url`, and `force_rebuild=True`. This forces the agent to call the `get_context_local` tool, which downloads the sample invoice PDF, processes it, and saves the `local_faiss.index` and `sentences.json` files.\n",
        "3.  **Query Index**: It calls `chat_with_agent` again, this time with a real question: \"what is pdf mainly described about?\". The agent will receive this query, decide to use the `get_context_local` tool to search the index it just built, get the relevant context (chunks from the invoice), and then use that context to generate a summary.\n",
        "4.  **Print Answer**: The final answer is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7KKcdKlqVP2n"
      },
      "outputs": [],
      "source": [
        "# Create a session\n",
        "session = await runner.session_service.create_session(app_name=APP_NAME, user_id=USER_ID)\n",
        "\n",
        "# 1Ô∏è‚É£ Build index from PDF\n",
        "response1 = await chat_with_agent(\n",
        "    query=\"Build index\",\n",
        "    session_id=session.id,\n",
        "    pdf_url=\"https://www.princexml.com/samples/invoice-colorful/invoicesample.pdf\",\n",
        "    force_rebuild=True\n",
        ")\n",
        "print(\"resposnse------------------\",response1)\n",
        "\n",
        "# 2Ô∏è‚É£ Query local vector index\n",
        "question1=\"can you provide the overview of the pdf\"\n",
        "response1 = await chat_with_agent(\n",
        "    query=\"what is pdf mainly described about?\",\n",
        "    session_id=session.id\n",
        ")\n",
        "print(\"\\nQuery:\",question1)\n",
        "print(\"\\nAnswer:\",response1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-9"
      },
      "source": [
        "## Step 9: Run Test: Ask Second Question\n",
        "\n",
        "This cell runs a second query against the agent within the *same session*. It asks the same question again (\"what is pdf mainly described about?\"). This demonstrates that:\n",
        "1.  The local index persists, so the agent doesn't need to rebuild it.\n",
        "2.  The agent can successfully use the tool to query the existing index.\n",
        "3.  The agent maintains conversational context (though in this specific query, it's just repeating the action)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrAc0uaVSrx"
      },
      "outputs": [],
      "source": [
        "question2=\"what is pdf mainly described about?\"\n",
        "response2 = await chat_with_agent(\n",
        "    query=question2,\n",
        "    session_id=session.id\n",
        ")\n",
        "print(\"\\nQuery:\",question2)\n",
        "print(\"\\nAnswer:\",response2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
