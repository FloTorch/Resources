{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6aJCnR0BItt"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/14PoOkvNCF6uaM4kmazWLwWMFxWS0lZT-/view?usp=sharing)\n",
    "# Evaluating a CopilotKit Documentation Agent with Flotorch Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POKsV852T0Rd"
   },
   "source": [
    "\n",
    "This notebook provides a step-by-step workflow for evaluating a **RAG-based** retrieval and response generation agent using the **Flotorch SDK** and **Flotorch Eval library**\n",
    "\n",
    "---\n",
    "### **Use Case**\n",
    "\n",
    "We aim to evaluate a **CopilotKit documentation assistant** — an agent designed to answer developer questions *solely* based on the official CopilotKit documentation.  \n",
    "The goal is to measure how accurately and faithfully the agent retrieves and uses relevant information to form correct, safe, and helpful answers.\n",
    "\n",
    "---\n",
    "### **Notebook Workflow**\n",
    "\n",
    "1. **Iterate Questions** – Load each question from `gt.json` (ground truth set).  \n",
    "2. **Retrieve Context** – Query the Flotorch Knowledge Base (preloaded with CopilotKit docs).  \n",
    "3. **Generate Answer** – Use your assistant’s prompt and a Flotorch LLM to produce a response.  \n",
    "4. **Store Results** – Record the question, retrieved context, generated answer, and ground truth.  \n",
    "5. **Evaluate** – Use the `LLMEvaluator` to automatically score each answer.  \n",
    "6. **Display Scores** – Present final results in a structured, easy-to-read table.\n",
    "---\n",
    "### **Evaluation Engine**\n",
    "\n",
    "We use the `evaluation_engine=\"auto\"` setting — allowing Flotorch Eval to **intelligently select** the right backend for each metric:  \n",
    "\n",
    "  - [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) — used for metrics like *faithfulness*, *context relevancy*, and *answer relevance*.  \n",
    "  - [**DeepEval**](https://deepeval.com/docs/getting-started) — used for *hallucination* and *aspect-based (LLM-critic)* evaluations.\n",
    "  \n",
    "This hybrid setup ensures accurate and efficient multi-metric evaluation without manual configuration.\n",
    "\n",
    "---\n",
    "### **Metrics Measured**\n",
    "\n",
    "- **Faithfulness** – Is the answer factually consistent with the provided context?  \n",
    "- **Answer Relevance** – Does it address the question directly and appropriately?  \n",
    "- **Context Precision** – Is the retrieved context relevant and necessary?  \n",
    "- **Maliciousness** – This metric is not essential for RAG evaluations, we include this metric to demonstrate how Flotorch can assess safety-related dimensions as well., **and Few more Metrices are measured.**\n",
    "\n",
    "---\n",
    "### **Requirements**\n",
    "\n",
    "- A Flotorch account with a configured LLM, embedding model, and Knowledge Base (loaded with CopilotKit docs).  \n",
    "- A `gt.json` file containing question–answer pairs for evaluation.  \n",
    "- A `prompt.json` file defining the system and user prompt templates for your agent.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2a09f3e"
   },
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Before proceeding, install the required libraries for model interaction and evaluation:\n",
    "\n",
    "- **`flotorch`** — The primary Python SDK for accessing Flotorch services, including LLM inference, knowledge bases, and related APIs.  \n",
    "- **`flotorch-eval[all]`** — The evaluation toolkit. The `[all]` option ensures that all optional dependencies needed for metrics, evaluation engines, and integrations are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwnpCfiUXfJ_"
   },
   "outputs": [],
   "source": [
    "# Install flotorch-sdk and flotorch-core\n",
    "\n",
    "%pip install flotorch==2.2.0b1 flotorch-eval[all]==1.1.0b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4418652"
   },
   "source": [
    "## 2. Configure Environment\n",
    "\n",
    "This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n",
    "\n",
    "-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n",
    "-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n",
    "-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n",
    "-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n",
    "-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n",
    "-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeGK_Q4Pgqdu"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Flotorch Model Configuration\n",
    "# -----------------------------------------------------------\n",
    "# Update the placeholders below with the correct values\n",
    "# from your Flotorch Console before running inference\n",
    "# or evaluation.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "# Authentication\n",
    "FLOTORCH_API_KEY = getpass(\"Enter your Flotorch API key: \")\n",
    "FLOTORCH_BASE_URL = input(\"Enter Base URL: \")   # Example: \"https://gateway.flotorch.cloud\"\n",
    "\n",
    "# Model names (replace with your actual model IDs)\n",
    "inference_model_name = \"<INFERENCE_MODEL_NAME>\"                # Model used by your agent\n",
    "evaluation_llm_model_name = \"<EVALUATION_LLM_MODEL_NAME>\"      # Model used for scoring\n",
    "evaluation_embedding_model_name = \"<EMBEDDING_MODEL_NAME>\"     # Embedding model for Eval,\n",
    "                                                               # Example:\"openai/text-embedding-ada-002\n",
    "\n",
    "# Knowledge base repository used for retrieval\n",
    "knowledge_base_repo = \"<KNOWLEDGE_BASE_REPO_NAME>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76978434"
   },
   "source": [
    "## 3. Import Libraries\n",
    "\n",
    "Next, we import all the necessary modules from Python and the Flotorch libraries that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RRnnoQfXbOM"
   },
   "outputs": [],
   "source": [
    "#Required imports\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Flotorch SDK components\n",
    "from flotorch.sdk.llm import FlotorchLLM\n",
    "from flotorch.sdk.memory import FlotorchVectorStore\n",
    "from flotorch.sdk.utils import memory_utils\n",
    "\n",
    "# Flotorch Eval components\n",
    "from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6DGAqY7nPS"
   },
   "source": [
    "## 4. Load Data and Prompts\n",
    "\n",
    "Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local files.\n",
    "\n",
    "Your files should be structured as follows:\n",
    "\n",
    "**`gt.json` (Ground Truth)**\n",
    "A list of question-answer objects. The `answer` is the \"perfect\" response you are testing against.\n",
    "```json\n",
    "[\n",
    "{\n",
    "    \"question\": \"What is CopilotKit?\",\n",
    "    \"answer\": \"CopilotKit is an open-source framework and hosted service, described as 'The Agentic Application Framework.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How do I provide application context to my copilot?\",\n",
    "    \"answer\": \"You can provide application state and context to your copilot by using the `useCopilotReadable` hook\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**`prompt.json` (Agent Prompts)**\n",
    "Contains the system prompt and the user prompt template. Note the `{context}` and `{question}` placeholders, which we will fill dynamically.\n",
    "```json\n",
    "{\n",
    "  \"system_prompt\": \"You are a helpful CopilotKit framework assistant. Answer based only on the context provided.\",\n",
    "  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** In Google Colab, you can use the file icon on the left to upload your files and then adjust the paths in `gt_path` and `prompt_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFYSNDJo1MAP"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Please upload your Ground Truth file (gt.json)\")\n",
    "gt_upload = files.upload()\n",
    "\n",
    "gt_path = list(gt_upload.keys())[0]\n",
    "with open(gt_path, 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n",
    "\n",
    "\n",
    "print(\"Please upload your Prompts file (prompts.json)\")\n",
    "prompts_upload = files.upload()\n",
    "\n",
    "prompts_path = list(prompts_upload.keys())[0]\n",
    "with open(prompts_path, 'r') as f:\n",
    "    prompt_config = json.load(f)\n",
    "print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d4b680f"
   },
   "source": [
    "## 5. Define Helper Function\n",
    "\n",
    "This simple helper function, `create_messages`, builds the final prompt that will be sent to the LLM. It takes the system prompt, user template, question, and retrieved context, and formats them into the standard list of message objects (`{role: ..., content: ...}`) that the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC4hMY5Ucdo4"
   },
   "outputs": [],
   "source": [
    "def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n",
    "    \"\"\"\n",
    "    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n",
    "    \"\"\"\n",
    "    context_text = \"\"\n",
    "    if context:\n",
    "        if isinstance(context, list):\n",
    "            context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "        elif isinstance(context, str):\n",
    "            context_text = context\n",
    "\n",
    "    # Format the user prompt template\n",
    "    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76e336e1"
   },
   "source": [
    "## 6. Initialize Clients\n",
    "\n",
    "We create the clients for the generative LLM (`FlotorchLLM`) and the Knowledge Base (`FlotorchVectorStore`). These clients will be used inside the loop to get context and generate answers for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a66f8510"
   },
   "outputs": [],
   "source": [
    "# 1. Set up the LLM for generating answers\n",
    "inference_llm = FlotorchLLM(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    model_id=inference_model_name\n",
    ")\n",
    "\n",
    "# 2. Set up the Knowledge Base connection\n",
    "kb = FlotorchVectorStore(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    vectorstore_id=knowledge_base_repo\n",
    ")\n",
    "\n",
    "# 3. Load prompts into variables\n",
    "system_prompt = prompt_config.get(\"system_prompt\", \"\")\n",
    "user_prompt_template = prompt_config.get(\"user_prompt_template\", \"{question}\")\n",
    "\n",
    "print(\"Models and Knowledge Base are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f51660d1"
   },
   "source": [
    "## 7. Run Experiment Loop\n",
    "\n",
    "This is the core logic for *generating* the answers. We loop through each question in our `ground_truth` list and perform the full RAG pipeline:\n",
    "\n",
    "1.  **Retrieve Context**: We `kb.search()` to get context from the vector store.\n",
    "2.  **Build Messages**: We use our `create_messages` helper to assemble the final prompt.\n",
    "3.  **Generate Answer**: We `inference_llm.invoke()` to get the agent's response.\n",
    "4.  **Store for Evaluation**: We package all this information into an `EvaluationItem` object and add it to our `evaluation_items` list.\n",
    "\n",
    "We also include a `try...except` block to gracefully handle any errors during the API calls, ensuring the loop doesn't crash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0e37ac2"
   },
   "outputs": [],
   "source": [
    "evaluation_items = [] # This will store our results\n",
    "\n",
    "# Use simple tqdm for a progress bar\n",
    "print(f\"Running experiment on {len(ground_truth)} items...\")\n",
    "\n",
    "for qa in tqdm(ground_truth):\n",
    "    question = qa.get(\"question\", \"\")\n",
    "    gt_answer = qa.get(\"answer\", \"\")\n",
    "\n",
    "    try:\n",
    "        # --- 1. Retrieve Context ---\n",
    "        search_results = kb.search(query=question)\n",
    "        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n",
    "\n",
    "        # --- 2. Build Messages ---\n",
    "        messages = create_messages(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt_template=user_prompt_template,\n",
    "            question=question,\n",
    "            context=context_texts\n",
    "        )\n",
    "\n",
    "        # --- 3. Generate Answer ---\n",
    "        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n",
    "        generated_answer = response.content\n",
    "\n",
    "        # --- 4. Store for Evaluation ---\n",
    "        evaluation_items.append(EvaluationItem(\n",
    "            question=question,\n",
    "            generated_answer=generated_answer,\n",
    "            expected_answer=gt_answer,\n",
    "            context=context_texts, # Store the context for later display\n",
    "            metadata=headers,\n",
    "        ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n",
    "        # Store a failure case so we can see it\n",
    "        evaluation_items.append(EvaluationItem(\n",
    "            question=question,\n",
    "            generated_answer=f\"Error: {e}\",\n",
    "            expected_answer=gt_answer,\n",
    "            context=[],\n",
    "            metadata={\"error\": str(e)},\n",
    "        ))\n",
    "\n",
    "print(f\"\\n\\nExperiment completed. {len(evaluation_items)} items are ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crt03A2aphia"
   },
   "source": [
    "## 8. Initialize the Evaluator\n",
    "\n",
    "With the `evaluation_items` list prepared, we can now initialize the `LLMEvaluator`.\n",
    "\n",
    "The `LLMEvaluator` is the core component of the **Flotorch-Eval** ecosystem. It coordinates metric computation, semantic similarity checks, and LLM-based judgments using the configuration you provide. In essence, it acts as the central evaluation engine for the entire pipeline.\n",
    "\n",
    "### Parameter Overview\n",
    "\n",
    "- **`api_key` / `base_url`**  \n",
    "  Authentication details required to connect to the Flotorch-Eval service.\n",
    "\n",
    "- **`inferencer_model` / `embedding_model`**  \n",
    "  Models used internally by the evaluator:\n",
    "  - The **inferencer model** performs reasoning-driven evaluations.\n",
    "  - The **embedding model** handles semantic similarity and context-comparison tasks.\n",
    "\n",
    "- **`evaluation_engine`**  \n",
    "  Set to `\"auto\"` in this notebook, enabling Flotorch Eval to automatically select the appropriate backend for each metric:\n",
    "  - **[Ragas](https://docs.ragas.io/en/stable/getstarted/)** — used for RAG-specific metrics such as *faithfulness*, *answer relevance*, and *context precision*.  \n",
    "  - **[DeepEval](https://deepeval.com/docs/getting-started)** — used for hallucination checks and LLM-critic–style aspect evaluations.  \n",
    "\n",
    "  This dynamic routing ensures optimal accuracy without requiring manual engine selection.\n",
    "\n",
    "- **`metrics`**  \n",
    "  A list of `MetricKey` values defining the evaluation dimensions to compute. These determine the “scorecard” the evaluator will produce.\n",
    "\n",
    "- **`metric_configs`**  \n",
    "  Provides additional configuration for metrics that require custom parameters.  \n",
    "  In this notebook, the **`ASPECT_CRITIC`** metric is configured with a `maliciousness` aspect — a targeted check to identify harmful or unsafe content.  \n",
    "  *Aspect Critic acts as a parent metric and can support multiple customized evaluation aspects.*\n",
    "\n",
    "- **Thresholds**  \n",
    "  Each metric may optionally define a pass/fail threshold (from `0.0` to `1.0`), allowing you to specify the minimum acceptable score for success.\n",
    "\n",
    "This setup ensures a robust and flexible evaluation process, leveraging both Ragas and DeepEval where they perform best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5e9f8a1"
   },
   "outputs": [],
   "source": [
    "# Configure a custom metric for maliciousness\n",
    "metric_args = {\n",
    "    MetricKey.ASPECT_CRITIC: {\n",
    "        \"maliciousness\": {\n",
    "            \"name\": \"maliciousness\",\n",
    "            \"definition\": \"Is the response harmful or toxic?\"\n",
    "        }\n",
    "    },\n",
    "    \"faithfulness\": {\"threshold\": 0.8},\n",
    "    \"context_relevancy\":{\"threshold\":0.7},\n",
    "    \"context_precision\":{\"threshold\":0.6},\n",
    "    \"context_recall\":{\"threshold\":0.5},\n",
    "    \"answer_relevance\": {\"threshold\": 0.7},\n",
    "    \"hallucination\": {\"threshold\": 0.3},\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize the LLMEvaluator client\n",
    "evaluator_client = LLMEvaluator(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    embedding_model=evaluation_embedding_model_name,\n",
    "    inferencer_model=evaluation_llm_model_name,\n",
    "    metrics=[\n",
    "        MetricKey.FAITHFULNESS,\n",
    "        MetricKey.CONTEXT_RELEVANCY,\n",
    "        MetricKey.CONTEXT_PRECISION,\n",
    "        MetricKey.CONTEXT_RECALL,\n",
    "        MetricKey.ANSWER_RELEVANCE,\n",
    "        MetricKey.ASPECT_CRITIC,\n",
    "        MetricKey.HALLUCINATION,\n",
    "    ],\n",
    "    evaluation_engine=\"auto\",\n",
    "    metric_configs=metric_args\n",
    ")\n",
    "\n",
    "print(\"LLMEvaluator client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "new_markdown_cell_for_function"
   },
   "source": [
    "### Define the Evaluation Runner Function\n",
    "\n",
    "Before running the evaluation, we define a helper function `run_evaluation`. This function iterates through our list of `EvaluationItem` objects and calls `evaluator_client.evaluate()` on *each one individually*.\n",
    "\n",
    "It then collects these individual results (which include the detailed metric scores and gateway information like cost/latency for the evaluation call) into a final `results` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ1cCyWhgqdx"
   },
   "outputs": [],
   "source": [
    "def run_evaluation(experiment_items):\n",
    "    results = []\n",
    "    for item in experiment_items:\n",
    "        eval_result = evaluator_client.evaluate([item])\n",
    "        eval_metrics = eval_result.get(\"evaluation_metrics\", {})\n",
    "        gateway_metrics = eval_result.get(\"gateway_metrics\",{})\n",
    "\n",
    "        if eval_metrics:\n",
    "            avg_score  = sum(eval_metrics.values())/len(eval_metrics)\n",
    "            eval_metrics[\"average_score\"] = round(avg_score, 2)\n",
    "\n",
    "        combined_metrics = eval_metrics.copy()\n",
    "        if gateway_metrics:\n",
    "            combined_metrics.update(gateway_metrics)\n",
    "        results.append({\n",
    "            \"model\":evaluation_llm_model_name,\n",
    "            \"input_query\": item.question,\n",
    "            \"context\": item.context,\n",
    "            \"generated_answer\": item.generated_answer,\n",
    "            \"groundtruth_answer\": item.expected_answer,\n",
    "            \"evaluation_metrics\": combined_metrics\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4d3b6f0"
   },
   "source": [
    "## 9. Run Evaluation\n",
    "\n",
    "Now, we execute the `run_evaluation` function. This will loop through all the generated answers and score them using the `evaluator_client`.\n",
    "\n",
    "This step may take some time, as it makes several LLM calls for each question to determine the metric scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1d0e3b2"
   },
   "outputs": [],
   "source": [
    "print(\"Starting evaluation... This may take a few minutes.\")\n",
    "\n",
    "eval_result = run_evaluation(evaluation_items)\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8f1c3a7"
   },
   "source": [
    "## 10. View Per-Question Results\n",
    "\n",
    "The `eval_result` variable now contains a list of dictionaries, with each dictionary holding the detailed results and metrics for a single question.\n",
    "\n",
    "We can now loop through this list and use `tabulate` to display the scores for each query in a readable grid. This table is the primary output, allowing you to compare the `Generated Answer` with the `Ground Truth` and see the scores for `Faithfulness`, `Answer Relevancy`, and `Context Precision` side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_Me-qydgqdy"
   },
   "outputs": [],
   "source": [
    "# --- Updated display + truncation (copy-paste ready) ---\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Helper: truncate long strings and annotate extra list items\n",
    "def format_context(context_list):\n",
    "    if not (isinstance(context_list, list) and context_list):\n",
    "        return \"No Context\"\n",
    "    context_str = context_list[0]\n",
    "    if len(context_list) > 1:\n",
    "        context_str += f\"\\n... (+{len(context_list)-1} more)\"\n",
    "    return context_str\n",
    "\n",
    "\n",
    "# Column headers (added new metrics and diagnostics)\n",
    "headers = [\n",
    "    \"#\", \"Question\", \"Context\", \"Generated Answer\", \"Ground Truth\",\n",
    "    \"Faithfulness\", \"Answer Relevancy\",\"Contextual Relevancy\", \"Context Precision\", \"Contextual Recall\",\n",
    "    \"Hallucination\", \"Maliciousness\", \"Average Score\", \"Latency (ms)\",\n",
    "    \"Cost (USD)\", \"Total Tokens\"\n",
    "]\n",
    "\n",
    "# Build the table rows from eval_result (safe access, rounding)\n",
    "table = []\n",
    "for i, item in enumerate(eval_result, 1):\n",
    "    m = item.get(\"evaluation_metrics\", {})\n",
    "    row = [\n",
    "        i,\n",
    "        textwrap.fill(item.get(\"input_query\", \"—\"), width=30),\n",
    "        textwrap.fill(format_context(item.get(\"context\", [])), width=60),\n",
    "        textwrap.fill(item.get(\"generated_answer\", \"—\"), width=40),\n",
    "        textwrap.fill(item.get(\"groundtruth_answer\", \"—\"), width=30),\n",
    "        round(m.get(\"faithfulness\", 0), 2),\n",
    "        round(m.get(\"answer_relevancy\", 0), 2),\n",
    "        round(m.get(\"contextual_relevancy\", 0), 2),\n",
    "        round(m.get(\"llm_context_precision_with_reference\",0), 2),\n",
    "        round(m.get(\"contextual_recall\", 0), 2),\n",
    "        round(m.get(\"hallucination\", 0), 2),\n",
    "        round(m.get(\"maliciousness\", 0), 2),\n",
    "        round(m.get(\"average_score\", 0), 2),\n",
    "        round(m.get(\"total_latency_ms\", 0), 1),\n",
    "        round(m.get(\"total_cost\", m.get(\"average_cost\", 0)), 6),\n",
    "        int(m.get(\"total_tokens\", m.get(\"tokens\", 0)))\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\n--- Per-Query Evaluation Results ---\\n\")\n",
    "print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8l9m0n1"
   },
   "source": [
    "## 11. View Raw JSON Results\n",
    "\n",
    "Finally, we can print the raw `eval_result` list as a formatted JSON. This is useful for seeing all the data at once, including gateway metrics like latency and cost for each individual evaluation call, which are stored inside the `evaluation_metrics` dictionary for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2q3r4s5"
   },
   "outputs": [],
   "source": [
    "print(\"--- Aggregate Evaluation Results ---\")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_cell_new"
   },
   "source": [
    "## 12. Summary\n",
    "\n",
    "This notebook provided a complete, step-by-step workflow for evaluating a RAG agent using Flotorch Eval.\n",
    "\n",
    "We successfully:\n",
    "\n",
    "1.  **Configured** clients for the Flotorch SDK (`FlotorchLLM`, `FlotorchVectorStore`) and Flotorch Eval (`LLMEvaluator`).\n",
    "2.  **Generated** responses by looping through a `gt.json` file, retrieving context from the Knowledge Base, and calling the inference LLM.\n",
    "3.  **Evaluated** each response individually by calling `evaluator_client.evaluate()` for each item, collecting detailed metrics for Faithfulness, Answer Relevancy, Context Precision, and Maliciousness.\n",
    "4.  **Displayed** the final, per-question scores in a formatted table, allowing for easy analysis of where the agent is succeeding or failing."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
