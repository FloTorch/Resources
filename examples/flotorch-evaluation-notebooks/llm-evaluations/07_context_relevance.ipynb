{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XeVVdFRcad0"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/15pHnnmV8qysz44jK2vl2E1MOz7OqeUj9?usp=sharing)\n",
    "\n",
    "# Evaluating the Corporate Travel Policy Assistant with Flotorch Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POKsV852T0Rd"
   },
   "source": [
    "This notebook provides a step-by-step guide to **evaluate a question-answering agent (RAG)** using the **Flotorch SDK** and **Flotorch Eval** library.  \n",
    "The use case here is a **Corporate Travel Policy Advisor** — an LLM-powered assistant that answers questions about the \"**Corporate Travel Policy – Comprehensive Guide**\" spanning authorization workflows, booking rules, class-of-service limits, reimbursement guardrails, and safety expectations for travelers.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Use Case Overview**\n",
    "\n",
    "The **Corporate Travel Policy Advisor** helps employees, finance approvers, and travel coordinators resolve questions about:\n",
    "- **Pre-Trip Governance** (authorization lead times, booking through approved agencies, itinerary sharing, exception routing)\n",
    "- **Air Travel Standards** (economy vs. premium eligibility, advance purchase rules, upgrade policies, baggage limits)\n",
    "- **Ground Transportation** (ride-share vs. rental guidance, parking reimbursement, personal vehicle mileage, insurance expectations)\n",
    "- **Lodging and Extended Stays** (hotel class limits, use of corporate rates, apartment-style housing for long trips)\n",
    "- **Meals, Per Diem, and Incidentals** (GSA-based allowances, receipt thresholds, non-reimbursable items)\n",
    "- **International and Safety Requirements** (visa/immigration prep, travel insurance, emergency contacts, high-risk region approvals)\n",
    "- **Expense Reporting & Compliance** (documentation standards, timelines, exception tracking, CFO approvals)\n",
    "\n",
    "Relevant passages are retrieved from the **Corporate Travel Knowledge Base**, ensuring that every answer maps back to vetted policy language before advising travelers or approvers.\n",
    "\n",
    "This notebook focuses on evaluating **retrieval quality** using the **DeepEval Contextual Relevancy metric** — verifying that the assistant consistently surfaces policy excerpts that stay on-topic for each travel scenario before it responds.\n",
    "\n",
    "---\n",
    "\n",
    "### **Notebook Workflow**\n",
    "\n",
    "We'll follow a structured evaluation process:\n",
    "\n",
    "1. **Iterate Questions** – Loop through each travel-policy scenario in the ground-truth set.  \n",
    "2. **Retrieve Context** – Fetch the relevant policy sections from the Corporate Travel Knowledge Base.  \n",
    "3. **Generate Answer** – Use the system prompt and LLM to craft a compliant travel-policy response.  \n",
    "4. **Store Results** – Log each question, retrieved context, generated answer, and reference answer.  \n",
    "5. **Evaluate Contextual Relevancy** – Use `LLMEvaluator` to run the DeepEval Contextual Relevancy check.  \n",
    "6. **Display Results** – Summarize relevancy outcomes in an at-a-glance table.\n",
    "\n",
    "---\n",
    "\n",
    "### **Metric Evaluated — Contextual Relevancy**\n",
    "\n",
    "We track **Contextual Relevancy** to ensure the assistant only leans on passages that directly support the traveler’s question. Scores approach 1.0 when retrieved snippets address the same policy elements as the question and generated answer; scores fall toward 0 when unrelated or extraneous excerpts are surfaced.\n",
    "\n",
    "#### DeepEval Contextual Relevancy (Flotorch `evaluation_engine=\"deepeval\"`)\n",
    "- Uses an LLM-as-a-judge to determine how well each snippet in `retrieval_context` aligns with the `input` question and `actual_output`, producing a reasoned verdict per statement.  \n",
    "- Requires `input`, `actual_output`, and `retrieval_context` fields so the evaluator can judge topical alignment, as outlined in the [DeepEval Contextual Relevancy docs](https://deepeval.com/docs/metrics-contextual-relevancy).  \n",
    "- Highlights noisy retrievals so teams can refine knowledge-base chunking, search parameters, or prompt instructions before policy answers reach employees.\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Engine**\n",
    "\n",
    "- `evaluation_engine=\"auto\"` — lets Flotorch Eval mix Ragas and DeepEval according to the priority routing described in the [flotorch-eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop), ensuring answer relevance and context metrics always run on the best available backend.  \n",
    "- `evaluation_engine=\"deepeval\"` — routes metrics through DeepEval’s engine (answer relevancy, context relevancy, context precision, context recall, hallucination, faithfulness) while still capturing Flotorch gateway telemetry. This mode is showcased later in the notebook.\n",
    "\n",
    "In this notebook we rely on the DeepEval pathway to ensure travel guidance cites the appropriate booking procedures, reimbursement limits, and safety protocols from the corporate policy guide.\n",
    "\n",
    "---\n",
    "\n",
    "### **Requirements**\n",
    "\n",
    "- Flotorch account with configured LLM, embedding model, and Corporate Travel Knowledge Base.  \n",
    "- `travel_policy_gt.json` (or similar) containing corporate travel policy Q&A pairs for evaluation.  \n",
    "- `travel_policy_prompt.json` containing the system and user prompt templates tailored to travel coordinators.  \n",
    "\n",
    "---\n",
    "#### **Documentation References**\n",
    "- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) — reference implementation with sample notebooks and evaluation pipelines.  \n",
    "- [**DeepEval Contextual Relevancy Documentation**](https://deepeval.com/docs/metrics-contextual-relevancy) — detailed explanation of the contextual relevancy metric and configuration options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2a09f3e"
   },
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n",
    "\n",
    "-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n",
    "-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwnpCfiUXfJ_"
   },
   "outputs": [],
   "source": [
    "# Install flotorch-sdk and flotorch-core\n",
    "# You can safely ignore dependency errors during installation.\n",
    "\n",
    "%pip install flotorch==2.2.0b1 flotorch-eval[llm]==1.1.0b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4418652"
   },
   "source": [
    "## 2. Configure Environment\n",
    "\n",
    "This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n",
    "\n",
    "-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n",
    "-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n",
    "-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n",
    "-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n",
    "-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n",
    "-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n",
    "\n",
    "### Example :\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n",
    "| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n",
    "| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n",
    "| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n",
    "| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n",
    "| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `digital-twin` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVfI3usASn2u"
   },
   "outputs": [],
   "source": [
    "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
    "\n",
    "# Prefect-side authentication for Flotorch access\n",
    "try:\n",
    "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n",
    "    print(f\"Success\")\n",
    "except getpass.GetPassWarning as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    FLOTORCH_API_KEY = \"\"\n",
    "\n",
    "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n",
    "\n",
    "inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n",
    "evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n",
    "evaluation_embedding_model_name = \"<provider>/<embedding_model_name>\"  # Embedding model for similarity checks\n",
    "\n",
    "knowledge_base_repo = \"<your_knowledge_base_id>\" #Knowledge_base ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76978434"
   },
   "source": [
    "## 3. Import Required Libraries\n",
    "\n",
    "### Purpose\n",
    "Import all required components for evaluating the RAG assistant.\n",
    "\n",
    "### Key Components\n",
    "- `json` : Loads configuration files and ground truth data from disk\n",
    "- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n",
    "- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n",
    "- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n",
    "- `memory_utils` : Utility helpers for extracting text from vector-store search results\n",
    "- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RRnnoQfXbOM"
   },
   "outputs": [],
   "source": [
    "#Required imports\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm import tqdm # Use standard tqdm for simple progress bars\n",
    "from google.colab import files\n",
    "\n",
    "# Flotorch SDK components\n",
    "from flotorch.sdk.llm import FlotorchLLM\n",
    "from flotorch.sdk.memory import FlotorchVectorStore\n",
    "from flotorch.sdk.utils import memory_utils\n",
    "\n",
    "# Flotorch Eval components\n",
    "from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n",
    "\n",
    "print(\"Imported necessary libraries successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6DGAqY7nPS"
   },
   "source": [
    "## 4. Load Data and Prompts\n",
    "\n",
    "### Purpose\n",
    "Here, we load our corporate travel ground-truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local files.\n",
    "\n",
    "### Files Required\n",
    "\n",
    "**1. `travel_policy_gt.json` (Ground Truth)**  \n",
    "Contains travel-policy question–answer pairs for evaluation. Each `answer` is the expected correct response that the contextual relevancy metric will compare against the retrieved passages.\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "  \"question\": \"What class of service is required for domestic air travel regardless of employee level?\",\n",
    "    \"answer\": \"Domestic air travel must be booked in economy class for all flights regardless of employee level or position.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the current IRS mileage reimbursement rate for personal vehicle use?\",\n",
    "    \"answer\": \"The current IRS mileage reimbursement rate is sixty-seven cents per mile.\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**2. `travel_policy_prompt.json` (Agent Prompts)**  \n",
    "Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"system_prompt\": \"You are a corporate Travel Policy assistant. Answer strictly with information from the provided policy excerpts.\",\n",
    "  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations so the evaluation set and prompts align with the Corporate Travel Policy guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFYSNDJo1MAP"
   },
   "outputs": [],
   "source": [
    "print(\"Please upload your Ground Truth file (gt.json)\")\n",
    "gt_upload = files.upload()\n",
    "\n",
    "gt_path = list(gt_upload.keys())[0]\n",
    "with open(gt_path, 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n",
    "\n",
    "\n",
    "print(\"Please upload your Prompts file (prompts.json)\")\n",
    "prompts_upload = files.upload()\n",
    "\n",
    "prompts_path = list(prompts_upload.keys())[0]\n",
    "with open(prompts_path, 'r') as f:\n",
    "    prompt_config = json.load(f)\n",
    "print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d4b680f"
   },
   "source": [
    "## 5. Define Helper Function\n",
    "\n",
    "### Purpose\n",
    "Create a prompt-formatting helper for LLM message construction.\n",
    "\n",
    "### Functionality\n",
    "The `create_messages` function:\n",
    "- Builds the final prompt that will be sent to the LLM.\n",
    "- Accepts system prompt, user prompt template, question, and retrieved context chunks\n",
    "- Replaces `{context}` and `{question}` placeholders in the user prompt\n",
    "- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC4hMY5Ucdo4"
   },
   "outputs": [],
   "source": [
    "def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n",
    "    \"\"\"\n",
    "    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n",
    "    \"\"\"\n",
    "    context_text = \"\"\n",
    "    if context:\n",
    "        if isinstance(context, list):\n",
    "            context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "        elif isinstance(context, str):\n",
    "            context_text = context\n",
    "\n",
    "    # Format the user prompt template\n",
    "    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76e336e1"
   },
   "source": [
    "## 6. Initialize Clients\n",
    "\n",
    "### Purpose\n",
    "Set up the infrastructure for RAG pipeline execution.\n",
    "\n",
    "### Components Initialized\n",
    "1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n",
    "2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n",
    "3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n",
    "\n",
    "These clients power the evaluation loop by retrieving relevant context and generating answers for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a66f8510"
   },
   "outputs": [],
   "source": [
    "# 1. Set up the LLM for generating answers\n",
    "inference_llm = FlotorchLLM(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    model_id=inference_model_name\n",
    ")\n",
    "\n",
    "# 2. Set up the Knowledge Base connection\n",
    "kb = FlotorchVectorStore(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    vectorstore_id=knowledge_base_repo\n",
    ")\n",
    "\n",
    "# 3. Load prompts into variables\n",
    "system_prompt = prompt_config.get(\"system_prompt\", \"\")\n",
    "user_prompt_template = prompt_config.get(\"user_prompt_template\", \"{question}\")\n",
    "\n",
    "print(\"Models and Knowledge Base are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f51660d1"
   },
   "source": [
    "## 7. Run Experiment Loop\n",
    "\n",
    "### Purpose\n",
    "Execute the full DeepEval for each question to generate answers for evaluation.\n",
    "\n",
    "### Pipeline Steps\n",
    "For each question in `ground_truth`, the loop performs:\n",
    "\n",
    "1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages\n",
    "2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n",
    "3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n",
    "4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n",
    "\n",
    "### Error Handling\n",
    "A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0e37ac2"
   },
   "outputs": [],
   "source": [
    "evaluation_items = [] # This will store our results\n",
    "\n",
    "# Use simple tqdm for a progress bar\n",
    "print(f\"Running experiment on {len(ground_truth)} items...\")\n",
    "\n",
    "for qa in tqdm(ground_truth):\n",
    "    question = qa.get(\"question\", \"\")\n",
    "    gt_answer = qa.get(\"answer\", \"\")\n",
    "\n",
    "    try:\n",
    "        # --- 1. Retrieve Context ---\n",
    "        search_results = kb.search(query=question)\n",
    "        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n",
    "\n",
    "        # --- 2. Build Messages ---\n",
    "        messages = create_messages(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt_template=user_prompt_template,\n",
    "            question=question,\n",
    "            context=context_texts\n",
    "        )\n",
    "\n",
    "        # --- 3. Generate Answer ---\n",
    "        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n",
    "        generated_answer = response.content\n",
    "\n",
    "        # --- 4. Store for Evaluation ---\n",
    "        evaluation_items.append(EvaluationItem(\n",
    "            question=question,\n",
    "            generated_answer=generated_answer,\n",
    "            expected_answer=gt_answer,\n",
    "            context=context_texts, # Store the context for later display\n",
    "            metadata=headers,\n",
    "        ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n",
    "        # Store a failure case so we can see it\n",
    "        evaluation_items.append(EvaluationItem(\n",
    "            question=question,\n",
    "            generated_answer=f\"Error: {e}\",\n",
    "            expected_answer=gt_answer,\n",
    "            context=[],\n",
    "            metadata={\"error\": str(e)},\n",
    "        ))\n",
    "\n",
    "print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crt03A2aphia"
   },
   "source": [
    "## 8. Initialize the Evaluator (DeepEval)\n",
    "\n",
    "### Using DeepEval Contextual Relevancy\n",
    "\n",
    "Now that we have our `evaluation_items` list, we switch the `LLMEvaluator` to the **DeepEval** backend so every score reflects how relevant the retrieved travel-policy snippets are to each employee question and generated answer.\n",
    "\n",
    "This class remains the *“head judge”* for the evaluation loop; we’re simply selecting the DeepEval rubric that specializes in topical alignment between the question, the answer, and the supporting context.\n",
    "\n",
    "### Parameter Insights (DeepEval Mode)\n",
    "\n",
    "- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch Eval service.  \n",
    "- **`inferencer_model` / `embedding_model`** — DeepEval-powered scoring still needs the evaluator LLM and embeddings for semantic checks.  \n",
    "- **`evaluation_engine=\"deepeval\"`** — Routes metrics through DeepEval, which (per the [flotorch-eval repository](https://github.com/FissionAI/flotorch-eval/tree/develop)) unlocks the following metric keys:  \n",
    "  - **`MetricKey.FAITHFULNESS`**\n",
    "  - **`MetricKey.ANSWER_RELEVANCY`**\n",
    "  - **`MetricKey.CONTEXT_RELEVANCY`**\n",
    "  - **`MetricKey.CONTEXT_PRECISION`**\n",
    "  - **`MetricKey.CONTEXT_RECALL`**\n",
    "  - **`MetricKey.HALLUCINATION`**\n",
    "  These are the same metrics surfaced in Flotorch’s *auto* mode when Ragas prerequisites (like embeddings) are missing.  \n",
    "- **`metrics`** — For this notebook we register only `MetricKey.CONTEXT_RELEVANCY`, keeping the focus on whether retrieved snippets stay on-topic for the traveler’s request.  \n",
    "- **`metric_configs`** — Pass DeepEval-specific arguments such as a `\"threshold\"` (e.g., `0.8`) to trigger pass/fail decisions.  \n",
    "- **Thresholds** — Set between `0.0–1.0`; travel-policy reviews typically target `0.9+` to ensure off-topic passages are flagged quickly.\n",
    "\n",
    "DeepEval’s contextual relevancy rubric expects each test case to include the `input`, `actual_output`, and `retrieval_context` fields so it can judge whether each snippet genuinely helps answer the question ([DeepEval Contextual Relevancy docs](https://deepeval.com/docs/metrics-contextual-relevancy)). The evaluator produces verdicts with reasons explaining why a passage was or wasn’t relevant, helping teams fine-tune retriever settings.\n",
    "\n",
    "### DeepEval Contextual Relevancy Metric\n",
    "\n",
    "**Definition**: verifies that the retrieved travel-policy context is pertinent to the employee’s question and the model’s answer using the DeepEval contextual relevancy rubric. A score of 1 indicates every snippet directly supports the topic at hand; lower scores reveal noisy or tangential excerpts that should be filtered out.\n",
    "\n",
    "**How It Works**:\n",
    "1. DeepEval reviews the question (`input`) and model answer (`actual_output`).  \n",
    "2. Each snippet in `retrieval_context` is scored for topical alignment, with explanations for irrelevant passages.  \n",
    "3. The final contextual relevancy score reflects the proportion of snippets deemed relevant and is compared against the configured threshold.  \n",
    "\n",
    "**Example**:\n",
    "\n",
    "*Question*: \"Can I use ride-sharing to get to the airport and have it reimbursed?\"\n",
    "\n",
    "- *Pass Scenario* (Score = 1.0): Retrieved context covers the ground transportation section outlining preferred airport transfer options, reimbursement caps, and tipping guidance.  \n",
    "- *Fail Scenario* (Score = 0.0): Retrieved context discusses per-diem meal limits or lodging standards with no mention of airport transit, so DeepEval flags the context as irrelevant.  \n",
    "\n",
    "This mirrors the broader RAG workflow while delivering guardrail-ready signals tailored to corporate travel policy coverage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5e9f8a1"
   },
   "outputs": [],
   "source": [
    "# Configure DeepEval Contextual Relevancy thresholds\n",
    "metric_args = {\n",
    "    \"context_relevancy\": {\"threshold\": 0.8},\n",
    "}\n",
    "\n",
    "# Initialize the LLMEvaluator client\n",
    "evaluator_client = LLMEvaluator(\n",
    "    api_key=FLOTORCH_API_KEY,\n",
    "    base_url=FLOTORCH_BASE_URL,\n",
    "    embedding_model=evaluation_embedding_model_name,\n",
    "    inferencer_model=evaluation_llm_model_name,\n",
    "    metrics=[\n",
    "        MetricKey.CONTEXT_RELEVANCY,\n",
    "    ],\n",
    "    evaluation_engine=\"deepeval\",\n",
    "    metric_configs=metric_args\n",
    ")\n",
    "\n",
    "print(\"LLMEvaluator client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "new_markdown_cell_for_function"
   },
   "source": [
    "### Define the Evaluation Runner Function\n",
    "\n",
    "### Purpose\n",
    "Before running the evaluation, we define a helper function `run_evaluation`. This function iterates through our list\n",
    "of `EvaluationItem` objects and calls `evaluator_client.evaluate()` on *each one individually*.\n",
    "\n",
    "### Functionality\n",
    "The `run_evaluation` function:\n",
    "1. **Iterates** through each `EvaluationItem` in the experiment\n",
    "2. **Evaluates** by calling `evaluator_client.evaluate()` to score contextual relevancy\n",
    "3. **Extracts** evaluation metrics (contextual relevancy score) and gateway metrics (cost, latency, tokens)\n",
    "4. **Calculates** average score across all metrics\n",
    "5. **Combines** evaluation and gateway metrics into a single dictionary\n",
    "6. **Structures** results with model name, input query, context, generated answer, ground truth, and all metrics\n",
    "7. **Returns** a complete results list ready for analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3aNu0GfSmKy"
   },
   "outputs": [],
   "source": [
    "def run_evaluation(experiment_items):\n",
    "    results = []\n",
    "    for item in experiment_items:\n",
    "        eval_result = evaluator_client.evaluate([item])\n",
    "        eval_metrics = eval_result.get(\"evaluation_metrics\", {})\n",
    "        gateway_metrics = eval_result.get(\"gateway_metrics\",{})\n",
    "\n",
    "        if eval_metrics:\n",
    "            avg_score  = sum(eval_metrics.values())/len(eval_metrics)\n",
    "            eval_metrics[\"average_score\"] = round(avg_score, 2)\n",
    "\n",
    "        combined_metrics = eval_metrics.copy()\n",
    "        if gateway_metrics:\n",
    "            combined_metrics.update(gateway_metrics)\n",
    "        results.append({\n",
    "            \"model\":evaluation_llm_model_name,\n",
    "            \"input_query\": item.question,\n",
    "            \"context\": item.context,\n",
    "            \"generated_answer\": item.generated_answer,\n",
    "            \"groundtruth_answer\": item.expected_answer,\n",
    "            \"evaluation_metrics\": combined_metrics\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4d3b6f0"
   },
   "source": [
    "## 9. Run Evaluation\n",
    "\n",
    "### Purpose\n",
    "Execute the evaluation process to score all generated answers using the DeepEval Contextual Relevancy metric.\n",
    "\n",
    "### Process\n",
    "- Calls `run_evaluation()` with the complete list of `evaluation_items`\n",
    "- For each item, the evaluator measures contextual relevancy by comparing the retrieved snippets against the traveler’s question and generated answer\n",
    "- Collects context-relevancy scores plus gateway metrics (cost, latency, tokens) and structured results\n",
    "- Outputs a complete evaluation report ready for analysis\n",
    "\n",
    "**Note**: This step may take a few minutes because each question triggers a DeepEval judge call to compute contextual relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c1d0e3b2"
   },
   "outputs": [],
   "source": [
    "print(\"Starting evaluation... This may take a few minutes.\")\n",
    "\n",
    "eval_result = run_evaluation(evaluation_items)\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8f1c3a7"
   },
   "source": [
    "## 10. View Per-Question Results\n",
    "\n",
    "### Purpose\n",
    "Display contextual relevancy scores in a compact table so travel program owners can confirm that each answer referenced the proper policy passages.\n",
    "\n",
    "### Table Structure\n",
    "The output table includes:\n",
    "- **#**: Sequence number for each travel-policy scenario.  \n",
    "- **Question**: The traveler or approver query (truncated to 30 characters for readability).  \n",
    "- **Context (preview)**: First retrieved policy excerpt with a count of additional snippets.  \n",
    "- **Generated Answer**: Assistant response trimmed to 40 characters.  \n",
    "- **Ground Truth**: Reference answer from the gold set (truncated to 30 characters).  \n",
    "- **Context Relevancy Score**: DeepEval contextual relevancy score between `0` and `1` (`1` = every snippet stayed on-topic).  \n",
    "\n",
    "### Functionality\n",
    "- Uses `tabulate` to render the table in a `fancy_grid` layout.  \n",
    "- Relies on `format_context()` to collapse long context lists into a single preview entry.  \n",
    "- Applies `textwrap.fill()` so each column stays readable, even for dense policy language.  \n",
    "\n",
    "This view highlights which answers were backed by relevant retrievals versus those that surfaced noisy passages requiring knowledge-base tuning or prompt adjustments.\n",
    "\n",
    "This table allows you to quickly compare generated answers against ground truth and identify cases where retrieval failed to reference critical booking, reimbursement, or safety guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "40BGB_ltSmKy"
   },
   "outputs": [],
   "source": [
    "# --- Updated display + truncation (copy-paste ready) ---\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Helper: truncate long strings and annotate extra list items\n",
    "def format_context(context_list):\n",
    "    if not (isinstance(context_list, list) and context_list):\n",
    "        return \"No Context\"\n",
    "    context_str = context_list[0]\n",
    "    if len(context_list) > 1:\n",
    "        context_str += f\"\\n... (+{len(context_list)-1} more)\"\n",
    "    return context_str\n",
    "\n",
    "\n",
    "# Column headers (added new metrics and diagnostics)\n",
    "headers = [\n",
    "    \"#\", \"Question\", \"Context\", \"Generated Answer\", \"Ground Truth\",\n",
    "    \"context_relevancy\",\n",
    "]\n",
    "\n",
    "# Build the table rows from eval_result (safe access, rounding)\n",
    "table = []\n",
    "for i, item in enumerate(eval_result, 1):\n",
    "    m = item.get(\"evaluation_metrics\", {})\n",
    "    row = [\n",
    "        i,\n",
    "        textwrap.fill(item.get(\"input_query\", \"—\"), width=30),\n",
    "        textwrap.fill(format_context(item.get(\"context\", [])), width=60),\n",
    "        textwrap.fill(item.get(\"generated_answer\", \"—\"), width=40),\n",
    "        textwrap.fill(item.get(\"groundtruth_answer\", \"—\"), width=30),\n",
    "        round(m.get(\"context_relevancy\", 2), 2),\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\n--- Per-Query Evaluation Results ---\\n\")\n",
    "print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8l9m0n1"
   },
   "source": [
    "## 11. View Raw JSON Results\n",
    "\n",
    "### Purpose\n",
    "Display the complete evaluation results in JSON format for detailed inspection and programmatic access.\n",
    "\n",
    "### Output Structure\n",
    "The JSON output includes for each question:\n",
    "- **model**: The evaluation LLM model used\n",
    "- **input_query**: The original question\n",
    "- **context**: Full retrieved context passages (not truncated)\n",
    "- **generated_answer**: Complete LLM-generated response\n",
    "- **groundtruth_answer**: Expected correct answer\n",
    "- **evaluation_metrics**: Dictionary containing:\n",
    "  - **context_relevancy**: DeepEval contextual relevancy score between `0` and `1`\n",
    "  - **total_latency_ms**: Total evaluation time in milliseconds\n",
    "  - **total_cost**: Cost of evaluation in USD\n",
    "  - **total_tokens**: Token count for evaluation\n",
    "\n",
    "This raw JSON output is useful for follow-up audits, regression tracking, or downstream automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2q3r4s5"
   },
   "outputs": [],
   "source": [
    "print(\"--- Aggregate Evaluation Results ---\")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_cell_new"
   },
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "This notebook delivered an end-to-end workflow for evaluating a Corporate Travel Policy Advisor with Flotorch Eval using the DeepEval contextual relevancy metric.\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "1. **Configured Infrastructure**\n",
    "   - Set up `FlotorchLLM` for answer generation.  \n",
    "   - Connected to `FlotorchVectorStore` for travel-policy retrieval.  \n",
    "   - Initialized `LLMEvaluator` with the DeepEval engine targeting contextual relevancy.  \n",
    "\n",
    "2. **Generated Responses**\n",
    "   - Loaded travel ground-truth questions from `travel_policy_gt.json`.  \n",
    "   - Retrieved relevant policy excerpts for each question.  \n",
    "   - Generated answers with the inference LLM and captured gateway metadata (latency, cost, tokens).  \n",
    "\n",
    "3. **Evaluated Contextual Relevancy**\n",
    "   - Ran DeepEval contextual relevancy scoring over every response.  \n",
    "   - Verified whether the retrieved snippets stayed on-topic for the traveler’s request.  \n",
    "   - Recorded contextual relevancy scores alongside gateway diagnostics for governance.  \n",
    "\n",
    "4. **Visualized Results**\n",
    "   - Displayed per-question contextual relevancy scores in a reviewer-friendly table.  \n",
    "   - Exported the full JSON payload for auditing or automation.  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Context Relevancy = 1.0** signals all retrieved snippets were pertinent to the travel question; lower scores highlight noisy or off-policy passages.  \n",
    "- DeepEval judges **Question & Answer ↔ Retrieved Context**, making it ideal for validating that booking, reimbursement, and safety guidance come from the right policy sections.  \n",
    "- Monitoring contextual relevancy keeps the focus on retriever quality, helping finance and travel teams tighten documentation coverage before guidance reaches employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWqCyZKnIHZ3"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xac-BocRIHZ3"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
