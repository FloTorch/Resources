{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLiDjiNihiBE"
      },
      "source": [
        "# Agent Evaluation Using Trace ID\n",
        "\n",
        "This notebook demonstrates how to evaluate agent performance using **Tool Call Accuracy** metric when you already have a trace ID from a previously executed agent run.\n",
        "\n",
        "Unlike other notebooks that create and run agents, this notebook assumes you have already executed your agent elsewhere and obtained a trace ID. It focuses solely on fetching the trace and evaluating it using the Flotorch Eval framework.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "* **Trace ID**: A unique identifier for an OpenTelemetry trace generated during agent execution.\n",
        "* **OpenTelemetry Traces**: Detailed records of the agent's execution steps (spans) used to analyze tool call decisions and accuracy.\n",
        "* **ToolCallAccuracy**: A Flotorch Eval metric that evaluates the accuracy and appropriateness of tool usage decisions. The evaluation metric used is **toolcall_accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/07_AgentEvaluationUsingTraceID_Workflow_Diagram.drawio.png)\n",
        "*Figure : Detailed workflow diagram showing the step-by-step process of agent evaluation using trace ID from trace fetching through metric computation.*\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models.\n",
        "* Valid Flotorch API key and gateway base URL.\n",
        "* A trace ID from a previously executed agent run with OpenTelemetry tracing enabled.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYxX8NNlhiBF"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for tool call accuracy evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies for tool call accuracy metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAeZF2FshiBF",
        "outputId": "378e2408-2719-4626-c0cc-fc1df3dc042c"
      },
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmN3T5oShiBG"
      },
      "source": [
        "## 2. Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_LXKjkJhiBG"
      },
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  \n",
        "    print(f\"✓ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"✗ FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # https://dev-gateway.flotorch.cloud\n",
        "print(f\"✓ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"✓ All credentials configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6PYKyOFhiBH"
      },
      "source": [
        "## 3. Global Provider Models and Evaluator Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure the evaluator model for running the ToolCallAccuracy metric.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Evaluator Configuration**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdCX06fGhiBH",
        "outputId": "f7e5c0c9-a39d-4c5d-fa77-4ee06248aa0f"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"✓ Global provider models defined\")\n",
        "\n",
        "# The LLM model used for evaluation.\n",
        "# Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "# You can use your own models from Flotorch Console as well\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU\n",
        "\n",
        "print(\"✓ Evaluator configuration defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xHNnJ-HhiBH"
      },
      "source": [
        "## 4. Provide Trace ID\n",
        "\n",
        "### Purpose\n",
        "Set the trace ID variable from a previously executed agent run. This trace ID is used to fetch the OpenTelemetry trace data for evaluation.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Trace ID Configuration**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `TRACE_ID` | The unique identifier of the trace you want to evaluate. This trace ID should come from a previously executed agent run with OpenTelemetry tracing enabled | `abc123def456...` |\n",
        "\n",
        "### How to Obtain a Trace ID\n",
        "\n",
        "If you have executed your agent elsewhere:\n",
        "- The trace ID is usually returned after agent execution\n",
        "- Check your agent execution logs or console output for trace IDs\n",
        "- Trace IDs are generated when agents run with OpenTelemetry tracing enabled\n",
        "\n",
        "**Note**: This notebook assumes you already have a trace ID. If you need to create and run an agent first, please refer to other example notebooks that demonstrate agent creation and execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RK8JHFvhiBI",
        "outputId": "098a9fca-9d44-4eb5-9d94-e117713114a0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Trace ID Configuration\n",
        "# ============================================================================\n",
        "# Paste your trace ID here\n",
        "# This trace ID should come from a previously executed agent run\n",
        "\n",
        "TRACE_ID = \"<Your_Trace_ID>\"  # The trace ID from a previously executed agent run   || ex : 89c81fa83eb639041529d3b7587b0d37\n",
        "\n",
        "print(\"✓ Trace ID configuration defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHKal85ehiBI"
      },
      "source": [
        "## 5. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating agent tool call accuracy using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`ToolCallAccuracy`**: Flotorch Eval metric that evaluates the accuracy and appropriateness of tool usage decisions\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8P1cnW4hiBI",
        "outputId": "a5b01889-e425-4b33-cf85-2336dccfd3e5"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.llm_evaluators import ToolCallAccuracy\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"✓ Imported necessary libraries successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bugN1NThiBI"
      },
      "source": [
        "## 6. Initialize AgentEvaluator\n",
        "\n",
        "### Purpose\n",
        "Initialize the `AgentEvaluator` client that will be used to fetch traces and run evaluations.\n",
        "\n",
        "### Key Components\n",
        "1. **AgentEvaluator** (`client`):\n",
        "   - Connects to the Flotorch gateway using API credentials\n",
        "   - Configured with a default evaluator model for running LLM-based metrics\n",
        "   - Provides methods to fetch traces using trace IDs and evaluate them\n",
        "\n",
        "The AgentEvaluator is the main interface for all evaluation operations in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabMtRBHhiBI",
        "outputId": "e3b25483-f70b-4252-bd88-6ca7f4eed9c1"
      },
      "outputs": [],
      "source": [
        "# Initialize the ToolCallAccuracy metric\n",
        "metrics = [ToolCallAccuracy()]\n",
        "\n",
        "# Initialize the AgentEvaluator client\n",
        "client = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "\n",
        "print(\"✓ AgentEvaluator initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dUa-AcGhiBI"
      },
      "source": [
        "## 7. Fetch Trace Using Trace ID\n",
        "\n",
        "### Purpose\n",
        "Fetch the OpenTelemetry trace data from the Flotorch gateway using the provided trace ID.\n",
        "\n",
        "### Process\n",
        "1. **Validate Trace ID**: Ensure a trace ID has been provided\n",
        "2. **Fetch Trace**: Use `AgentEvaluator.fetch_traces()` method to retrieve the trace data\n",
        "3. **Verify Success**: Confirm the trace was successfully fetched\n",
        "\n",
        "### Key Components\n",
        "- **`client.fetch_traces(trace_id)`**: Method that retrieves trace data from the Flotorch API\n",
        "  - Takes a trace ID string as input\n",
        "  - Returns the complete trace data containing all spans and execution information\n",
        "  - The trace data includes tool call decisions, parameter details, and execution results\n",
        "\n",
        "The fetched trace contains detailed information about tool call decisions and execution, which will be analyzed by the ToolCallAccuracy metric to compute the toolcall_accuracy score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q26MK2VhiBI",
        "outputId": "410fd89d-d627-4f0d-a17f-666ae5c87041"
      },
      "outputs": [],
      "source": [
        "# Fetch the trace data from the Flotorch gateway using the trace ID\n",
        "if TRACE_ID:\n",
        "    try:\n",
        "        traces = client.fetch_traces(TRACE_ID)\n",
        "        if traces:\n",
        "            print(f\"✓ Trace fetched successfully for trace ID: {TRACE_ID}\")\n",
        "        else:\n",
        "            print(f\"✗ No trace data found for trace ID: {TRACE_ID}\")\n",
        "            traces = None\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error fetching trace: {e}\")\n",
        "        traces = None\n",
        "else:\n",
        "    print(\"✗ No trace ID provided. Please set TRACE_ID variable with your trace ID in section 4.\")\n",
        "    traces = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5pGeK8ohiBI"
      },
      "source": [
        "## 8. Run Evaluation\n",
        "\n",
        "### Purpose\n",
        "Execute the tool call accuracy evaluation by processing the fetched OpenTelemetry trace using the ToolCallAccuracy metric to assess tool usage decisions.\n",
        "\n",
        "### Process\n",
        "- Calls `client.evaluate()` with the trace data and ToolCallAccuracy metric\n",
        "- The evaluator processes the trace to analyze tool call decisions and execution\n",
        "- Computes the **toolcall_accuracy** metric which includes:\n",
        "  - Accuracy score (0.0 to 1.0) indicating how appropriate tool usage was\n",
        "  - Detailed evaluation explanation of tool call decisions\n",
        "  - Assessment of tool selection, parameter accuracy, and timing\n",
        "- Returns evaluation results with trajectory ID and metric scores\n",
        "\n",
        "This step generates the tool call accuracy analysis that will be displayed in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWUx7IgfhiBI",
        "outputId": "6a3df654-b2e6-49a1-8a11-b4fb6ab7f4f7"
      },
      "outputs": [],
      "source": [
        "if 'traces' in locals() and traces:\n",
        "    try:\n",
        "        # Evaluate the trace using the ToolCallAccuracy metric\n",
        "        results = await client.evaluate(\n",
        "            trace=traces,\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "        print(\"✓ Evaluation completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error during evaluation: {e}\")\n",
        "        results = None\n",
        "else:\n",
        "    print(\"Cannot evaluate: No traces were available.\")\n",
        "    results = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfrRm5GAhiBI"
      },
      "source": [
        "## 9. Display and Interpret Results\n",
        "\n",
        "### Purpose\n",
        "Define helper functions to format and display the evaluation output clearly, showing the toolcall_accuracy metric results in a readable format.\n",
        "\n",
        "### Functionality\n",
        "The `display_metrics` function:\n",
        "- Extracts the `toolcall_accuracy` metric from evaluation results\n",
        "- Formats the accuracy score and evaluation details\n",
        "- Creates a structured display showing:\n",
        "  - Tool Call Accuracy Score (0.0 to 1.0)\n",
        "  - Detailed evaluation explanation\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize tool call accuracy metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NuY_pkUhiBJ",
        "outputId": "37b64990-bbbd-43d9-f36a-024586a9bcf2"
      },
      "outputs": [],
      "source": [
        "def display_metrics(result):\n",
        "    \"\"\"\n",
        "    Display tool call accuracy metrics in a formatted table.\n",
        "    \"\"\"\n",
        "    # Find the toolcall_accuracy metric\n",
        "    metric = next((m for m in result.scores if m.name == \"toolcall_accuracy\"), None)\n",
        "    if not metric:\n",
        "        print(\"No toolcall_accuracy metric found.\")\n",
        "        return\n",
        "\n",
        "    # Extract metric details\n",
        "    d = metric.details\n",
        "\n",
        "    # Get the details string (which contains the evaluation explanation)\n",
        "    details_text = d.get(\"details\", \"No details available.\")\n",
        "\n",
        "    # Format the details string with better readability\n",
        "    details = f\"Score: {metric.score:.2f} / 1.0\\n\\nEvaluation Details:\\n{details_text}\"\n",
        "\n",
        "    # Create DataFrame for display\n",
        "    df = pd.DataFrame([{\n",
        "        \"Metric\": metric.name.replace(\"_\", \" \").title(),\n",
        "        \"Score\": f\"{metric.score:.2f}\",\n",
        "        \"Details\": details\n",
        "    }])\n",
        "\n",
        "    # Display DataFrame with multiline support\n",
        "    display(df.style.set_properties(\n",
        "        subset=['Details'],\n",
        "        **{'white-space': 'pre-wrap', 'text-align': 'left'}\n",
        "    ))\n",
        "\n",
        "print(\"✓ Display metrics function defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEMiaeD2hiBJ"
      },
      "source": [
        "## 10. View Tool Call Accuracy Results\n",
        "\n",
        "### Purpose\n",
        "Display the tool call accuracy evaluation results in a formatted table showing the complete assessment.\n",
        "\n",
        "### Output\n",
        "The displayed table includes:\n",
        "- **Metric**: The evaluation metric name (toolcall_accuracy)\n",
        "- **Score**: The tool call accuracy score (0.0 to 1.0)\n",
        "- **Details**: Comprehensive evaluation showing:\n",
        "  - Accuracy score out of 1.0\n",
        "  - Detailed explanation of tool call decisions\n",
        "  - Assessment of tool selection, parameter accuracy, and timing appropriateness\n",
        "\n",
        "This visualization helps identify tool usage issues and optimize the agent's tool call decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "F6w-1iPchiBJ",
        "outputId": "544f814f-7166-4382-a73b-89c457fabcd7"
      },
      "outputs": [],
      "source": [
        "if 'results' in locals() and results:\n",
        "    display_metrics(results)\n",
        "else:\n",
        "    print(\"No results object found. Please run sections 7 and 8 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lide79khiBJ"
      },
      "source": [
        "### Interpreting the Tool Call Accuracy Results\n",
        "\n",
        "The **toolcall_accuracy** metric is a vital tool for quality monitoring:\n",
        "\n",
        "* **Accuracy Score (0.0 to 1.0)**: Indicates how appropriate and accurate the agent's tool usage decisions were:\n",
        "    * **1.0**: Perfect tool call accuracy - tool selection, parameters, and timing were all appropriate\n",
        "    * **0.5-0.9**: Good accuracy with minor issues in tool selection or parameter formatting\n",
        "    * **0.0-0.4**: Poor accuracy - incorrect tool selection, wrong parameters, or inappropriate timing\n",
        "* **Evaluation Details**: Provides a detailed explanation of:\n",
        "    * **Tool Selection Appropriateness**: Whether the agent selected the correct tool for the task\n",
        "    * **Parameter Accuracy and Formatting**: Whether tool parameters were correctly formatted and accurate\n",
        "    * **Timing and Necessity**: Whether the tool call was made at the right time and was necessary for the task\n",
        "    * **Overall Quality**: Comprehensive assessment of tool usage decision quality\n",
        "\n",
        "Understanding tool call accuracy helps identify:\n",
        "- **Tool selection issues**: If the agent uses incorrect tools or fails to use tools when needed\n",
        "- **Parameter formatting problems**: If parameters are incorrectly extracted or formatted\n",
        "- **Timing optimization**: If tool calls are made unnecessarily or at inappropriate times\n",
        "- **Overall reliability**: Monitor accuracy to ensure the agent delivers accurate and reliable results via proper tool integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOX4v6mDhiBJ"
      },
      "source": [
        "## 11. Summary\n",
        "\n",
        "This notebook demonstrates how to evaluate agent tool call accuracy using a **trace ID** from a previously executed agent run.\n",
        "\n",
        "**Use Case**: Evaluate agent performance when you already have a trace ID.\n",
        "\n",
        "**Evaluation Metric**: toolcall_accuracy\n",
        "\n",
        "## Core Process\n",
        "\n",
        "### 1. Trace ID Input\n",
        "- Provide the trace ID from a previously executed agent run\n",
        "- The trace ID should correspond to an agent execution with OpenTelemetry tracing enabled\n",
        "\n",
        "### 2. Trace Fetching\n",
        "- Use `AgentEvaluator.fetch_traces(trace_id)` to retrieve the complete trace data\n",
        "- The trace contains detailed information about tool call decisions and execution\n",
        "\n",
        "### 3. Evaluation\n",
        "- Use the `AgentEvaluator` client along with the specialized **ToolCallAccuracy** metric from `flotorch-eval`\n",
        "- The evaluator processes the trace data to compute tool call accuracy statistics using the **toolcall_accuracy** metric\n",
        "\n",
        "### 4. Analysis\n",
        "- The notebook displays a thorough tool call accuracy assessment, including:\n",
        "  - **Accuracy Score** (0.0 to 1.0)\n",
        "  - **Evaluation Details** explaining tool usage decisions\n",
        "  - Assessment of tool selection, parameter accuracy, and timing\n",
        "\n",
        "## Purpose and Benefits\n",
        "\n",
        "This evaluation workflow is useful when:\n",
        "\n",
        "- You have executed agents in separate environments and want to evaluate the traces\n",
        "- You want to re-evaluate existing traces with different metrics or evaluators\n",
        "- You need to evaluate traces without recreating the agent setup\n",
        "- You want to analyze historical agent runs using their trace IDs\n",
        "\n",
        "This approach provides **flexibility and separation of concerns** by allowing evaluation to be performed independently of agent execution.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
