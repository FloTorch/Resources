{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/1IrwoNrb3AWLAhAqjlAkJNYa39p9eT9ui?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flotorch Agent Evaluation - All Metrics Combined\n",
        "\n",
        "This comprehensive notebook demonstrates how to measure and analyze **all agent evaluation metrics** for **Flotorch ADK agents** using the **Flotorch Eval** framework.\n",
        "\n",
        "The evaluation relies on **OpenTelemetry Traces** generated during the agent's run to provide comprehensive insights across multiple dimensions of agent performance.\n",
        "\n",
        "**Key Feature**: This notebook uses **ONE agent** and **ONE AgentEvaluator** to evaluate all metrics, making it efficient and easy to use.\n",
        "\n",
        "---\n",
        "\n",
        "## Metrics Covered\n",
        "\n",
        "This notebook includes evaluation for the following metrics:\n",
        "\n",
        "1. **Latency Metric** (`latency_summary`) - Measures agent response time and performance breakdown\n",
        "2. **Trajectory Evaluation with LLM** (`trajectory_evaluation_with_llm`) - Assesses overall trajectory quality using LLM-based evaluation\n",
        "3. **Trajectory Evaluation with Reference** (`trajectory_evaluation_with_reference`) - Compares trajectory against a reference trajectory\n",
        "4. **Tool Call Accuracy** (`toolcall_accuracy`) - Evaluates accuracy and appropriateness of tool usage decisions\n",
        "5. **Agent Goal Accuracy** (`agent_goal_accuracy`) - Assesses whether the agent successfully accomplished the user's true goal\n",
        "6. **Usage Metric** (`usage_summary`) - Provides detailed breakdown of token usage and costs\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/09_unified-agent_Workflow_Diagram.drawio.png)\n",
        "*Figure 2: Detailed workflow diagram showing the step-by-step process of unified agent evaluation with all metrics from agent execution through trace collection to comprehensive metric computations.*\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models\n",
        "* Valid Flotorch API key and gateway base URL\n",
        "* Agent configured with OpenTelemetry tracing enabled\n",
        "* For Latency Metric: Knowledge Base containing Flotorch documentation (files provided in `agents/flotorch_assistant_kb/`)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent Setup in Flotorch Console\n",
        "\n",
        "**Important**: Before running this notebook, you need to create an agent in the Flotorch Console. This section provides step-by-step instructions on how to set up the agent.\n",
        "\n",
        "### Step 1: Access Flotorch Console\n",
        "\n",
        "1. **Log in to Flotorch Console**:\n",
        "   - Navigate to your Flotorch Console (e.g., `https://dev-console.flotorch.cloud`)\n",
        "   - Ensure you have the necessary permissions to create agents\n",
        "\n",
        "2. **Navigate to Agents Section**:\n",
        "   - Click on **\"Agents\"** in the left sidebar\n",
        "   - You should see the \"Agent Builder\" option selected\n",
        "\n",
        "### Step 2: Create New Agent\n",
        "\n",
        "1. **Click \"Create FloTorch Agent\"**:\n",
        "   - Look for the blue **\"+ Create FloTorch Agent\"** button in the top right corner\n",
        "   - Click it to start creating a new agent\n",
        "\n",
        "2. **Agent Configuration**:\n",
        "   - **Agent Name**: Choose a unique name for your agent (e.g., `unified-agent`, `all-metrics-agent`)\n",
        "     - **Important**: The name should only contain alphanumeric characters and dashes (a-z, A-Z, 0-9, -)\n",
        "     - **Note**: Copy this agent name - you'll need to use it in the `agent_name` variable later\n",
        "   - **Description** (Optional): Add a description if desired\n",
        "\n",
        "### Step 3: Configure Agent Details\n",
        "\n",
        "After creating the agent, you'll be directed to the agent configuration page. Configure the following:\n",
        "\n",
        "#### Required Configuration:\n",
        "\n",
        "1. **Model** (`* Model`):\n",
        "   - **Required**: Select a model from the available options\n",
        "   - Example: `gpt-model` or any available model from your Flotorch gateway\n",
        "   - Click the edit icon to configure\n",
        "\n",
        "2. **Agent Details** (`* Agent Details`):\n",
        "   - **Required**: Configure agent details\n",
        "   - **System Prompt**: Copy and paste the following system prompt:\n",
        "\n",
        "You are a versatile AI assistant capable of handling multiple types of tasks:\n",
        "\n",
        "1. **Flotorch Documentation Assistant**: When users ask about Flotorch search the knowledge base to provide accurate, context-aware responses based on Flotorch documentation.\n",
        "\n",
        "2. **Research Assistant**: When users need information from the web, use web search to find current, relevant information and synthesize comprehensive answers.\n",
        "\n",
        "3. **Code Reviewer**: When users provide code for review, analyze it thoroughly for correctness, code quality, readability, performance, and provide structured feedback with ratings.\n",
        "\n",
        "4. **Weather Assistant**: When users ask about weather, use the weather tool to fetch real-time weather data including temperature, wind speed, humidity, and location details.\n",
        "\n",
        "5. **Travel Planner**: When users request travel planning, help create comprehensive travel plans including flight options, hotel recommendations, daily itineraries, and cost estimates. Use your knowledge and reasoning to provide detailed, practical travel advice.\n",
        "\n",
        "6. **News Assistant**: When users request news, fetch the latest news articles and provide summaries with key information, prioritizing relevant and recent content.\n",
        "\n",
        "Always:\n",
        "\n",
        "Use the appropriate tools when needed (knowledge base search, web search, weather API, news API)\n",
        "\n",
        "Provide clear, accurate, and helpful responses\n",
        "\n",
        "Structure your answers logically\n",
        "\n",
        "Cite sources when using external information\n",
        "\n",
        "Be concise but comprehensive\n",
        "\n",
        "   - **Goal**: Copy and paste the following goal:\n",
        "\n",
        "To be a versatile, multi-capability AI assistant that can:\n",
        "\n",
        "Answer questions about Flotorch using knowledge base search (RAG)\n",
        "\n",
        "Perform web-based research and synthesize information\n",
        "\n",
        "Review code and provide structured feedback\n",
        "\n",
        "Provide real-time weather information\n",
        "\n",
        "Create comprehensive travel plans\n",
        "\n",
        "Fetch and summarize news articles\n",
        "\n",
        "The agent should intelligently select and use the appropriate tools based on user queries, providing accurate, helpful, and well-structured responses for each type of task.\n",
        "\n",
        "#### Optional Configuration:\n",
        "\n",
        "1. **Tools**:\n",
        "   - Tools will be added programmatically via the notebook (see Section 8)\n",
        "   - You can leave this as \"Not Configured\" in the console\n",
        "\n",
        "2. **Input Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "3. **Output Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "### Step 4: Publish the Agent\n",
        "\n",
        "1. **Review Configuration**:\n",
        "   - Ensure the Model and Agent Details are configured correctly\n",
        "   - Verify the System Prompt and Goal are set\n",
        "\n",
        "2. **Publish Agent**:\n",
        "   - After configuration, click **\"Publish\"** or **\"Make a revision\"** to publish the agent\n",
        "   - Once published, the agent will have a version number (e.g., v1)\n",
        "\n",
        "3. **Note the Agent Name**:\n",
        "   - **Important**: Copy the exact agent name you used when creating the agent\n",
        "   - You will need to replace `<your_agent_name>` in the `agent_name` variable in Section 2.1 (Global Provider Models and Agent Configuration)\n",
        "\n",
        "### Step 5: Update Notebook Configuration\n",
        "\n",
        "1. **Update Agent Name**:\n",
        "   - Navigate to Section 2.1 in this notebook\n",
        "   - Find the `agent_name` variable\n",
        "   - Replace `<your_agent_name>` with the exact agent name you created in the console\n",
        "\n",
        "**Example**:\n",
        "- If you created an agent named `unified-agent` in the console\n",
        "- Set `agent_name = \"unified-agent\"` in the notebook\n",
        "\n",
        "### Summary of Required vs Optional Settings\n",
        "\n",
        "| Setting | Required/Optional | Value |\n",
        "|---------|------------------|-------|\n",
        "| **Agent Name** | **Required** | Choose a unique name (copy it for notebook) |\n",
        "| **Model** | **Required** | Select from available models |\n",
        "| **System Prompt** | **Required** | Use the system prompt provided above |\n",
        "| **Goal** | **Required** | Use the goal provided above |\n",
        "| **Tools** | **Optional** | Will be added via notebook code |\n",
        "| **Input Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "| **Output Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "\n",
        "**Note**: The tools (Knowledge Base, Web Search, Weather, News) will be added to the agent programmatically in the notebook code, so you don't need to configure them manually in the console.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for all agent evaluation metrics.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies for all metrics\n",
        "- **`flotorch[adk]`**: Flotorch ADK (Agent Development Kit) package with all dependencies for building and running Flotorch agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "# flotorch[adk]: Flotorch ADK package with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  \n",
        "    print(f\"âœ“ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"âœ— FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # https://dev-gateway.flotorch.cloud\n",
        "print(f\"âœ“ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"âœ“ All credentials configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Global Provider Models and Agent Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure agent-specific parameters.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation and agent operations:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Agent Configuration Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n",
        "| `agent_name` | The name of your Flotorch ADK agent | `flotorch-agent` |\n",
        "| `app_name` | The application name identifier | `agent-evaluation-app-name` |\n",
        "| `user_id` | The user identifier | `agent-evaluation-user` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation, agent operations, and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"âœ“ Global provider models defined\")\n",
        "\n",
        "# The LLM model used for evaluation. \n",
        "# Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "# You can use your own models from Flotorch Console as well\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU\n",
        "\n",
        "# Agent configuration - can be customized per metric section\n",
        "agent_name = \"<your_agent_name>\"  # The name of your Flotorch ADK agent                                        || ex : flotorch-agent\n",
        "app_name = \"<your_app_name>\"  # The application name identifier                                                || ex : agent-evaluation-app-name\n",
        "user_id = \"<your_user_id>\"  # The user identifier                                                              || ex : agent-evaluation-user\n",
        "\n",
        "print(\"âœ“ Agent Configuration Parameter defined \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating Flotorch ADK agents across all metrics using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`LatencyMetric`**: Flotorch Eval metric that measures agent response time and performance breakdown\n",
        "- **`TrajectoryEvalWithLLM`**: Flotorch Eval metric that assesses overall trajectory quality using LLM-based evaluation\n",
        "- **`TrajectoryEvalWithLLMWithReference`**: Flotorch Eval metric that compares trajectory against a reference trajectory using LLM-based evaluation\n",
        "- **`ToolCallAccuracy`**: Flotorch Eval metric that evaluates the accuracy and appropriateness of tool usage decisions\n",
        "- **`AgentGoalAccuracy`**: Flotorch Eval metric that assesses whether the agent successfully accomplished the user's true goal\n",
        "- **`UsageMetric`**: Flotorch Eval metric that provides detailed breakdown of token usage and costs\n",
        "- **`ReferenceTrajectory`**: Schema for defining the ideal \"golden path\" trajectory for comparison in TrajectoryEvalWithLLMWithReference\n",
        "- **`MetricConfig`**: Configuration class for customizing metric parameters and settings\n",
        "- **`FlotorchADKAgent`**: Creates and configures Flotorch ADK agents with custom tools and tracing\n",
        "- **`FlotorchADKSession`**: Manages agent sessions for multi-turn conversations\n",
        "- **`FlotorchVectorStore`**: Provides access to Flotorch knowledge bases for RAG-based searches\n",
        "- **`extract_vectorstore_texts`**: Utility function for extracting text results from vector store searches\n",
        "- **`Runner`**: Executes agent queries and coordinates the agent execution flow\n",
        "- **`FunctionTool`**: Wraps Python functions as tools that can be used by the agent\n",
        "- **`types`**: Google ADK types for creating message content and handling agent events\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs in notebooks\n",
        "- **`json`**: JSON parsing utilities for handling metric details and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.latency_metrics import LatencyMetric\n",
        "from flotorch_eval.agent_eval.metrics.llm_evaluators import (\n",
        "    TrajectoryEvalWithLLM,\n",
        "    TrajectoryEvalWithLLMWithReference,\n",
        "    ToolCallAccuracy,\n",
        "    AgentGoalAccuracy\n",
        ")\n",
        "from flotorch_eval.agent_eval.metrics.usage_metrics import UsageMetric\n",
        "from flotorch_eval.agent_eval.core.schemas import ReferenceTrajectory\n",
        "from flotorch_eval import MetricConfig\n",
        "\n",
        "# Flotorch ADK components\n",
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "from flotorch.sdk.memory import FlotorchVectorStore\n",
        "from flotorch.sdk.utils.memory_utils import extract_vectorstore_texts\n",
        "\n",
        "# Google ADK components\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.genai import types\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import json\n",
        "\n",
        "print(\"âœ“ Imported necessary libraries successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Base Setup:\n",
        "\n",
        "**Important**: Before using this tool, you need to create a Knowledge Base in the Flotorch Console.\n",
        "\n",
        "We have provided the files needed to create the Knowledge Base for this use case in the `agents/flotorch_assistant_kb/` directory. This folder contains all the files required to set up the knowledge base.\n",
        "\n",
        "**Instructions**:\n",
        "1. **Navigate to the Knowledge Base folder**: Go to the `agents/flotorch_assistant_kb/` directory\n",
        "2. **Create Knowledge Base in Flotorch Console**:\n",
        "   - Log in to your Flotorch Console\n",
        "   - Navigate to the Knowledge Bases section\n",
        "   - Create a new Knowledge Base (e.g., name it \"flotorch-documentation\" or \"flotorch-assistant-kb\")\n",
        "   - Upload all the files from the `agents/flotorch_assistant_kb/` folder\n",
        "3. **Get Knowledge Base ID**:\n",
        "   - After creating the Knowledge Base in the console, copy its ID\n",
        "   - Update the `KNOWLEDGE_BASE` variable in the configuration section (Section 4) with your Knowledge Base ID\n",
        "\n",
        "**Note**: Make sure to replace `<your_knowledge_base_id>` with your actual Knowledge Base ID in the configuration cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Knowledge Base Tool (for Latency Metric)\n",
        "\n",
        "**Purpose**: This tool searches the Flotorch knowledge base for information. It's used for the Latency Metric to enable RAG-based knowledge base search.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a search query as input\n",
        "- Searches the Flotorch Vector Store (Knowledge Base)\n",
        "- Returns relevant search results extracted from the knowledge base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 1: Knowledge Base Tool (for Latency Metric)\n",
        "# ============================================================================\n",
        "# Initialize the Flotorch Vector Store (Knowledge Base)\n",
        "kb = FlotorchVectorStore(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    vectorstore_id=KNOWLEDGE_BASE\n",
        ")\n",
        "\n",
        "def kb_tool(query: str) -> dict:\n",
        "    \"\"\"Custom tool for searching the Flotorch knowledge base.\"\"\"\n",
        "    try:\n",
        "        if not query:\n",
        "            return {\"success\": False, \"results\": [], \"error\": \"Empty query provided.\"}\n",
        "        context = kb.search(query)\n",
        "        results = extract_vectorstore_texts(context)\n",
        "        return {\"success\": True, \"results\": results, \"error\": None}\n",
        "    except Exception as e:\n",
        "        return {\"success\": False, \"results\": [], \"error\": str(e)}\n",
        "\n",
        "print(\"âœ“ Knowledge Base Tool (for Latency Metric)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Web Search Tool (for Trajectory Evaluation)\n",
        "\n",
        "**Purpose**: This tool performs Google searches and returns top results. It's used for Trajectory Evaluation metrics to enable the agent to search for information.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a search query as input\n",
        "- Uses Google Custom Search API to retrieve results\n",
        "- Returns formatted search results with titles, snippets, and links\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 2: Web Search Tool (for Trajectory Evaluation)\n",
        "# ============================================================================\n",
        "import requests\n",
        "\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Perform a Google search and return top results.\"\"\"\n",
        "\n",
        "    api_key = \"<YOUR_GOOGLE_API_KEY>\"\n",
        "    cse_id = \"<YOUR_GOOGLE CSE_ID>\"\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    \n",
        "    params = {\"key\": api_key, \"cx\": cse_id, \"q\": query, \"num\": 5}\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    if \"items\" not in data:\n",
        "        return \"No results found.\"\n",
        "    results = []\n",
        "    for item in data[\"items\"]:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        snippet = item.get(\"snippet\", \"\")\n",
        "        link = item.get(\"link\", \"\")\n",
        "        results.append(f\"ðŸ”¹ {title}\\n{snippet}\\n{link}\")\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "print(\"âœ“ Web Search Tool (for Trajectory Evaluation)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Weather Tool (for Tool Call Accuracy)\n",
        "\n",
        "**Purpose**: This tool retrieves current weather information for a given city. It's used for the Tool Call Accuracy metric to test the agent's ability to correctly select and use appropriate tools.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a city name as input\n",
        "- Uses geocoding API to get latitude and longitude\n",
        "- Fetches current weather data including temperature, wind speed, and humidity\n",
        "- Returns comprehensive weather information for the city\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 3: Weather Tool (for Tool Call Accuracy)\n",
        "# ============================================================================\n",
        "def get_weather(city_name: str) -> dict:\n",
        "    \"\"\"Return latitude, longitude, and current weather for a given city name.\"\"\"\n",
        "    geo_url = \"https://geocoding-api.open-meteo.com/v1/search\"\n",
        "    geo_params = {\"name\": city_name, \"count\": 1, \"language\": \"en\", \"format\": \"json\"}\n",
        "    geo_res = requests.get(geo_url, params=geo_params).json()\n",
        "    if \"results\" not in geo_res:\n",
        "        raise ValueError(f\"City '{city_name}' not found\")\n",
        "    city = geo_res[\"results\"][0]\n",
        "    lat, lon = city[\"latitude\"], city[\"longitude\"]\n",
        "    weather_url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    weather_params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"current\": \"temperature_2m,wind_speed_10m,relative_humidity_2m\"\n",
        "    }\n",
        "    weather_res = requests.get(weather_url, params=weather_params).json()\n",
        "    current_weather = weather_res.get(\"current\", {})\n",
        "    return {\n",
        "        \"city\": city[\"name\"],\n",
        "        \"country\": city.get(\"country\"),\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"weather\": current_weather\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Weather Tool (for Tool Call Accuracy)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. News Tool (for Usage Metric)\n",
        "\n",
        "**Purpose**: This tool fetches the latest news articles from around the world. It's used for the Usage Metric to measure token usage and costs when the agent processes news-related queries.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a limit parameter for the number of articles to fetch\n",
        "- Parses RSS feeds from Google News, with priority on India news\n",
        "- Returns formatted news articles with title, description, URL, and metadata\n",
        "- Handles errors gracefully and returns structured data for agent processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 4: News Tool (for Usage Metric)\n",
        "# ============================================================================\n",
        "from datetime import datetime\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def get_top_news(limit: int = 7) -> dict:\n",
        "    \"\"\"Get the latest top news articles from worldwide, with priority on India.\"\"\"\n",
        "    try:\n",
        "        articles = []\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "        \n",
        "        def parse_rss_feed(url: str, max_items: int) -> list:\n",
        "            parsed_articles = []\n",
        "            try:\n",
        "                response = requests.get(url, timeout=10, headers=headers)\n",
        "                if response.status_code == 200:\n",
        "                    root = ET.fromstring(response.content)\n",
        "                    for item in root.findall('.//item'):\n",
        "                        if len(parsed_articles) >= max_items:\n",
        "                            break\n",
        "                        title_elem = item.find('title')\n",
        "                        link_elem = item.find('link')\n",
        "                        desc_elem = item.find('description')\n",
        "                        title = title_elem.text if title_elem is not None else \"No title\"\n",
        "                        if \" - \" in title:\n",
        "                            title = title.split(\" - \", 1)[-1]\n",
        "                        if not any(a[\"title\"] == title for a in articles + parsed_articles):\n",
        "                            parsed_articles.append({\n",
        "                                \"title\": title,\n",
        "                                \"description\": desc_elem.text if desc_elem is not None else \"\",\n",
        "                                \"url\": link_elem.text if link_elem is not None else \"\",\n",
        "                                \"publishedAt\": str(datetime.now()),\n",
        "                                \"source\": {\"name\": \"Google News\"}\n",
        "                            })\n",
        "            except Exception:\n",
        "                pass\n",
        "            return parsed_articles\n",
        "        \n",
        "        india_news_urls = [\"https://news.google.com/rss/search?q=india+when:1d&hl=en-IN&gl=IN&ceid=IN:en\"]\n",
        "        for url in india_news_urls:\n",
        "            if len(articles) >= limit:\n",
        "                break\n",
        "            new_articles = parse_rss_feed(url, limit - len(articles))\n",
        "            articles.extend(new_articles)\n",
        "        articles = articles[:limit]\n",
        "        return {\"status\": \"success\", \"totalResults\": len(articles), \"articles\": articles, \"fetchedAt\": str(datetime.now())}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": f\"Failed to fetch news: {str(e)}\", \"articles\": []}\n",
        "\n",
        "# ============================================================================\n",
        "# Combine All Tools\n",
        "# ============================================================================\n",
        "all_tools = [\n",
        "    FunctionTool(kb_tool),\n",
        "    FunctionTool(web_search),\n",
        "    FunctionTool(get_weather),\n",
        "    FunctionTool(get_top_news)\n",
        "]\n",
        "\n",
        "print(\"âœ“ News Tool (for Usage Metric)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Agent, Session, and Evaluator Initialization\n",
        "\n",
        "### Purpose\n",
        "Initialize all the necessary components to run and evaluate the Flotorch ADK agent with OpenTelemetry tracing enabled.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Creates the agent client with custom tools (Web Search, Weather)\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Sets tracing endpoint: `\"https://dev-observability.flotorch.cloud/v1/traces\"`\n",
        "   - Essential for evaluation as traces contain complete execution information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "4. **AgentEvaluator** (`evaluator`):\n",
        "   - Initializes the evaluation client for metric assessment\n",
        "   - Configures the default evaluator model (LLM) for metric evaluation\n",
        "   - Used to evaluate agent performance across all  metrics\n",
        "\n",
        "These components work together to run the Flotorch ADK Agent and generate OpenTelemetry traces for evaluation across all  metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ONE Flotorch ADK Agent with all tools\n",
        "agent_client = FlotorchADKAgent(\n",
        "    agent_name=agent_name,\n",
        "    custom_tools=all_tools,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    tracer_config={\n",
        "        \"enabled\": True,  # Enable tracing for metrics\n",
        "        \"endpoint\": \"https://dev-observability.flotorch.cloud/v1/traces\",\n",
        "        \"sampling_rate\": 1  # Sample 100% of traces\n",
        "    }\n",
        ")\n",
        "agent = agent_client.get_agent()\n",
        "\n",
        "# Initialize session service\n",
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        ")\n",
        "\n",
        "# Create the ADK Runner to execute agent queries\n",
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=app_name,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "# Initialize ONE AgentEvaluator for all metrics\n",
        "evaluator = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "\n",
        "print(\"âœ“ This agent and evaluator will be used for all metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Helper Function for Running a Query\n",
        "\n",
        "### Purpose\n",
        "Define a helper function that executes a single-turn query with the agent and extracts the final response. The agent execution is automatically traced for evaluation.\n",
        "\n",
        "### Functionality\n",
        "The `run_single_turn` function:\n",
        "- Accepts a `Runner`, query string, session ID, and user ID as parameters\n",
        "- Creates a user message using Google ADK types (`types.Content` and `types.Part`)\n",
        "- Executes the query through the runner which coordinates the agent execution flow\n",
        "- Iterates through events returned by the runner to find and return the final agent response\n",
        "- Returns a fallback message (\"No response from agent.\") if no response is found\n",
        "- Automatically generates OpenTelemetry traces during execution for metric evaluation\n",
        "\n",
        "This function simplifies the process of running queries and ensures trace generation during execution, which is essential for evaluating the all metrics (LatencyMetric, TrajectoryEvalWithLLM, TrajectoryEvalWithLLMWithReference, ToolCallAccuracy, AgentGoalAccuracy, UsageMetric)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_turn(runner: Runner, query: str, session_id: str, user_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute a single-turn query with the agent and return the final response.\n",
        "    The agent execution is traced automatically.\n",
        "    \"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Extract the final response\n",
        "    for event in events:\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            return event.content.parts[0].text\n",
        "    return \"No response from agent.\"\n",
        "\n",
        "print(\"âœ“ Helper function 'run_single_turn' defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Define Queries for Different Metrics\n",
        "\n",
        "### Purpose\n",
        "Define sample queries for different metrics that will be executed by the Flotorch ADK agent to generate OpenTelemetry traces for evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`queries`**: A dictionary containing sample questions for each metric type\n",
        "  - **`\"trajectory\"`**: A query for TrajectoryEvalWithLLM metric that triggers the agent to search and provide information (e.g., \"Tell me about Google ADK\")\n",
        "  - **`\"reference\"`**: A query for TrajectoryEvalWithLLMWithReference metric that requires code review/analysis (e.g., \"Review this Python function and tell me if it works and how good the code quality is\")\n",
        "  - **`\"toolcall\"`**: A query for ToolCallAccuracy metric that requires the agent to use a specific tool (e.g., \"what is the weather in the hyderabad\")\n",
        "  - Each query will trigger agent execution that is automatically traced to capture execution information\n",
        "  - Different queries are used to ensure each metric is evaluated with the most appropriate scenario\n",
        "\n",
        "The queries can be modified to test different scenarios and measure performance for various types of questions. Each query generates a separate trace that is used for evaluating its corresponding metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define queries for different metrics\n",
        "queries = {\n",
        "    \"latency\": \"How to create agents in flotorch console?\",\n",
        "    \"trajectory\": \"Tell me about Google ADK\",\n",
        "    \"reference\": \"\"\"Review this Python function and tell me if it works and how good the code quality is.\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\"\"\",\n",
        "    \"toolcall\": \"what is the weather in the hyderabad\",\n",
        "    \"goal\": \"Plan a 5-day trip to Singapore from Bangalore in March, including flight options, hotel recommendations, daily itinerary, and estimated total cost.\",\n",
        "    \"usage\": \"Get me the top 7 latest news articles from around the world, especially from India\"\n",
        "}\n",
        "\n",
        "print(f\"âœ“ Queries defined for {len(queries)} metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Run Queries and Get Trace IDs\n",
        "\n",
        "### Purpose\n",
        "Execute sample queries for different metrics to generate OpenTelemetry traces. All queries use the same agent and each query execution generates a separate trace ID that will be used for metric evaluation.\n",
        "\n",
        "### Functionality\n",
        "This section:\n",
        "- Creates a `trace_ids` dictionary to store trace IDs for each metric\n",
        "- Iterates through each query in the `queries` dictionary (6 queries total: latency, trajectory, reference, toolcall, goal, usage)\n",
        "- For each query:\n",
        "  - Creates a new session using the session service\n",
        "  - Tracks existing tracer IDs before executing the query\n",
        "  - Executes the query using the agent via `run_single_turn` function\n",
        "  - Waits 4 seconds for trace generation to complete\n",
        "  - Compares tracer IDs after execution to identify new trace IDs\n",
        "  - Stores the most recent new trace ID in the `trace_ids` dictionary\n",
        "- Verifies that all 6 required trace IDs are collected successfully:\n",
        "  - `latency` - for Latency Metric\n",
        "  - `trajectory` - for Trajectory Evaluation with LLM\n",
        "  - `reference` - for Trajectory Evaluation with Reference\n",
        "  - `toolcall` - for Tool Call Accuracy\n",
        "  - `goal` - for Agent Goal Accuracy\n",
        "  - `usage` - for Usage Metric\n",
        "- Prints warnings if any trace IDs are missing\n",
        "\n",
        "The trace IDs are essential for evaluation as they contain the complete execution information needed by each metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Store trace IDs for each metric\n",
        "trace_ids = {}\n",
        "\n",
        "# Run queries and collect trace IDs\n",
        "for metric_name, query in queries.items():\n",
        "    print(f\"\\n--- Running query for {metric_name.upper()} metric ---\")\n",
        "    \n",
        "    # Create session and execute query\n",
        "    session = await runner.session_service.create_session(\n",
        "        app_name=app_name, \n",
        "        user_id=user_id\n",
        "    )\n",
        "    tracer_ids_before = set(agent_client.get_tracer_ids() or [])\n",
        "    \n",
        "    response = run_single_turn(\n",
        "        runner=runner, \n",
        "        query=query, \n",
        "        session_id=session.id, \n",
        "        user_id=user_id\n",
        "    )\n",
        "    \n",
        "    # Wait for trace generation and collect trace ID\n",
        "    time.sleep(4)\n",
        "    tracer_ids_after = agent_client.get_tracer_ids() or []\n",
        "    new_trace_ids = [tid for tid in tracer_ids_after if tid not in tracer_ids_before]\n",
        "    \n",
        "    if new_trace_ids:\n",
        "        trace_ids[metric_name] = new_trace_ids[-1]\n",
        "        print(f\"âœ“ {metric_name.upper()} trace ID: {trace_ids[metric_name]}\")\n",
        "    else:\n",
        "        print(f\"âœ— No trace ID found for {metric_name}\")\n",
        "\n",
        "# Verify all required trace IDs collected\n",
        "required_keys = [\"latency\", \"trajectory\", \"reference\", \"toolcall\", \"goal\", \"usage\"]\n",
        "missing = [k for k in required_keys if k not in trace_ids]\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n WARNING: Missing trace IDs: {missing}\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ All {len(trace_ids)} trace IDs collected successfully!\")\n",
        "    for name, tid in trace_ids.items():\n",
        "        print(f\"  - {name}: {tid}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Reference Trajectory Setup :\n",
        " `(For TrajectoryEvalWithLLMWithReference)`\n",
        "\n",
        "### Key Components\n",
        "- **`REFERENCE_TRAJECTORY`**: A dictionary that defines the ideal trajectory for the agent\n",
        "  - **`input`**: The user's query or prompt that will trigger the agent's response\n",
        "  - **`expected_steps`**: A list of expected steps the agent should take, where each step contains:\n",
        "    - **`thought`**: The agent's reasoning or thought process at this step\n",
        "    - **`final_response`**: The expected final response from the agent\n",
        "  - Example: A code review query with expected analysis covering correctness, code quality, readability, performance, and suggestions\n",
        "\n",
        "- **`ReferenceTrajectory`**: Schema validation class that ensures the reference trajectory structure is correct\n",
        "  - Validates that all required fields are present and properly formatted\n",
        "  - Used to ensure the reference trajectory meets the requirements for TrajectoryEvalWithLLMWithReference metric\n",
        "\n",
        "This reference trajectory will be compared against the actual agent execution to evaluate how closely the agent follows the expected behavior and reasoning path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define reference trajectory for TrajectoryEvalWithLLMWithReference metric\n",
        "REFERENCE_TRAJECTORY = {\n",
        "    \"input\": \"\"\"Review this Python function and tell me if it works and how good the code quality is.\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\"\"\",\n",
        "    \"expected_steps\": [\n",
        "        {\n",
        "            \"thought\": \"The user wants a review of the Python function, focusing on whether it works and the quality of the implementation. I will analyze correctness, edge cases, style, performance, and then provide a structured review.\",\n",
        "            \"final_response\": \"\"\"### Summary\n",
        "\n",
        "- The function `add_items` sums all elements in the `items` list.\n",
        "- It likely works correctly when `items` is a list of numeric values.\n",
        "- The implementation is not very idiomatic Python and can be simplified.\n",
        "- There is no validation for invalid or unexpected input types.\n",
        "\n",
        "### Correctness\n",
        "\n",
        "- For a typical case like `add_items([1, 2, 3])`, the function will return `6`, which is correct.\n",
        "- If `items` contains non-numeric values (e.g., strings or `None`), it will raise a `TypeError` when trying to add them.\n",
        "- If `items` is not indexable or does not implement `__len__` (e.g., a generator), the function will also fail.\n",
        "- There is no explicit handling of `None` or other invalid inputs.\n",
        "\n",
        "### Code Quality & Readability\n",
        "\n",
        "- The logic is straightforward and easy to understand.\n",
        "- Using `range(len(items))` is not idiomatic Python for iterating over a list.\n",
        "- A more Pythonic version would iterate directly over the elements or use the built-in `sum` function.\n",
        "- Variable naming is acceptable, but the function could be shorter and clearer.\n",
        "\n",
        "### Performance\n",
        "\n",
        "- Time complexity is O(n), where n is the length of `items`, which is optimal for summing a list.\n",
        "- There is a minor overhead from indexing (`items[i]`) instead of iterating directly over the elements.\n",
        "- For most real-world cases, this overhead is negligible, but the idiomatic version is both clearer and slightly more efficient.\n",
        "\n",
        "### Improved Version\n",
        "\n",
        "A more Pythonic and concise implementation would be:\n",
        "\n",
        "```python\n",
        "def add_items(items):\n",
        "    return sum(items)\n",
        "```\n",
        "\n",
        "Ratings\n",
        "Correctness: 8/10 (works for standard numeric lists, no validation)\n",
        "Code Quality: 6/10 (clear but non-idiomatic and slightly verbose)\n",
        "Overall: 7/10\"\"\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "validated_ref = ReferenceTrajectory(**REFERENCE_TRAJECTORY)\n",
        "print(\"âœ“ Reference trajectory defined for TrajectoryEvalWithLLMWithReference metric\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Initialize All Metrics\n",
        "\n",
        "### Purpose\n",
        "Initialize all metrics that will be evaluated. Each metric is configured with its specific requirements and will be evaluated using the traces collected from the agent execution.\n",
        "\n",
        "### Key Components\n",
        "- **`all_metrics`**: A list containing instances of all 6 metrics\n",
        "  - **`LatencyMetric()`**: Metric that measures agent response time and performance breakdown\n",
        "  - **`TrajectoryEvalWithLLM()`**: Metric that assesses overall trajectory quality using LLM-based evaluation\n",
        "  - **`TrajectoryEvalWithLLMWithReference()`**: Metric that compares trajectory against a reference trajectory using LLM-based evaluation\n",
        "    - Requires `default_evaluator` (LLM model) for evaluation\n",
        "    - Requires `MetricConfig` with `reference` parameter set to the `REFERENCE_TRAJECTORY` defined earlier\n",
        "  - **`ToolCallAccuracy()`**: Metric that evaluates the accuracy and appropriateness of tool usage decisions\n",
        "  - **`AgentGoalAccuracy()`**: Metric that assesses whether the agent successfully accomplished the user's true goal\n",
        "  - **`UsageMetric()`**: Metric that provides detailed breakdown of token usage and costs\n",
        "\n",
        "All metrics are initialized together so they can be evaluated in a unified manner using the same evaluator and traces. The code prints each metric name for verification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ALL metrics at once\n",
        "all_metrics = [\n",
        "    LatencyMetric(),\n",
        "    TrajectoryEvalWithLLM(),\n",
        "    TrajectoryEvalWithLLMWithReference(\n",
        "        llm=default_evaluator,\n",
        "        config=MetricConfig(\n",
        "            metric_params={\"reference\": REFERENCE_TRAJECTORY}\n",
        "        )\n",
        "    ),\n",
        "    ToolCallAccuracy(),\n",
        "    AgentGoalAccuracy(),\n",
        "    UsageMetric()\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Initialized {len(all_metrics)} metrics:\")\n",
        "for i, metric in enumerate(all_metrics, 1):\n",
        "    print(f\"  {i}. {metric.__class__.__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Configure Metric-to-Trace Mapping\n",
        "\n",
        "### Purpose\n",
        "Define the mapping between each metric and its corresponding trace. Each metric uses a different trace (from different queries) to ensure accurate evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`metric_configs`**: A list of dictionaries, each mapping a metric to its trace. Contains 6 metric configurations:\n",
        "  - **`name`**: Human-readable metric name\n",
        "  - **`trace_key`**: Key from the `trace_ids` dictionary that identifies which trace to use\n",
        "  - **`metric`**: The metric instance (created fresh for each configuration)\n",
        "  - **`reference`**: Reference trajectory (only for TrajectoryEvalWithLLMWithReference, otherwise None)\n",
        "\n",
        "**Metric-to-Trace Mapping**:\n",
        "1. **Latency Metric** â†’ Uses `\"latency\"` trace (Flotorch console question)\n",
        "2. **Trajectory Evaluation** â†’ Uses `\"trajectory\"` trace (Google ADK question)\n",
        "3. **Trajectory Evaluation with Reference** â†’ Uses `\"reference\"` trace (code review question)\n",
        "   - Includes `REFERENCE_TRAJECTORY` in the reference field\n",
        "4. **Tool Call Accuracy** â†’ Uses `\"toolcall\"` trace (weather question)\n",
        "5. **Agent Goal Accuracy** â†’ Uses `\"goal\"` trace (travel planning question)\n",
        "6. **Usage Metric** â†’ Uses `\"usage\"` trace (news question)\n",
        "\n",
        "This mapping ensures each metric is evaluated with the most appropriate trace for accurate assessment. The code also imports `EvaluationResult` schema which will be used to combine all evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate ALL metrics at once, each with its own trace\n",
        "# Note: The evaluate() method accepts ONE trace per call, so we evaluate each metric separately\n",
        "\n",
        "from flotorch_eval.agent_eval.core.schemas import EvaluationResult\n",
        "\n",
        "# Define metric-to-trace mapping and metric instances\n",
        "metric_configs = [\n",
        "    {\n",
        "        \"name\": \"Latency\",\n",
        "        \"trace_key\": \"latency\",\n",
        "        \"metric\": LatencyMetric(),\n",
        "        \"reference\": None\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Trajectory\",\n",
        "        \"trace_key\": \"trajectory\",\n",
        "        \"metric\": TrajectoryEvalWithLLM(),\n",
        "        \"reference\": None\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Reference\",\n",
        "        \"trace_key\": \"reference\",\n",
        "        \"metric\": TrajectoryEvalWithLLMWithReference(\n",
        "            llm=default_evaluator,\n",
        "            config=MetricConfig(metric_params={\"reference\": REFERENCE_TRAJECTORY})\n",
        "        ),\n",
        "        \"reference\": REFERENCE_TRAJECTORY\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ToolCall\",\n",
        "        \"trace_key\": \"toolcall\",\n",
        "        \"metric\": ToolCallAccuracy(),\n",
        "        \"reference\": None\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Goal\",\n",
        "        \"trace_key\": \"goal\",\n",
        "        \"metric\": AgentGoalAccuracy(),\n",
        "        \"reference\": None\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Usage\",\n",
        "        \"trace_key\": \"usage\",\n",
        "        \"metric\": UsageMetric(),\n",
        "        \"reference\": None\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Evaluating ALL metrics, each with its corresponding trace...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Run Evaluation for All Metrics\n",
        "\n",
        "### Purpose\n",
        "Evaluate all 6 metrics, each with its own trace (different question). The `evaluate()` method accepts ONE trace per call, so we loop through each metric and evaluate it separately with its corresponding trace, then combine all results.\n",
        "\n",
        "### Functionality\n",
        "This section:\n",
        "- Initializes empty lists: `all_scores` to collect all metric scores and `all_trajectory_ids` to track trajectory IDs\n",
        "- Iterates through each metric configuration in `metric_configs` (6 total metrics)\n",
        "- For each metric:\n",
        "  - Checks if the required trace ID exists, skipping with a warning if missing\n",
        "  - Fetches the trace for the metric using `evaluator.fetch_traces(trace_id)`\n",
        "  - Prepares evaluation arguments with the trace and metric instance\n",
        "  - Adds reference trajectory for metrics that require it (TrajectoryEvalWithLLMWithReference)\n",
        "  - Evaluates the metric using `evaluator.evaluate()` with error handling\n",
        "  - Extends `all_scores` with the result scores and appends trajectory ID info\n",
        "  - Prints success or failure messages for each metric\n",
        "- After all evaluations:\n",
        "  - Creates a combined `EvaluationResult` object with all scores if any were collected\n",
        "  - Uses the first trajectory ID from the collected IDs or \"combined\" as fallback\n",
        "  - Sets timestamp to current UTC time\n",
        "  - Prints summary message with total number of metrics evaluated successfully\n",
        "\n",
        "The evaluation results are stored in `all_results` for display in the next section. Each metric is evaluated independently, so if one fails, others can still complete successfully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_scores = []\n",
        "all_trajectory_ids = []\n",
        "\n",
        "# Evaluate each metric with its own trace\n",
        "for i, config in enumerate(metric_configs, 1):\n",
        "    trace_key = config[\"trace_key\"]\n",
        "    metric_name = config[\"name\"]\n",
        "    \n",
        "    if trace_key not in trace_ids:\n",
        "        print(f\" Skipping {metric_name}: No trace ID for '{trace_key}'\")\n",
        "        continue\n",
        "    \n",
        "    trace_id = trace_ids[trace_key]\n",
        "    print(f\"{i}. Evaluating {metric_name} Metric...\")\n",
        "    \n",
        "    # Fetch and evaluate\n",
        "    trace = evaluator.fetch_traces(trace_id)\n",
        "    eval_kwargs = {\"trace\": trace, \"metrics\": [config[\"metric\"]]}\n",
        "    \n",
        "    if config[\"reference\"]:\n",
        "        eval_kwargs[\"reference\"] = config[\"reference\"]\n",
        "    \n",
        "    try:\n",
        "        result = await evaluator.evaluate(**eval_kwargs)\n",
        "        all_scores.extend(result.scores)\n",
        "        all_trajectory_ids.append((metric_name, result.trajectory_id))\n",
        "        print(f\"   âœ“ {metric_name} Metric completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âœ— {metric_name} Metric failed: {str(e)[:100]}\")\n",
        "\n",
        "# Create combined results\n",
        "if all_scores:\n",
        "    from datetime import datetime\n",
        "    all_results = EvaluationResult(\n",
        "        trajectory_id=all_trajectory_ids[0][1] if all_trajectory_ids else \"combined\",\n",
        "        timestamp=datetime.utcnow(),\n",
        "        scores=all_scores\n",
        "    )\n",
        "    print(f\"\\nâœ“ All {len(all_scores)} metrics evaluated successfully!\")\n",
        "else:\n",
        "    print(\"âœ— No metrics evaluated. Please check trace IDs.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Display All Metrics Results\n",
        "\n",
        "### Purpose\n",
        "Define helper function to format and display the evaluation output clearly, showing all three metric results in a readable format.\n",
        "\n",
        "### Functionality\n",
        "The `display_all_metrics` function:\n",
        "- Extracts all three metrics from the evaluation results (`trajectory_evaluation`, `trajectory_evaluation_with_reference`, `toolcall_accuracy`)\n",
        "- Formats the score and details for each metric type appropriately\n",
        "- Creates a structured display showing:\n",
        "  - Metric name\n",
        "  - Score (0.0 to 1.0 scale)\n",
        "  - Detailed evaluation results for each metric\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize all three metric evaluation results in a single formatted table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_all_metrics(result):\n",
        "    \"\"\"\n",
        "    Display ALL metrics in a single formatted table.\n",
        "    This function handles all 6 metrics and formats them appropriately.\n",
        "    \"\"\"\n",
        "    if not result or not result.scores:\n",
        "        print(\"No metrics found in results.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Trajectory ID: {result.trajectory_id}\")\n",
        "    print(f\"Timestamp: {result.timestamp}\\n\")\n",
        "    \n",
        "    # Prepare data for all metrics\n",
        "    metrics_data = []\n",
        "    \n",
        "    for metric in result.scores:\n",
        "        metric_name = metric.name\n",
        "        details_text = \"\"\n",
        "        \n",
        "        # Format details based on metric type\n",
        "        if metric_name == \"latency_summary\":\n",
        "            d = metric.details\n",
        "            breakdown = \"\\n\".join(\n",
        "                f\"    - {s['step_name']}: {s['latency_ms']} ms\"\n",
        "                for s in d.get(\"latency_breakdown\", [])\n",
        "            )\n",
        "            details_text = (\n",
        "                f\"Total Latency: {d.get('total_latency_ms')} ms\\n\"\n",
        "                f\"Average Step Latency: {d.get('average_step_latency_ms')} ms\\n\"\n",
        "                f\"Breakdown:\\n{breakdown}\"\n",
        "            )\n",
        "        \n",
        "        elif metric_name == \"trajectory_evaluation\":\n",
        "            d = metric.details\n",
        "            details_text = f\"Score: {metric.score:.2f} / 1.0\\n\\nDetails:\\n{d.get('details', 'No details available.')}\"\n",
        "        \n",
        "        elif metric_name == \"trajectory_evaluation_with_reference\":\n",
        "            details_text = \"\\n\".join(f\"{k}: {v}\" for k, v in metric.details.items())\n",
        "        \n",
        "        elif metric_name == \"toolcall_accuracy\":\n",
        "            d = metric.details\n",
        "            details_text = f\"Score: {metric.score:.2f} / 1.0\\n\\nDetails:\\n{d.get('details', 'No details available.')}\"\n",
        "        \n",
        "        elif metric_name == \"agent_goal_accuracy\":\n",
        "            raw_details = metric.details.get(\"details\", \"{}\")\n",
        "            try:\n",
        "                parsed = json.loads(raw_details)\n",
        "                details_text = \"\\n\".join(f\"{k}: {v}\" for k, v in parsed.items())\n",
        "            except:\n",
        "                details_text = raw_details\n",
        "        \n",
        "        elif metric_name == \"usage_summary\":\n",
        "            d = metric.details\n",
        "            total_cost = d.get(\"total_cost\", \"0.000000\")\n",
        "            avg_cost = d.get(\"average_cost_per_call\", \"0.000000\")\n",
        "            cost_breakdown = d.get(\"cost_breakdown\", [])\n",
        "            \n",
        "            if cost_breakdown:\n",
        "                breakdown_lines = []\n",
        "                for item in cost_breakdown:\n",
        "                    if isinstance(item, dict):\n",
        "                        item_str = f\"    - {item.get('operation', 'Unknown')}: ${item.get('cost', '0.000000')}\"\n",
        "                        if 'count' in item:\n",
        "                            item_str += f\" ({item['count']} calls)\"\n",
        "                        breakdown_lines.append(item_str)\n",
        "                    else:\n",
        "                        breakdown_lines.append(f\"    - {item}\")\n",
        "                breakdown_text = \"\\n\".join(breakdown_lines)\n",
        "            else:\n",
        "                breakdown_text = \"    No breakdown available.\"\n",
        "            \n",
        "            details_text = (\n",
        "                f\"Total Cost: ${total_cost}\\n\"\n",
        "                f\"Average Cost per Call: ${avg_cost}\\n\"\n",
        "                f\"Cost Breakdown:\\n{breakdown_text}\"\n",
        "            )\n",
        "        \n",
        "        else:\n",
        "            # Generic formatting for unknown metrics\n",
        "            details_text = str(metric.details)\n",
        "        \n",
        "        # Format score based on metric type\n",
        "        if metric_name == \"usage_summary\":\n",
        "            score_display = f\"${metric.details.get('total_cost', '0.000000')}\"\n",
        "        else:\n",
        "            score_display = f\"{metric.score:.2f}\"\n",
        "        \n",
        "        metrics_data.append({\n",
        "            \"Metric\": metric_name.replace(\"_\", \" \").title(),\n",
        "            \"Score\": score_display,\n",
        "            \"Details\": details_text\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame and display\n",
        "    df = pd.DataFrame(metrics_data)\n",
        "    \n",
        "    display(df.style.set_properties(\n",
        "        subset=['Details'],\n",
        "        **{'white-space': 'pre-wrap', 'text-align': 'left'}\n",
        "    ).set_table_styles([\n",
        "        {'selector': 'th', 'props': [('text-align', 'left')]}\n",
        "    ]))\n",
        "\n",
        "# Display all metrics\n",
        "if 'all_results' in locals():\n",
        "    display_all_metrics(all_results)\n",
        "else:\n",
        "    print(\"No results found. Please run the evaluation section first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "This comprehensive notebook demonstrates how to evaluate Flotorch ADK agents across **all 6 evaluation metrics** using a **unified approach**:\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Single Agent**: ONE `FlotorchADKAgent` with all necessary tools (KB, Web Search, Weather, News)\n",
        "2. **Single Evaluator**: ONE `AgentEvaluator` instance for all metrics\n",
        "3. **Unified Evaluation**: All 6 metrics evaluated at once in a single call\n",
        "4. **Unified Display**: ONE display function shows all metric results together\n",
        "\n",
        "## Metrics Evaluated\n",
        "\n",
        "1. **Latency Metric** (`latency_summary`) - Performance and response time analysis\n",
        "2. **Trajectory Evaluation with LLM** (`trajectory_evaluation`) - Overall trajectory quality assessment\n",
        "3. **Trajectory Evaluation with Reference** (`trajectory_evaluation_with_reference`) - Comparison against reference trajectories\n",
        "4. **Tool Call Accuracy** (`toolcall_accuracy`) - Tool usage decision evaluation\n",
        "5. **Agent Goal Accuracy** (`agent_goal_accuracy`) - Goal accomplishment assessment\n",
        "6. **Usage Metric** (`usage_summary`) - Token usage and cost analysis\n",
        "\n",
        "## Benefits\n",
        "\n",
        "- **Efficiency**: Single agent and evaluator reduce setup overhead\n",
        "- **Consistency**: All metrics use the same agent configuration\n",
        "- **Simplicity**: One evaluation call and one display function\n",
        "- **Comprehensive**: All metrics displayed together for easy comparison\n",
        "\n",
        "The notebook provides comprehensive evaluation capabilities for monitoring and optimizing Flotorch ADK agents across multiple dimensions of performance and quality.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
