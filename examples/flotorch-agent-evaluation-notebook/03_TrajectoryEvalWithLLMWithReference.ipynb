{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "nRViePVanlPR",
      "metadata": {
        "id": "nRViePVanlPR"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/1IrwoNrb3AWLAhAqjlAkJNYa39p9eT9ui?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0202c9",
      "metadata": {
        "id": "5e0202c9"
      },
      "source": [
        "# Flotorch Agent Trajectory Evaluation with Reference (Code Review Agent Use Case)\n",
        "\n",
        "This notebook demonstrates how to measure and analyze the **trajectory evaluation with reference** of a **Flotorch ADK agent** (configured as a **Code Review Agent** that analyzes code for issues and suggests improvements and optimizations) using the **Flotorch Eval** framework.\n",
        "\n",
        "The evaluation relies on **OpenTelemetry Traces** generated during the agent's run to assess the overall quality and effectiveness of the agent's trajectory by comparing it against a reference trajectory using LLM-based evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "* **Code Review Agent**: An agent designed to analyze code for issues and suggest improvements and optimizations.\n",
        "* **OpenTelemetry Traces**: Detailed records of the agent's execution steps (spans) used to analyze the complete agent trajectory.\n",
        "* **TrajectoryEvalWithLLMWithReference**: A Flotorch Eval metric that uses LLM-based evaluation to assess trajectory quality by comparing against a reference trajectory. The evaluation metric used is **trajectory_evaluation_with_reference**.\n",
        "* **Reference Trajectory**: A predefined expected trajectory that serves as a benchmark for evaluating the agent's actual performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/03_TrajectoryEvalWithLLMWithReference_Workflow_Diagram.drawio.png)\n",
        "*Figure : Detailed workflow diagram showing the step-by-step process of trajectory evaluation with reference from agent execution through trace collection to metric computation.*\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models.\n",
        "* Valid Flotorch API key and gateway base URL.\n",
        "* Agent configured with OpenTelemetry tracing enabled.\n",
        "* Reference trajectory definition for comparison.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d70391a",
      "metadata": {},
      "source": [
        "## Agent Setup in Flotorch Console\n",
        "\n",
        "**Important**: Before running this notebook, you need to create an agent in the Flotorch Console. This section provides step-by-step instructions on how to set up the agent.\n",
        "\n",
        "### Step 1: Access Flotorch Console\n",
        "\n",
        "1. **Log in to Flotorch Console**:\n",
        "   - Navigate to your Flotorch Console (e.g., `https://dev-console.flotorch.cloud`)\n",
        "   - Ensure you have the necessary permissions to create agents\n",
        "\n",
        "2. **Navigate to Agents Section**:\n",
        "   - Click on **\"Agents\"** in the left sidebar\n",
        "   - You should see the \"Agent Builder\" option selected\n",
        "\n",
        "### Step 2: Create New Agent\n",
        "\n",
        "1. **Click \"Create FloTorch Agent\"**:\n",
        "   - Look for the blue **\"+ Create FloTorch Agent\"** button in the top right corner\n",
        "   - Click it to start creating a new agent\n",
        "\n",
        "2. **Agent Configuration**:\n",
        "   - **Agent Name**: Choose a unique name for your agent (e.g., `code-reviewer-agent`)\n",
        "     - **Important**: The name should only contain alphanumeric characters and dashes (a-z, A-Z, 0-9, -)\n",
        "     - **Note**: Copy this agent name - you'll need to use it in the `agent_name` variable later\n",
        "   - **Description** (Optional): Add a description if desired\n",
        "\n",
        "### Step 3: Configure Agent Details\n",
        "\n",
        "After creating the agent, you'll be directed to the agent configuration page. Configure the following:\n",
        "\n",
        "#### Required Configuration:\n",
        "\n",
        "1. **Model** (`* Model`):\n",
        "   - **Required**: Select a model from the available options\n",
        "   - Example: `gpt-model` or any available model from your Flotorch gateway\n",
        "   - Click the edit icon to configure\n",
        "\n",
        "2. **Agent Details** (`* Agent Details`):\n",
        "   - **Required**: Configure agent details\n",
        "   - **System Prompt**: Copy and paste the following system prompt:\n",
        "\n",
        "You are SeniorCodeReviewer, an expert software engineer.\n",
        "Your job is to review any code the user provides and give:\n",
        "\n",
        "Whether the code likely works (based on static analysis; do not claim certainty).\n",
        "\n",
        "The quality of the code: readability, maintainability, structure, performance, and security.\n",
        "\n",
        "A numeric rating of the code (Correctness / Quality / Overall, each out of 10).\n",
        "\n",
        "Actionable improvements and best-practice suggestions.\n",
        "\n",
        "Guidelines:\n",
        "\n",
        "Be direct, technical, and professional.\n",
        "\n",
        "Point out bugs, edge cases, smells, and anti-patterns.\n",
        "\n",
        "Suggest better patterns or small rewritten snippets only when useful.\n",
        "\n",
        "Follow language-appropriate conventions (PEP8 for Python, etc.).\n",
        "\n",
        "Output in clear sections: Summary, Correctness, Quality, Performance, Security (if relevant), Ratings, Improvements, Suggested Tests.\n",
        "\n",
        "\n",
        "   - **Goal**: Copy and paste the following goal:\n",
        "   \n",
        "Review any given source code and provide an accurate assessment of whether it is likely to work as intended, how good its quality is (readability, maintainability, performance, security), a numeric rating, and specific, actionable improvement suggestions.\n",
        "\n",
        "\n",
        "#### Optional Configuration:\n",
        "\n",
        "1. **Tools**:\n",
        "   - Tools will be added programmatically via the notebook (see Section 8)\n",
        "   - You can leave this as \"Not Configured\" in the console\n",
        "\n",
        "2. **Input Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "3. **Output Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "### Step 4: Publish the Agent\n",
        "\n",
        "1. **Review Configuration**:\n",
        "   - Ensure the Model and Agent Details are configured correctly\n",
        "   - Verify the System Prompt and Goal are set\n",
        "\n",
        "2. **Publish Agent**:\n",
        "   - After configuration, click **\"Publish\"** or **\"Make a revision\"** to publish the agent\n",
        "   - Once published, the agent will have a version number (e.g., v1)\n",
        "\n",
        "3. **Note the Agent Name**:\n",
        "   - **Important**: Copy the exact agent name you used when creating the agent\n",
        "   - You will need to replace `<your_agent_name>` in the `agent_name` variable in Section 2.1 (Global Provider Models and Agent Configuration)\n",
        "\n",
        "### Step 5: Update Notebook Configuration\n",
        "\n",
        "1. **Update Agent Name**:\n",
        "   - Navigate to Section 2.1 in this notebook\n",
        "   - Find the `agent_name` variable\n",
        "   - Replace `<your_agent_name>` with the exact agent name you created in the console\n",
        "\n",
        "**Example**:\n",
        "- If you created an agent named `code-reviewer-agent` in the console\n",
        "- Set `agent_name = \"code-reviewer-agent\"` in the notebook\n",
        "\n",
        "### Summary of Required vs Optional Settings\n",
        "\n",
        "| Setting | Required/Optional | Value |\n",
        "|---------|------------------|-------|\n",
        "| **Agent Name** | **Required** | Choose a unique name (copy it for notebook) |\n",
        "| **Model** | **Required** | Select from available models |\n",
        "| **System Prompt** | **Required** | Use the system prompt provided above |\n",
        "| **Goal** | **Required** | Use the goal provided above |\n",
        "| **Tools** | **Optional** | Will be added via notebook code |\n",
        "| **Input Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "| **Output Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "\n",
        "**Note**: The tools (Knowledge Base, Web Search, Weather, News) will be added to the agent programmatically in the notebook code, so you don't need to configure them manually in the console.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87732d23",
      "metadata": {
        "id": "87732d23"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for agent trajectory evaluation with reference.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies for trajectory evaluation with reference metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hsL4kCgINxXB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsL4kCgINxXB",
        "outputId": "3ffd4c45-17ce-4ad6-ee28-c27dcba7a947"
      },
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dd94ce",
      "metadata": {
        "id": "56dd94ce"
      },
      "source": [
        "## 2.Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29373306",
      "metadata": {
        "id": "29373306"
      },
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  \n",
        "    print(f\"✓ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"✗ FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint          || https://dev-console.flotorch.cloud\n",
        "print(f\"✓ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"✓ All credentials configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5ead4f",
      "metadata": {
        "id": "0f5ead4f"
      },
      "source": [
        "### 2.1. Global Provider Models and Agent Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure agent-specific parameters.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation and agent operations:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Agent Configuration Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n",
        "| `agent_name` | The name of your Flotorch ADK agent | `code-reviewer-agent` |\n",
        "| `app_name` | The application name identifier | `agent-evaluation-app-name_03` |\n",
        "| `user_id` | The user identifier | `agent-evaliation-user-03` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf8ab66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bf8ab66",
        "outputId": "ffc6173d-24c3-45bc-d477-5abb364c7826"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation, agent operations, and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"✓ Global provider models defined\")\n",
        "\n",
        "# The LLM model used for evaluation.\n",
        "# Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "# You can use your own models from Flotorch Console as well\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU\n",
        "\n",
        "agent_name = \"<your_agent_name>\"  # The name of your Flotorch ADK agent                                        || ex : code-reviewer-agent\n",
        "app_name = \"<your_app_name>\"  # The application name identifier                                                || ex : agent-evaluation-app-name_03\n",
        "user_id = \"<your_user_id>\"  # The user identifier                                                              || ex : agent-evaliation-user-03\n",
        "\n",
        "print(\"✓ Agent Configuration Parameter defined \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SayrKjcEN1Nc",
      "metadata": {
        "id": "SayrKjcEN1Nc"
      },
      "outputs": [],
      "source": [
        "FLOTORCH_API_KEY=\"sk_MEB9a2iaB0pXf2P8IVjldtw8OvLgwRo5ReXgdbOYKpA=_MTAwNjNjNDQtYjk3YS00NjdhLWFkYTgtZWJkYzU0OTFlNjY5_OTFiYWY4ZDQtNWFjNy00YWJmLWI3OGYtODMxZWFlOWY1ZWY5\"\n",
        "FLOTORCH_BASE_URL=\"https://dev-gateway.flotorch.cloud/\"\n",
        "agent_name=\"code-reviewer-agent\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99963dc0",
      "metadata": {
        "id": "99963dc0"
      },
      "source": [
        "## 3. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating the Code Review Agent trajectory with reference using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`TrajectoryEvalWithLLMWithReference`**: Flotorch Eval metric that uses LLM-based evaluation to assess trajectory quality by comparing against a reference trajectory\n",
        "- **`ReferenceTrajectory`**: Schema for defining reference trajectories for comparison\n",
        "- **`MetricConfig`**: Configuration for metric parameters including reference trajectory\n",
        "- **`FlotorchADKAgent`**: Creates and configures Flotorch ADK agents with tracing\n",
        "- **`FlotorchADKSession`**: Manages agent sessions for multi-turn conversations\n",
        "- **`Runner`**: Executes agent queries and coordinates the agent execution flow\n",
        "- **`types`**: Google ADK types for creating message content and handling agent events\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5345ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5345ba",
        "outputId": "fe0eda5b-af12-4847-fb99-991d15e9061f"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.llm_evaluators import TrajectoryEvalWithLLMWithReference\n",
        "from flotorch_eval.agent_eval.core.schemas import ReferenceTrajectory\n",
        "from flotorch_eval import MetricConfig\n",
        "\n",
        "# Flotorch ADK components\n",
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "\n",
        "# Google ADK components\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"✓ Imported necessary libraries successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d236c2",
      "metadata": {
        "id": "33d236c2"
      },
      "source": [
        "## 4. Code Review Agent Setup\n",
        "\n",
        "### Purpose\n",
        "Set up the Code Review Agent with OpenTelemetry tracing enabled to capture detailed execution data for trajectory evaluation with reference.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Initializes the agent for code review tasks\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Essential for evaluation as traces contain complete trajectory information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "\n",
        "These components work together to run the Code Review Agent and generate OpenTelemetry traces for trajectory evaluation with reference analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ea47ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ea47ea",
        "outputId": "da8d219a-0424-4ce7-ec95-c021700d8733"
      },
      "outputs": [],
      "source": [
        "# Initialize Flotorch ADK Agent with tracing enabled\n",
        "agent_client = FlotorchADKAgent(\n",
        "    agent_name=agent_name,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    tracer_config={\n",
        "        \"enabled\": True,                                                   # Enable tracing for latency measurement\n",
        "        \"endpoint\": \"https://dev-observability.flotorch.cloud/v1/traces\",  # Dev observability OTLP HTTP endpoint (used by QA)\n",
        "        \"sampling_rate\": 1                                                 # Sample 100% of traces\n",
        "    }\n",
        ")\n",
        "agent = agent_client.get_agent()\n",
        "\n",
        "# Initialize session service\n",
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        ")\n",
        "\n",
        "# Create the ADK Runner to execute agent queries\n",
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=app_name,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "print(\"✓ Agent and runner and session initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed6bc1e",
      "metadata": {
        "id": "7ed6bc1e"
      },
      "source": [
        "## 5. Helper Function for Running a Query\n",
        "\n",
        "### Purpose\n",
        "Define a helper function that executes a single-turn query with the agent and extracts the final response. The agent execution is automatically traced for trajectory evaluation.\n",
        "\n",
        "### Functionality\n",
        "The `run_single_turn` function:\n",
        "- Accepts a `Runner`, query string, session ID, and user ID as parameters\n",
        "- Creates a user message using Google ADK types\n",
        "- Executes the query through the runner\n",
        "- Iterates through events to find and return the final agent response\n",
        "- Returns a fallback message if no response is found\n",
        "\n",
        "This function simplifies the process of running queries and ensures trace generation during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51ef384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51ef384",
        "outputId": "7badc79e-a0ac-4438-b68f-b7e2738235cd"
      },
      "outputs": [],
      "source": [
        "def run_single_turn(runner: Runner, query: str, session_id: str, user_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute a single-turn query with the agent and return the final response.\n",
        "    The agent execution is traced automatically.\n",
        "    \"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Extract the final response\n",
        "    for event in events:\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            return event.content.parts[0].text\n",
        "    return \"No response from agent.\"\n",
        "\n",
        "print(\"✓ Helper function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8e77a8",
      "metadata": {
        "id": "9c8e77a8"
      },
      "source": [
        "## 6. Define Query\n",
        "\n",
        "### Purpose\n",
        "Define the sample code review query that will be executed by the Code Review Agent to generate OpenTelemetry traces for trajectory evaluation with reference.\n",
        "\n",
        "### Key Components\n",
        "- **`query`**: A sample code review request that will be processed by the agent\n",
        "  - This query contains a Python function that needs to be reviewed\n",
        "  - The query will trigger the agent to analyze the code, check correctness, evaluate code quality, and provide feedback\n",
        "  - The execution will be automatically traced to capture the complete agent trajectory\n",
        "  - The trajectory will be evaluated using LLM-based assessment against a reference trajectory to measure quality and effectiveness\n",
        "  - Example: A code review request with a Python function to analyze\n",
        "\n",
        "The query can be modified to test different code review scenarios and evaluate trajectory quality against reference trajectories for various types of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc59094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fc59094",
        "outputId": "23c43f1e-befd-4eb0-b237-54da29736713"
      },
      "outputs": [],
      "source": [
        "# Execute the query to generate traces\n",
        "\n",
        "query = \"\"\"Review this Python function and tell me if it works and how good the code quality is.\"\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\"\"\"\n",
        "\n",
        "print(f\"✓ Query defined: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5742fdf",
      "metadata": {
        "id": "c5742fdf"
      },
      "source": [
        "## 7. Run the Query and Get Trace ID\n",
        "\n",
        "### Purpose\n",
        "Execute a sample code review query with the Code Review Agent to generate OpenTelemetry traces that contain trajectory data for evaluation.\n",
        "\n",
        "### Process\n",
        "1. **Create Session**: Initialize a new session for the agent interaction\n",
        "2. **Execute Query**: Run a sample code review query through the agent\n",
        "3. **Retrieve Trace IDs**: Extract the generated trace IDs from the agent client\n",
        "4. **Display Results**: Print the agent response and trace ID for verification\n",
        "\n",
        "The execution automatically generates OpenTelemetry traces that record the complete agent trajectory, which will be used for trajectory evaluation with reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98ad20c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98ad20c",
        "outputId": "09d85e00-8808-4825-f2e4-bd569526302c"
      },
      "outputs": [],
      "source": [
        "# Create a new session\n",
        "session = await runner.session_service.create_session(\n",
        "    app_name=app_name,\n",
        "    user_id=user_id,\n",
        ")\n",
        "print(f\"Session created: {session.id}\")\n",
        "\n",
        "response = run_single_turn(\n",
        "    runner=runner,\n",
        "    query=query,\n",
        "    session_id=session.id,\n",
        "    user_id=user_id\n",
        ")\n",
        "\n",
        "# Retrieve the generated trace IDs\n",
        "trace_ids = agent_client.get_tracer_ids()\n",
        "print(\"Agent Response:\")\n",
        "print(response[:200] + \"...\" if len(response) > 200 else response)\n",
        "print(f\"Found {len(trace_ids)} trace(s). First trace ID: {trace_ids[0] if trace_ids else 'N/A'}\")\n",
        "print(f\"✓ Query execution completed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8ad6f0",
      "metadata": {
        "id": "8a8ad6f0"
      },
      "source": [
        "## 8. Define Reference Trajectory\n",
        "\n",
        "### Purpose\n",
        "Define a reference trajectory that represents the expected behavior for the code review task. This reference serves as a benchmark for evaluating the agent's actual performance.\n",
        "\n",
        "### Key Components\n",
        "1. **Reference Trajectory Structure**:\n",
        "   - **`input`**: The expected input query (code review request)\n",
        "   - **`expected_steps`**: Array of expected steps, each containing:\n",
        "     - **`thought`**: Expected reasoning/thought process for the step\n",
        "     - **`final_response`**: Expected final response structure and content\n",
        "2. **ReferenceTrajectory Schema**: Validates the reference trajectory structure using the `ReferenceTrajectory` schema\n",
        "3. **Validation**: Ensures the reference trajectory is properly formatted before use in evaluation\n",
        "\n",
        "This reference trajectory will be used by the TrajectoryEvalWithLLMWithReference metric to compare the agent's actual trajectory against the expected behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e95bdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6e95bdf",
        "outputId": "e4a7998b-5dab-4e7f-80f4-e3f3e0c5ec5f"
      },
      "outputs": [],
      "source": [
        "REFERENCE_TRAJECTORY = {\n",
        "    \"input\": \"\"\"Review this Python function and tell me if it works and how good the code quality is.\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\n",
        "\"\"\",\n",
        "    \"expected_steps\": [\n",
        "        {\n",
        "            \"thought\": \"The user wants a review of the Python function, focusing on whether it works and the quality of the implementation. I will analyze correctness, edge cases, style, performance, and then provide a structured review.\",\n",
        "            \"final_response\": \"\"\"### Summary\n",
        "\n",
        "- The function `add_items` sums all elements in the `items` list.\n",
        "- It likely works correctly when `items` is a list of numeric values.\n",
        "- The implementation is not very idiomatic Python and can be simplified.\n",
        "- There is no validation for invalid or unexpected input types.\n",
        "\n",
        "### Correctness\n",
        "\n",
        "- For a typical case like `add_items([1, 2, 3])`, the function will return `6`, which is correct.\n",
        "- If `items` contains non-numeric values (e.g., strings or `None`), it will raise a `TypeError` when trying to add them.\n",
        "- If `items` is not indexable or does not implement `__len__` (e.g., a generator), the function will also fail.\n",
        "- There is no explicit handling of `None` or other invalid inputs.\n",
        "\n",
        "### Code Quality & Readability\n",
        "\n",
        "- The logic is straightforward and easy to understand.\n",
        "- Using `range(len(items))` is not idiomatic Python for iterating over a list.\n",
        "- A more Pythonic version would iterate directly over the elements or use the built-in `sum` function.\n",
        "- Variable naming is acceptable, but the function could be shorter and clearer.\n",
        "\n",
        "### Performance\n",
        "\n",
        "- Time complexity is O(n), where n is the length of `items`, which is optimal for summing a list.\n",
        "- There is a minor overhead from indexing (`items[i]`) instead of iterating directly over the elements.\n",
        "- For most real-world cases, this overhead is negligible, but the idiomatic version is both clearer and slightly more efficient.\n",
        "\n",
        "### Improved Version\n",
        "\n",
        "A more Pythonic and concise implementation would be:\n",
        "\n",
        "```python\n",
        "def add_items(items):\n",
        "    return sum(items)\n",
        "If you want to be safer about input types, you could add basic validation:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def add_items(items):\n",
        "    if items is None:\n",
        "        return 0\n",
        "    return sum(items)\n",
        "Ratings\n",
        "Correctness: 8/10 (works for standard numeric lists, no validation)\n",
        "\n",
        "Code Quality: 6/10 (clear but non-idiomatic and slightly verbose)\n",
        "\n",
        "Overall: 7/10\"\"\"\n",
        "}\n",
        "]\n",
        "}\n",
        "\n",
        "validated_ref = ReferenceTrajectory(**REFERENCE_TRAJECTORY)\n",
        "print(validated_ref)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45d0ab5",
      "metadata": {
        "id": "c45d0ab5"
      },
      "source": [
        "## 9. Trajectory Evaluation with Reference\n",
        "\n",
        "### Purpose\n",
        "Initialize the `AgentEvaluator`, define a reference trajectory, fetch the OpenTelemetry trace, and run the `TrajectoryEvalWithLLMWithReference` metric to evaluate trajectory quality against a reference. The evaluation metric **trajectory_evaluation_with_reference** provides comprehensive assessment of the Code Review Agent's trajectory compared to an expected reference.\n",
        "\n",
        "### Key Components\n",
        "1. **Reference Trajectory**: Defines the expected trajectory with input, expected steps, thoughts, and final response\n",
        "2. **TrajectoryEvalWithLLMWithReference**: Initializes the trajectory evaluation metric that uses LLM-based evaluation to compare actual trajectory against reference\n",
        "3. **AgentEvaluator** (`client`):\n",
        "   - Connects to the Flotorch gateway using API credentials\n",
        "   - Configured with a default evaluator model\n",
        "   - Provides methods to fetch and evaluate traces\n",
        "4. **Trace Fetching**: Retrieves the complete trace data using the trace ID generated during agent execution\n",
        "\n",
        "The fetched trace contains detailed information about the complete agent trajectory, which will be analyzed by the TrajectoryEvalWithLLMWithReference metric to compute the trajectory_evaluation_with_reference score by comparing against the reference trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c963e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c963e4",
        "outputId": "5386f4b4-1f7b-45b8-946b-e043a2c498b4"
      },
      "outputs": [],
      "source": [
        "# Initialize the TrajectoryEvalWithLLMWithReference metric with reference trajectory\n",
        "metrics = [TrajectoryEvalWithLLMWithReference(\n",
        "    llm=default_evaluator,\n",
        "    config=MetricConfig(\n",
        "        metric_params={\"reference\": REFERENCE_TRAJECTORY}\n",
        "    )\n",
        ")]\n",
        "\n",
        "# Initialize the AgentEvaluator client\n",
        "client = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "\n",
        "traces = None\n",
        "if trace_ids:\n",
        "    # Fetch the trace data from the Flotorch gateway\n",
        "    traces = client.fetch_traces(trace_ids[0])\n",
        "    print(f\"✓ Trace fetched successfully\")\n",
        "else:\n",
        "    print(\"✗ No trace IDs found to fetch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218e8814",
      "metadata": {
        "id": "218e8814"
      },
      "source": [
        "## 10. Run Evaluation\n",
        "\n",
        "### Purpose\n",
        "Execute the trajectory evaluation with reference by processing the fetched OpenTelemetry trace using the TrajectoryEvalWithLLMWithReference metric to assess trajectory quality against the reference.\n",
        "\n",
        "### Process\n",
        "- Calls `client.evaluate()` with the trace data, TrajectoryEvalWithLLMWithReference metric, and reference trajectory\n",
        "- The evaluator processes the trace to analyze the complete agent trajectory\n",
        "- Compares the actual trajectory against the reference trajectory\n",
        "- Computes the **trajectory_evaluation_with_reference** metric which includes:\n",
        "  - Quality score (0.0 to 1.0) indicating how well the trajectory matches the reference\n",
        "  - Detailed LLM-based evaluation explanation comparing actual vs. expected trajectory\n",
        "  - Assessment of alignment with reference in terms of reasoning, tool usage, and response quality\n",
        "- Returns evaluation results with trajectory ID and metric scores\n",
        "\n",
        "This step generates the trajectory evaluation with reference analysis that will be displayed in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d832fc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d832fc0",
        "outputId": "172905f0-f4fc-4d30-a4ec-2cd4ea749918"
      },
      "outputs": [],
      "source": [
        "if traces:\n",
        "    # Evaluate the trace using the TrajectoryEvalWithLLMWithReference metric\n",
        "    results = await client.evaluate(\n",
        "        trace=traces,\n",
        "        metrics=metrics,\n",
        "        reference=REFERENCE_TRAJECTORY\n",
        "    )\n",
        "\n",
        "    print(\"✓ Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Cannot evaluate: No traces were available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e8da4c",
      "metadata": {
        "id": "35e8da4c"
      },
      "source": [
        "## 11. Display and Interpret Results\n",
        "\n",
        "### Purpose\n",
        "Define helper functions to format and display the evaluation output clearly, showing the trajectory_evaluation_with_reference metric results in a readable format.\n",
        "\n",
        "### Functionality\n",
        "The `display_metrics` function:\n",
        "- Extracts the `trajectory_evaluation_with_reference` metric from evaluation results\n",
        "- Formats the quality score and evaluation details\n",
        "- Creates a structured display showing:\n",
        "  - Trajectory Quality Score (0.0 to 1.0) compared to reference\n",
        "  - Detailed LLM-based evaluation explanation comparing actual vs. expected trajectory\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize trajectory evaluation with reference metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a11561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a11561",
        "outputId": "f25e47c4-9f60-4ecf-f200-2661aa73a125"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def display_metrics(result):\n",
        "    \"\"\"\n",
        "    Display only the 'trajectory_evaluation_with_reference' metric.\n",
        "    \"\"\"\n",
        "    print(f\"Trajectory ID: {result.trajectory_id}\")\n",
        "    print(f\"Timestamp    : {result.timestamp}\\n\")\n",
        "\n",
        "    # Find the metric\n",
        "    metric = next(\n",
        "        (m for m in result.scores if m.name == \"trajectory_evaluation_with_reference\"),\n",
        "        None\n",
        "    )\n",
        "    if not metric:\n",
        "        print(\"Metric 'trajectory_evaluation_with_reference' not found.\")\n",
        "        return\n",
        "\n",
        "    # Format details (simple key:value lines)\n",
        "    details = \"\\n\".join(f\"{k}: {v}\" for k, v in metric.details.items())\n",
        "\n",
        "    # Build DataFrame\n",
        "    df = pd.DataFrame([{\n",
        "        \"Metric\": metric.name,\n",
        "        \"Score\": f\"{metric.score:.2f}\",\n",
        "        \"Details\": details\n",
        "    }])\n",
        "\n",
        "    # Display with multiline support\n",
        "    display(\n",
        "        df.style.set_properties(\n",
        "            subset=[\"Details\"],\n",
        "            **{\"white-space\": \"pre-wrap\", \"text-align\": \"left\"}\n",
        "        )\n",
        "    )\n",
        "\n",
        "print(\"✓ Display metrics function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27de8dc3",
      "metadata": {
        "id": "27de8dc3"
      },
      "source": [
        "## 12. View Trajectory Evaluation Results\n",
        "\n",
        "### Purpose\n",
        "Display the trajectory evaluation with reference results in a formatted table showing the complete assessment for the Code Review Agent.\n",
        "\n",
        "### Output\n",
        "The displayed table includes:\n",
        "- **Metric**: The evaluation metric name (trajectory_evaluation_with_reference)\n",
        "- **Score**: The trajectory quality score (0.0 to 1.0) compared to reference\n",
        "- **Details**: Comprehensive evaluation showing:\n",
        "  - Quality score out of 1.0\n",
        "  - Detailed LLM-based explanation comparing actual vs. expected trajectory\n",
        "  - Assessment of alignment with reference in reasoning, tool usage, and response quality\n",
        "\n",
        "This visualization helps identify trajectory quality issues and optimize the agent's code review capabilities to better match expected behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d055ba70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "d055ba70",
        "outputId": "b61649b6-6066-4b60-dffd-a82623920f06"
      },
      "outputs": [],
      "source": [
        "if 'results' in locals():\n",
        "    display_metrics(results)\n",
        "else:\n",
        "    print(\"No results object found. Please run sections 5 and 6 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0401dec6",
      "metadata": {
        "id": "0401dec6"
      },
      "source": [
        "### Interpreting the Trajectory Evaluation with Reference Results\n",
        "\n",
        "The **trajectory_evaluation_with_reference** metric is a vital tool for quality monitoring of the Code Review Agent:\n",
        "\n",
        "* **Quality Score (0.0 to 1.0)**: Indicates how well the agent's trajectory matches the reference trajectory:\n",
        "    * **1.0**: Excellent alignment - trajectory closely matches reference in reasoning, analysis, and response quality\n",
        "    * **0.5-0.9**: Good alignment with minor deviations from reference in some aspects\n",
        "    * **0.0-0.4**: Poor alignment - significant differences from reference in reasoning or response quality\n",
        "* **Evaluation Details**: Provides a detailed LLM-based explanation of:\n",
        "    * **Reasoning Alignment**: Whether the agent's thought process matches the expected reasoning in the reference\n",
        "    * **Analysis Quality**: Assessment of how well the code analysis compares to the reference (correctness, code quality, performance analysis)\n",
        "    * **Response Quality**: Evaluation of how well the final response matches the reference in structure, completeness, and accuracy\n",
        "    * **Overall Trajectory Comparison**: Comprehensive evaluation of how the actual trajectory aligns with the expected reference trajectory\n",
        "\n",
        "For a Code Review Agent, understanding trajectory evaluation with reference helps identify:\n",
        "- **Reasoning gaps**: If the agent's analysis process deviates from expected code review methodology\n",
        "- **Analysis completeness**: If the agent misses important aspects (correctness, code quality, performance) compared to the reference\n",
        "- **Response structure**: If the agent's response format and organization differ from the expected reference format\n",
        "- **Overall effectiveness**: Monitor trajectory alignment to ensure the agent delivers comprehensive and structured code reviews that match expected quality standards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c3b6be",
      "metadata": {
        "id": "a3c3b6be"
      },
      "source": [
        "## 13. Summary of Agent Trajectory Evaluation with Reference Notebook\n",
        "\n",
        "This notebook demonstrates the professional methodology for evaluating the trajectory quality of a **Flotorch ADK Agent** (configured as a **Code Review Agent** that analyzes code for issues and suggests improvements and optimizations) using the **Flotorch Eval framework** with reference trajectory comparison.\n",
        "\n",
        "**Use Case**: Code Review Agent - Analyzes code for issues and suggests improvements and optimizations.\n",
        "\n",
        "**Evaluation Metric**: trajectory_evaluation_with_reference\n",
        "\n",
        "## Core Process\n",
        "\n",
        "### 1. Setup and Instrumentation\n",
        "- Configure a `FlotorchADKAgent` for code review tasks.\n",
        "- Enable **OpenTelemetry Tracing** via the `tracer_config`.\n",
        "- This instrumentation allows detailed capture of the complete agent trajectory and decision-making process.\n",
        "\n",
        "### 2. Reference Trajectory Definition\n",
        "- Define a reference trajectory that represents the expected behavior for the code review task.\n",
        "- Include expected input, thought process, and final response structure.\n",
        "- This reference serves as a benchmark for evaluating the agent's actual performance.\n",
        "\n",
        "### 3. Execution and Data Generation\n",
        "- Run a sample code review query through the agent using the **Runner**.\n",
        "- This automatically generates an **Agent Trajectory** in the form of OpenTelemetry traces.\n",
        "- The trace records the complete execution path, including:\n",
        "  - Code analysis and reasoning\n",
        "  - Issue identification and categorization\n",
        "  - Improvement suggestions and optimizations\n",
        "  - Step-by-step agent operations\n",
        "\n",
        "### 4. Evaluation\n",
        "- Use the `AgentEvaluator` client along with the specialized **TrajectoryEvalWithLLMWithReference** metric from `flotorch-eval`.\n",
        "- The evaluator processes the trace data to compute trajectory quality statistics using the **trajectory_evaluation_with_reference** metric by comparing against the reference trajectory.\n",
        "\n",
        "### 5. Analysis\n",
        "- The notebook displays a thorough trajectory evaluation assessment, including:\n",
        "  - **Quality Score** (0.0 to 1.0) compared to reference\n",
        "  - **LLM-based Evaluation Details** explaining how the actual trajectory compares to the reference\n",
        "  - Assessment of alignment in reasoning, analysis quality, and response structure\n",
        "\n",
        "## Purpose and Benefits\n",
        "\n",
        "This evaluation provides **actionable quality metrics** that help developers:\n",
        "\n",
        "- Identify trajectory quality issues in the Code Review Agent compared to expected behavior  \n",
        "- Optimize code review methodology to better match reference standards  \n",
        "- Track quality trends over time  \n",
        "- Ensure the Code Review Agent delivers **comprehensive and structured code reviews** that match expected quality standards via reference trajectory comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ade6ebd",
      "metadata": {
        "id": "8ade6ebd"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
