{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ddBxWhCnWvq",
      "metadata": {
        "id": "6ddBxWhCnWvq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/1IrwoNrb3AWLAhAqjlAkJNYa39p9eT9ui?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0202c9",
      "metadata": {
        "id": "5e0202c9"
      },
      "source": [
        "# Flotorch Agent Trajectory Evaluation (Intelligent Research Assistant Use Case)\n",
        "\n",
        "This notebook demonstrates how to measure and analyze the **trajectory evaluation** of a **Flotorch ADK agent** (configured as an **Intelligent Research Assistant** that performs web-based research and synthesizes information using search APIs) using the **Flotorch Eval** framework.\n",
        "\n",
        "The evaluation relies on **OpenTelemetry Traces** generated during the agent's run to assess the overall quality and effectiveness of the agent's trajectory through LLM-based evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "* **Intelligent Research Assistant**: An agent designed to perform web-based research and synthesize information using search APIs.\n",
        "* **OpenTelemetry Traces**: Detailed records of the agent's execution steps (spans) used to analyze the complete agent trajectory.\n",
        "* **TrajectoryEvalWithLLM**: A Flotorch Eval metric that uses LLM-based evaluation to assess the overall quality and effectiveness of agent trajectories. The evaluation metric used is **trajectory_evaluation_with_llm**.\n",
        "\n",
        "---\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/02_TrajectoryEvalWithLLM_Workflow_Diagram.drawio.png)\n",
        "*Figure 2: Detailed workflow diagram showing the step-by-step process of Trajectory eval with llm evaluation from agent execution through trace collection to metric computation.*\n",
        "\n",
        "---\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models.\n",
        "* Valid Flotorch API key and gateway base URL.\n",
        "* Agent configured with OpenTelemetry tracing enabled.\n",
        "* External API access for web search (e.g., Google Custom Search API).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1feed71",
      "metadata": {},
      "source": [
        "## Agent Setup in Flotorch Console\n",
        "\n",
        "**Important**: Before running this notebook, you need to create an agent in the Flotorch Console. This section provides step-by-step instructions on how to set up the agent.\n",
        "\n",
        "### Step 1: Access Flotorch Console\n",
        "\n",
        "1. **Log in to Flotorch Console**:\n",
        "   - Navigate to your Flotorch Console (e.g., `https://dev-console.flotorch.cloud`)\n",
        "   - Ensure you have the necessary permissions to create agents\n",
        "\n",
        "2. **Navigate to Agents Section**:\n",
        "   - Click on **\"Agents\"** in the left sidebar\n",
        "   - You should see the \"Agent Builder\" option selected\n",
        "\n",
        "### Step 2: Create New Agent\n",
        "\n",
        "1. **Click \"Create FloTorch Agent\"**:\n",
        "   - Look for the blue **\"+ Create FloTorch Agent\"** button in the top right corner\n",
        "   - Click it to start creating a new agent\n",
        "\n",
        "2. **Agent Configuration**:\n",
        "   - **Agent Name**: Choose a unique name for your agent (e.g., `research-agent`)\n",
        "     - **Important**: The name should only contain alphanumeric characters and dashes (a-z, A-Z, 0-9, -)\n",
        "     - **Note**: Copy this agent name - you'll need to use it in the `agent_name` variable later\n",
        "   - **Description** (Optional): Add a description if desired\n",
        "\n",
        "### Step 3: Configure Agent Details\n",
        "\n",
        "After creating the agent, you'll be directed to the agent configuration page. Configure the following:\n",
        "\n",
        "#### Required Configuration:\n",
        "\n",
        "1. **Model** (`* Model`):\n",
        "   - **Required**: Select a model from the available options\n",
        "   - Example: `gpt-model` or any available model from your Flotorch gateway\n",
        "   - Click the edit icon to configure\n",
        "\n",
        "2. **Agent Details** (`* Agent Details`):\n",
        "   - **Required**: Configure agent details\n",
        "   - **System Prompt**: Copy and paste the following system prompt:\n",
        "\n",
        "you are the helpful assistant.  you need to call the web_search tool when the user ask about anything. you need give more context about the user required data.  give context in the bullet points minimum 20 bullet points.\n",
        "\n",
        "Available Tools:\n",
        "web_search\n",
        "\n",
        "\n",
        "   - **Goal**: Copy and paste the following goal:\n",
        "\n",
        "you are the helpful assistant.  you need to call the web_search tool when the user ask about anything.\n",
        "\n",
        "\n",
        "#### Optional Configuration:\n",
        "\n",
        "1. **Tools**:\n",
        "   - Tools will be added programmatically via the notebook (see Section 8)\n",
        "   - You can leave this as \"Not Configured\" in the console\n",
        "\n",
        "2. **Input Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "3. **Output Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "### Step 4: Publish the Agent\n",
        "\n",
        "1. **Review Configuration**:\n",
        "   - Ensure the Model and Agent Details are configured correctly\n",
        "   - Verify the System Prompt and Goal are set\n",
        "\n",
        "2. **Publish Agent**:\n",
        "   - After configuration, click **\"Publish\"** or **\"Make a revision\"** to publish the agent\n",
        "   - Once published, the agent will have a version number (e.g., v1)\n",
        "\n",
        "3. **Note the Agent Name**:\n",
        "   - **Important**: Copy the exact agent name you used when creating the agent\n",
        "   - You will need to replace `<your_agent_name>` in the `agent_name` variable in Section 2.1 (Global Provider Models and Agent Configuration)\n",
        "\n",
        "### Step 5: Update Notebook Configuration\n",
        "\n",
        "1. **Update Agent Name**:\n",
        "   - Navigate to Section 2.1 in this notebook\n",
        "   - Find the `agent_name` variable\n",
        "   - Replace `<your_agent_name>` with the exact agent name you created in the console\n",
        "\n",
        "**Example**:\n",
        "- If you created an agent named `research-agent` in the console\n",
        "- Set `agent_name = \"research-agent\"` in the notebook\n",
        "\n",
        "### Summary of Required vs Optional Settings\n",
        "\n",
        "| Setting | Required/Optional | Value |\n",
        "|---------|------------------|-------|\n",
        "| **Agent Name** | **Required** | Choose a unique name (copy it for notebook) |\n",
        "| **Model** | **Required** | Select from available models |\n",
        "| **System Prompt** | **Required** | Use the system prompt provided above |\n",
        "| **Goal** | **Required** | Use the goal provided above |\n",
        "| **Tools** | **Optional** | Will be added via notebook code |\n",
        "| **Input Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "| **Output Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "\n",
        "**Note**: The tools (Knowledge Base, Web Search, Weather, News) will be added to the agent programmatically in the notebook code, so you don't need to configure them manually in the console.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87732d23",
      "metadata": {
        "id": "87732d23"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for agent trajectory evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies for trajectory metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c01cb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9c01cb9",
        "outputId": "4444614d-bb87-4914-830a-e928b8539603"
      },
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dd94ce",
      "metadata": {
        "id": "56dd94ce"
      },
      "source": [
        "## 2.Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3791a166",
      "metadata": {
        "id": "3791a166"
      },
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \") \n",
        "    print(f\"âœ“ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"âœ— FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # https://dev-gateway.flotorch.cloud\n",
        "print(f\"âœ“ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"âœ“ All credentials configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63cea92",
      "metadata": {
        "id": "f63cea92"
      },
      "source": [
        "### 2.1. Global Provider Models and Agent Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure agent-specific parameters.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation and agent operations:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Agent Configuration Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n",
        "| `agent_name` | The name of your Flotorch ADK agent | `research-agent` |\n",
        "| `app_name` | The application name identifier | `agent-evaluation-app-name_02` |\n",
        "| `user_id` | The user identifier | `agent-evaliation-user-02` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d863c918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d863c918",
        "outputId": "995e03b2-10b8-4abc-af98-a54e23bb3baf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation, agent operations, and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"âœ“ Global provider models defined\")\n",
        "\n",
        "# The LLM model used for evaluation.\n",
        "# Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "# You can use your own models from Flotorch Console as well\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU\n",
        "\n",
        "agent_name = \"<your_agent_name>\"  # The name of your Flotorch ADK agent                                        || ex : research-agent\n",
        "app_name = \"<your_app_name>\"  # The application name identifier                                                || ex : agent-evaluation-app-name_02\n",
        "user_id = \"<your_user_id>\"  # The user identifier                                                              || ex : agent-evaliation-user-02\n",
        "\n",
        "print(\"âœ“ Agent Configuration Parameter defined \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99963dc0",
      "metadata": {
        "id": "99963dc0"
      },
      "source": [
        "## 3. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating the Intelligent Research Assistant trajectory using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`TrajectoryEvalWithLLM`**: Flotorch Eval metric that uses LLM-based evaluation to assess overall trajectory quality\n",
        "- **`FlotorchADKAgent`**: Creates and configures Flotorch ADK agents with custom tools and tracing\n",
        "- **`FlotorchADKSession`**: Manages agent sessions for multi-turn conversations\n",
        "- **`Runner`**: Executes agent queries and coordinates the agent execution flow\n",
        "- **`FunctionTool`**: Wraps Python functions as tools that can be used by the agent\n",
        "- **`types`**: Google ADK types for creating message content and handling agent events\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5345ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5345ba",
        "outputId": "5366ef1c-130b-4bc8-b715-0f6662658880"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.llm_evaluators import TrajectoryEvalWithLLM\n",
        "\n",
        "# Flotorch ADK components\n",
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "\n",
        "# Google ADK components\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.genai import types\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"âœ“ Imported necessary libraries successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d236c2",
      "metadata": {
        "id": "33d236c2"
      },
      "source": [
        "## 4. Intelligent Research Assistant Setup\n",
        "\n",
        "### Purpose\n",
        "Set up the Intelligent Research Assistant with OpenTelemetry tracing enabled to capture detailed execution data for trajectory evaluation.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Initializes the agent for performing web-based research and synthesizing information using search APIs\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Essential for evaluation as traces contain complete trajectory information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "\n",
        "These components work together to run the Intelligent Research Assistant and generate OpenTelemetry traces for trajectory evaluation analysis.\n",
        "\n",
        "### Custom Tool: Web Search API Integration\n",
        "\n",
        "The Intelligent Research Assistant uses a custom tool (`web_search`) that integrates with external search APIs to retrieve and synthesize information. This tool:\n",
        "- Accepts a search query string as input\n",
        "- Uses Google Custom Search API to perform web searches\n",
        "- Retrieves top search results with titles, snippets, and links\n",
        "- Returns formatted search results for information synthesis\n",
        "- Handles errors gracefully with fallback responses\n",
        "\n",
        "The tool is wrapped as a `FunctionTool` that can be used by the agent to perform web-based research and synthesize information using search APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882c2118",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882c2118",
        "outputId": "97ff1aea-691e-4492-8f9b-41ee968076e8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Perform a Google search and return top results.\"\"\"\n",
        "    api_key = \"<GOOGLE_API_KEY>\" # Enter your google api key || ex : AIzaSyA_jSk0x7mubtDTo..........\n",
        "    cse_id = \"<GOOGLE CSE_ID>\"   # Enter your google cse_id  || ex : 77ef93ba66............\n",
        "\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        \"key\": api_key,\n",
        "        \"cx\": cse_id,\n",
        "        \"q\": query,\n",
        "        \"num\": 5  # top 5 results\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"items\" not in data:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    results = []\n",
        "    for item in data[\"items\"]:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        snippet = item.get(\"snippet\", \"\")\n",
        "        link = item.get(\"link\", \"\")\n",
        "        results.append(f\"ðŸ”¹ {title}\\n{snippet}\\n{link}\")\n",
        "\n",
        "    # Combine results into a single text output\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "# --- Wrap as ADK Tool ---\n",
        "tools = [FunctionTool(func=web_search)]\n",
        "\n",
        "print(\"âœ“ Web search tool defined and registered successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c42916b",
      "metadata": {
        "id": "6c42916b"
      },
      "source": [
        "## 5. Agent and Runner Initialization\n",
        "\n",
        "### Purpose\n",
        "Set up the Flotorch ADK Agent and Runner with OpenTelemetry tracing enabled to capture detailed execution data for trajectory evaluation.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Initializes the agent with custom web search tools\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Essential for evaluation as traces contain complete trajectory information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "\n",
        "These components work together to run the Intelligent Research Assistant and generate OpenTelemetry traces for trajectory evaluation analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ea47ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ea47ea",
        "outputId": "16599653-005f-4c60-8c05-7db4a9bdf604"
      },
      "outputs": [],
      "source": [
        "# Initialize Flotorch ADK Agent with tracing enabled\n",
        "agent_client = FlotorchADKAgent(\n",
        "    agent_name=agent_name,\n",
        "    custom_tools=tools,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    tracer_config={\n",
        "        \"enabled\": True,                                                   # Enable tracing for trajectory evaluation\n",
        "        \"endpoint\": \"https://dev-observability.flotorch.cloud/v1/traces\",  # Dev observability OTLP HTTP endpoint (used by QA)\n",
        "        \"sampling_rate\": 1                                                 # Sample 100% of traces\n",
        "    }\n",
        ")\n",
        "agent = agent_client.get_agent()\n",
        "\n",
        "# Initialize session service\n",
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        ")\n",
        "\n",
        "# Create the ADK Runner to execute agent queries\n",
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=app_name,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "print(\"âœ“ Agent and runner and session initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed6bc1e",
      "metadata": {
        "id": "7ed6bc1e"
      },
      "source": [
        "## 6. Helper Function for Running a Query\n",
        "\n",
        "### Purpose\n",
        "Define a helper function that executes a single-turn query with the agent and extracts the final response. The agent execution is automatically traced for trajectory evaluation.\n",
        "\n",
        "### Functionality\n",
        "The `run_single_turn` function:\n",
        "- Accepts a `Runner`, query string, session ID, and user ID as parameters\n",
        "- Creates a user message using Google ADK types\n",
        "- Executes the query through the runner\n",
        "- Iterates through events to find and return the final agent response\n",
        "- Returns a fallback message if no response is found\n",
        "\n",
        "This function simplifies the process of running queries and ensures trace generation during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51ef384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51ef384",
        "outputId": "fc6bf953-ba62-48bf-b271-a99fdc60d6d7"
      },
      "outputs": [],
      "source": [
        "def run_single_turn(runner: Runner, query: str, session_id: str, user_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute a single-turn query with the agent and return the final response.\n",
        "    The agent execution is traced automatically.\n",
        "    \"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Extract the final response\n",
        "    for event in events:\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            return event.content.parts[0].text\n",
        "    return \"No response from agent.\"\n",
        "\n",
        "print(\"âœ“ Helper function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37df1093",
      "metadata": {
        "id": "37df1093"
      },
      "source": [
        "## 7. Define Query\n",
        "\n",
        "### Purpose\n",
        "Define the sample query that will be executed by the Intelligent Research Assistant agent to generate OpenTelemetry traces for trajectory evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`query`**: A sample research question that will be processed by the agent\n",
        "  - This query will trigger the agent to perform web-based research using search APIs and synthesize information\n",
        "  - The execution will be automatically traced to capture the complete agent trajectory\n",
        "  - The trajectory will be evaluated using LLM-based assessment to measure quality and effectiveness\n",
        "  - Example: \"Tell me about Google ADK?\"\n",
        "\n",
        "The query can be modified to test different research scenarios and evaluate trajectory quality for various types of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bbcf68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0bbcf68",
        "outputId": "4ab64ebf-4d3d-4da5-e7d8-57c9db36b018"
      },
      "outputs": [],
      "source": [
        "# Execute the query to generate traces\n",
        "\n",
        "query = \"Tell me about Google ADK?\"\n",
        "\n",
        "print(f\"âœ“ Query defined: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5742fdf",
      "metadata": {
        "id": "c5742fdf"
      },
      "source": [
        "## 8. Run the Query and Get Trace ID\n",
        "\n",
        "### Purpose\n",
        "Execute a sample query with the Intelligent Research Assistant to generate OpenTelemetry traces that contain trajectory data for evaluation.\n",
        "\n",
        "### Process\n",
        "1. **Create Session**: Initialize a new session for the agent interaction\n",
        "2. **Execute Query**: Run a sample query through the agent\n",
        "3. **Retrieve Trace IDs**: Extract the generated trace IDs from the agent client\n",
        "4. **Display Results**: Print the agent response and trace ID for verification\n",
        "\n",
        "The execution automatically generates OpenTelemetry traces that record the complete agent trajectory, which will be used for trajectory evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98ad20c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98ad20c",
        "outputId": "ef6046a6-ea06-4291-b922-a4f9b1d79150"
      },
      "outputs": [],
      "source": [
        "# Create a new session\n",
        "session = await runner.session_service.create_session(\n",
        "    app_name=app_name,\n",
        "    user_id=user_id,\n",
        ")\n",
        "print(f\"Session created: {session.id}\")\n",
        "\n",
        "response = run_single_turn(\n",
        "    runner=runner,\n",
        "    query=query,\n",
        "    session_id=session.id,\n",
        "    user_id=user_id\n",
        ")\n",
        "\n",
        "# Retrieve the generated trace IDs\n",
        "trace_ids = agent_client.get_tracer_ids()\n",
        "print(trace_ids)\n",
        "print(\"Agent Response:\")\n",
        "print(response[:200] + \"...\" if len(response) > 200 else response)\n",
        "print(f\"Found {len(trace_ids)} trace(s). First trace ID: {trace_ids[0] if trace_ids else 'N/A'}\")\n",
        "\n",
        "print(f\"âœ“ Query execution completed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00df1d34",
      "metadata": {
        "id": "00df1d34"
      },
      "source": [
        "## 9. Trajectory Evaluation with Flotorch Eval\n",
        "\n",
        "### Purpose\n",
        "Initialize the `AgentEvaluator`, fetch the OpenTelemetry trace, and run the `TrajectoryEvalWithLLM` metric to evaluate overall trajectory quality. The evaluation metric **trajectory_evaluation_with_llm** provides comprehensive assessment of the Intelligent Research Assistant's trajectory.\n",
        "\n",
        "### Key Components\n",
        "1. **TrajectoryEvalWithLLM**: Initializes the trajectory evaluation metric that uses LLM-based evaluation to assess trajectory quality\n",
        "2. **AgentEvaluator** (`client`):\n",
        "   - Connects to the Flotorch gateway using API credentials\n",
        "   - Configured with a default evaluator model\n",
        "   - Provides methods to fetch and evaluate traces\n",
        "3. **Trace Fetching**: Retrieves the complete trace data using the trace ID generated during agent execution\n",
        "\n",
        "The fetched trace contains detailed information about the complete agent trajectory, which will be analyzed by the TrajectoryEvalWithLLM metric to compute the trajectory_evaluation_with_llm score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c963e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c963e4",
        "outputId": "5c55d0be-de17-4b82-973b-a895fcd25212"
      },
      "outputs": [],
      "source": [
        "# Initialize the TrajectoryEvalWithLLM metric\n",
        "metrics = [TrajectoryEvalWithLLM()]\n",
        "\n",
        "# Initialize the AgentEvaluator client\n",
        "client = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "\n",
        "traces = None\n",
        "if trace_ids:\n",
        "    # Fetch the trace data from the Flotorch gateway\n",
        "    traces = client.fetch_traces(trace_ids[0])\n",
        "    print(f\"âœ“ Trace fetched successfully\")\n",
        "else:\n",
        "    print(\"âœ— No trace IDs found to fetch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218e8814",
      "metadata": {
        "id": "218e8814"
      },
      "source": [
        "## 10. Run Evaluation\n",
        "\n",
        "### Purpose\n",
        "Execute the trajectory evaluation by processing the fetched OpenTelemetry trace using the TrajectoryEvalWithLLM metric to assess overall trajectory quality.\n",
        "\n",
        "### Process\n",
        "- Calls `client.evaluate()` with the trace data and TrajectoryEvalWithLLM metric\n",
        "- The evaluator processes the trace to analyze the complete agent trajectory\n",
        "- Computes the **trajectory_evaluation_with_llm** metric which includes:\n",
        "  - Quality score (0.0 to 1.0) indicating overall trajectory effectiveness\n",
        "  - Detailed LLM-based evaluation explanation of trajectory quality\n",
        "  - Assessment of tool usage, reasoning, and response quality\n",
        "- Returns evaluation results with trajectory ID and metric scores\n",
        "\n",
        "This step generates the trajectory evaluation analysis that will be displayed in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d832fc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d832fc0",
        "outputId": "1cc42700-bddb-4f83-aa08-42544723150d"
      },
      "outputs": [],
      "source": [
        "if 'traces' in locals() and traces:\n",
        "    # Evaluate the trace using the TrajectoryEvalWithLLM metric\n",
        "    results = await client.evaluate(\n",
        "        trace=traces,\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    print(\"âœ“ Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Cannot evaluate: No traces were available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e8da4c",
      "metadata": {
        "id": "35e8da4c"
      },
      "source": [
        "## 11. Display and Interpret Results\n",
        "\n",
        "### Purpose\n",
        "Define helper functions to format and display the evaluation output clearly, showing the trajectory_evaluation_with_llm metric results in a readable format.\n",
        "\n",
        "### Functionality\n",
        "The `display_metrics` function:\n",
        "- Extracts the `trajectory_evaluation` metric from evaluation results\n",
        "- Formats the quality score and evaluation details\n",
        "- Creates a structured display showing:\n",
        "  - Trajectory Quality Score (0.0 to 1.0)\n",
        "  - Detailed LLM-based evaluation explanation\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize trajectory evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a11561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a11561",
        "outputId": "080f070a-be06-4e98-cf95-648d026bb13c"
      },
      "outputs": [],
      "source": [
        "def display_metrics(result):\n",
        "    \"\"\"\n",
        "    Display trajectory evaluation metrics in a formatted table.\n",
        "    \"\"\"\n",
        "    # Find the trajectory_evaluation metric\n",
        "    metric = next((m for m in result.scores if m.name == \"trajectory_evaluation\"), None)\n",
        "    if not metric:\n",
        "        print(\"No trajectory_evaluation metric found.\")\n",
        "        return\n",
        "\n",
        "    # Extract metric details\n",
        "    d = metric.details\n",
        "\n",
        "    # Get the details string (which contains the evaluation explanation)\n",
        "    details_text = d.get(\"details\", \"No details available.\")\n",
        "\n",
        "    # Format the details string with better readability\n",
        "    details = f\"Score: {metric.score:.2f} / 1.0\\n\\nEvaluation Details:\\n{details_text}\"\n",
        "\n",
        "    # Create DataFrame for display\n",
        "    df = pd.DataFrame([{\n",
        "        \"Metric\": metric.name.replace(\"_\", \" \").title(),\n",
        "        \"Score\": f\"{metric.score:.2f}\",\n",
        "        \"Details\": details\n",
        "    }])\n",
        "\n",
        "    # Display DataFrame with multiline support\n",
        "    display(df.style.set_properties(\n",
        "        subset=['Details'],\n",
        "        **{'white-space': 'pre-wrap', 'text-align': 'left'}\n",
        "    ))\n",
        "\n",
        "print(\"âœ“ Display metrics function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27de8dc3",
      "metadata": {
        "id": "27de8dc3"
      },
      "source": [
        "## 12. View Trajectory Evaluation Results\n",
        "\n",
        "### Purpose\n",
        "Display the trajectory evaluation results in a formatted table showing the complete assessment for the Intelligent Research Assistant.\n",
        "\n",
        "### Output\n",
        "The displayed table includes:\n",
        "- **Metric**: The evaluation metric name (trajectory_evaluation)\n",
        "- **Score**: The trajectory quality score (0.0 to 1.0)\n",
        "- **Details**: Comprehensive evaluation showing:\n",
        "  - Quality score out of 1.0\n",
        "  - Detailed LLM-based explanation of trajectory quality\n",
        "  - Assessment of tool usage, reasoning, and response effectiveness\n",
        "\n",
        "This visualization helps identify trajectory quality issues and optimize the agent's research and synthesis capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d055ba70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "d055ba70",
        "outputId": "c2090dc3-1763-43a0-ca34-4904280fdf28"
      },
      "outputs": [],
      "source": [
        "if 'results' in locals():\n",
        "    display_metrics(results)\n",
        "else:\n",
        "    print(\"No results object found. Please run sections 5 and 6 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0401dec6",
      "metadata": {
        "id": "0401dec6"
      },
      "source": [
        "### Interpreting the Trajectory Evaluation Results\n",
        "\n",
        "The **trajectory_evaluation_with_llm** metric is a vital tool for quality monitoring of the Intelligent Research Assistant:\n",
        "\n",
        "* **Quality Score (0.0 to 1.0)**: Indicates the overall effectiveness and quality of the agent's trajectory:\n",
        "    * **1.0**: Excellent trajectory - effective tool usage, sound reasoning, and high-quality responses\n",
        "    * **0.5-0.9**: Good trajectory with minor issues in tool usage or response quality\n",
        "    * **0.0-0.4**: Poor trajectory - ineffective tool usage, poor reasoning, or low-quality responses\n",
        "* **Evaluation Details**: Provides a detailed LLM-based explanation of:\n",
        "    * **Tool Usage Effectiveness**: Whether tools were used appropriately and effectively (e.g., web search queries)\n",
        "    * **Reasoning Quality**: Assessment of the agent's reasoning and decision-making process\n",
        "    * **Response Quality**: Evaluation of the final response's accuracy, completeness, and usefulness\n",
        "    * **Overall Trajectory Assessment**: Comprehensive evaluation of the complete agent execution path\n",
        "\n",
        "For an Intelligent Research Assistant, understanding trajectory evaluation helps identify:\n",
        "- **Tool usage issues**: If web search tools are used ineffectively or queries are poorly formulated\n",
        "- **Reasoning problems**: If the agent's research and synthesis process has logical gaps\n",
        "- **Response quality**: If the synthesized information is incomplete, inaccurate, or poorly presented\n",
        "- **Overall effectiveness**: Monitor trajectory quality to ensure the agent delivers accurate and comprehensive research results via proper API integration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c3b6be",
      "metadata": {
        "id": "a3c3b6be"
      },
      "source": [
        "## 13. Summary of Agent Trajectory Evaluation Notebook\n",
        "\n",
        "This notebook demonstrates the professional methodology for evaluating the trajectory quality of a **Flotorch ADK Agent** (configured as an **Intelligent Research Assistant** that performs web-based research and synthesizes information using search APIs) using the **Flotorch Eval framework**.\n",
        "\n",
        "**Use Case**: Intelligent Research Assistant - Performs web-based research and synthesizes information using search APIs.\n",
        "\n",
        "**Evaluation Metric**: trajectory_evaluation_with_llm\n",
        "\n",
        "## Core Process\n",
        "\n",
        "### 1. Setup and Instrumentation\n",
        "- Configure a `FlotorchADKAgent` with custom web search tools (e.g., Google Custom Search API integration).\n",
        "- Enable **OpenTelemetry Tracing** via the `tracer_config`.\n",
        "- This instrumentation allows detailed capture of the complete agent trajectory and decision-making process.\n",
        "\n",
        "### 2. Execution and Data Generation\n",
        "- Run a sample query through the agent using the **Runner**.\n",
        "- This automatically generates an **Agent Trajectory** in the form of OpenTelemetry traces.\n",
        "- The trace records the complete execution path, including:\n",
        "  - Tool usage decisions (web search queries)\n",
        "  - LLM interactions and reasoning\n",
        "  - Information synthesis and response generation\n",
        "  - Step-by-step agent operations\n",
        "\n",
        "### 3. Evaluation\n",
        "- Use the `AgentEvaluator` client along with the specialized **TrajectoryEvalWithLLM** metric from `flotorch-eval`.\n",
        "- The evaluator processes the trace data to compute trajectory quality statistics using the **trajectory_evaluation_with_llm** metric.\n",
        "\n",
        "### 4. Analysis\n",
        "- The notebook displays a thorough trajectory evaluation assessment, including:\n",
        "  - **Quality Score** (0.0 to 1.0)\n",
        "  - **LLM-based Evaluation Details** explaining trajectory quality\n",
        "  - Assessment of tool usage, reasoning, and response effectiveness\n",
        "\n",
        "## Purpose and Benefits\n",
        "\n",
        "This evaluation provides **actionable quality metrics** that help developers:\n",
        "\n",
        "- Identify trajectory quality issues in the Intelligent Research Assistant  \n",
        "- Optimize tool usage decisions, particularly web search query formulation  \n",
        "- Track quality trends over time  \n",
        "- Ensure the Intelligent Research Assistant delivers **accurate and comprehensive research results** via proper API integration"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
