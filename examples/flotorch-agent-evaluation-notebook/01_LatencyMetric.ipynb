{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "VMvamBGBm4l9",
      "metadata": {
        "id": "VMvamBGBm4l9"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/1IrwoNrb3AWLAhAqjlAkJNYa39p9eT9ui?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0202c9",
      "metadata": {
        "id": "5e0202c9"
      },
      "source": [
        "# Flotorch Agent Latency Evaluation (Flotorch-Assistant Use Case)\n",
        "\n",
        "This notebook demonstrates how to measure and analyze the **agent latency** of a **Flotorch ADK agent** (configured as a **Flotorch-Assistant** that provides context-aware responses using Flotorch documentation via RAG) using the **Flotorch Eval** framework.\n",
        "\n",
        "The evaluation relies on **OpenTelemetry Traces** generated during the agent's run to provide a detailed breakdown of execution time across all steps (LLM calls, tool execution, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "* **Flotorch-Assistant**: An agent designed to provide context-aware responses using Flotorch documentation via RAG (Retrieval-Augmented Generation).\n",
        "* **OpenTelemetry Traces**: Detailed records of the agent's execution steps (spans) used to measure time taken by different operations.\n",
        "* **LatencyMetric**: A Flotorch Eval metric that extracts and summarizes latency information from the execution traces (agent trajectories). The evaluation metric used is **latency_summary**.\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/01_LatencyMetric_Workflow_Diagram.drawio.png)\n",
        "*Figure : Detailed workflow diagram showing the step-by-step process of latency evaluation from agent execution through trace collection to metric computation.*\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models and a knowledge base containing Flotorch documentation.\n",
        "* Valid Flotorch API key and gateway base URL.\n",
        "* Agent configured with OpenTelemetry tracing enabled.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d054681c",
      "metadata": {},
      "source": [
        "## Agent Setup in Flotorch Console\n",
        "\n",
        "**Important**: Before running this notebook, you need to create an agent in the Flotorch Console. This section provides step-by-step instructions on how to set up the agent.\n",
        "\n",
        "### Step 1: Access Flotorch Console\n",
        "\n",
        "1. **Log in to Flotorch Console**:\n",
        "   - Navigate to your Flotorch Console (e.g., `https://dev-console.flotorch.cloud`)\n",
        "   - Ensure you have the necessary permissions to create agents\n",
        "\n",
        "2. **Navigate to Agents Section**:\n",
        "   - Click on **\"Agents\"** in the left sidebar\n",
        "   - You should see the \"Agent Builder\" option selected\n",
        "\n",
        "### Step 2: Create New Agent\n",
        "\n",
        "1. **Click \"Create FloTorch Agent\"**:\n",
        "   - Look for the blue **\"+ Create FloTorch Agent\"** button in the top right corner\n",
        "   - Click it to start creating a new agent\n",
        "\n",
        "2. **Agent Configuration**:\n",
        "   - **Agent Name**: Choose a unique name for your agent (e.g., `flotorch-agent`)\n",
        "     - **Important**: The name should only contain alphanumeric characters and dashes (a-z, A-Z, 0-9, -)\n",
        "     - **Note**: Copy this agent name - you'll need to use it in the `agent_name` variable later\n",
        "   - **Description** (Optional): Add a description if desired\n",
        "\n",
        "### Step 3: Configure Agent Details\n",
        "\n",
        "After creating the agent, you'll be directed to the agent configuration page. Configure the following:\n",
        "\n",
        "#### Required Configuration:\n",
        "\n",
        "1. **Model** (`* Model`):\n",
        "   - **Required**: Select a model from the available options\n",
        "   - Example: `gpt-model` or any available model from your Flotorch gateway\n",
        "   - Click the edit icon to configure\n",
        "\n",
        "2. **Agent Details** (`* Agent Details`):\n",
        "   - **Required**: Configure agent details\n",
        "   - **System Prompt**: Copy and paste the following system prompt:\n",
        "\n",
        "You are the Flotorch Support Agent. Your purpose is to assist users with questions about Flotorch by providing accurate, context-aware answers grounded strictly in official Flotorch documentation.\n",
        "\n",
        "Use the kb_tool to look up any information you are not fully certain about. Always query the knowledge base before answering if clarification, validation, or exact details are needed.\n",
        "\n",
        "Guidelines:\n",
        "Base every response on retrieved knowledge from kb_tool.\n",
        "If multiple document fragments are returned, synthesize them into a single clear answer.\n",
        "Never invent features, APIs, capabilities, or behaviors not found in the retrieved documentation.\n",
        "If the knowledge base lacks information, say so clearly and provide a safe, general-purpose suggestion.\n",
        "Provide concise, helpful explanations with step-by-step details when relevant.\n",
        "Maintain a professional, technical-support tone.\n",
        "\n",
        "Your outputs should always reflect the most accurate Flotorch information available via the kb_tool\n",
        "\n",
        "\n",
        "   - **Goal**: Copy and paste the following goal:\n",
        "   \n",
        "To deliver precise, context-aware technical support for Flotorch by retrieving and grounding all answers in the Flotorch knowledge base using kb_tool, ensuring correctness, clarity, and factual alignment with documentation.\n",
        "\n",
        "\n",
        "#### Optional Configuration:\n",
        "\n",
        "1. **Tools**:\n",
        "   - Tools will be added programmatically via the notebook (see Section 8)\n",
        "   - You can leave this as \"Not Configured\" in the console\n",
        "\n",
        "2. **Input Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "3. **Output Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "### Step 4: Publish the Agent\n",
        "\n",
        "1. **Review Configuration**:\n",
        "   - Ensure the Model and Agent Details are configured correctly\n",
        "   - Verify the System Prompt and Goal are set\n",
        "\n",
        "2. **Publish Agent**:\n",
        "   - After configuration, click **\"Publish\"** or **\"Make a revision\"** to publish the agent\n",
        "   - Once published, the agent will have a version number (e.g., v1)\n",
        "\n",
        "3. **Note the Agent Name**:\n",
        "   - **Important**: Copy the exact agent name you used when creating the agent\n",
        "   - You will need to replace `<your_agent_name>` in the `agent_name` variable in Section 2.1 (Global Provider Models and Agent Configuration)\n",
        "\n",
        "### Step 5: Update Notebook Configuration\n",
        "\n",
        "1. **Update Agent Name**:\n",
        "   - Navigate to Section 2.1 in this notebook\n",
        "   - Find the `agent_name` variable\n",
        "   - Replace `<your_agent_name>` with the exact agent name you created in the console\n",
        "\n",
        "**Example**:\n",
        "- If you created an agent named `flotorch-agent` in the console\n",
        "- Set `agent_name = \"flotorch-agent\"` in the notebook\n",
        "\n",
        "### Summary of Required vs Optional Settings\n",
        "\n",
        "| Setting | Required/Optional | Value |\n",
        "|---------|------------------|-------|\n",
        "| **Agent Name** | **Required** | Choose a unique name (copy it for notebook) |\n",
        "| **Model** | **Required** | Select from available models |\n",
        "| **System Prompt** | **Required** | Use the system prompt provided above |\n",
        "| **Goal** | **Required** | Use the goal provided above |\n",
        "| **Tools** | **Optional** | Will be added via notebook code |\n",
        "| **Input Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "| **Output Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "\n",
        "**Note**: The tools (Knowledge Base, Web Search, Weather, News) will be added to the agent programmatically in the notebook code, so you don't need to configure them manually in the console.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "511a5ffe",
      "metadata": {
        "id": "511a5ffe"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for agent latency evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies for latency metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd32dd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1f9565",
      "metadata": {
        "id": "1f1f9565"
      },
      "source": [
        "## 2.Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29373306",
      "metadata": {
        "id": "29373306"
      },
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  \n",
        "    print(f\"✓ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"✗ FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")   # https://dev-gateway.flotorch.cloud\n",
        "print(f\"✓ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"✓ All credentials configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kJPaDWzhwu3I",
      "metadata": {
        "id": "kJPaDWzhwu3I"
      },
      "source": [
        "### 2.1. Global Provider Models and Agent Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure agent-specific parameters.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation and agent operations:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Agent Configuration Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `KNOWLEDGE_BASE` | The ID of your Flotorch Knowledge Base | `knowledge-base` |\n",
        "| `agent_name` | The name of your Flotorch ADK agent | `flotorch-agent` |\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n",
        "| `app_name` | The application name identifier | `agent-evaluation-app-name_01` |\n",
        "| `user_id` | The user identifier | `agent-evaliation-user-01` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MKnqcT0Lp3gB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKnqcT0Lp3gB",
        "outputId": "1e367789-4a46-4646-ee04-216412e7d3f2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation, agent operations, and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"✓ Global provider models defined\")\n",
        "\n",
        "KNOWLEDGE_BASE = \"<your_knowledge_base_id>\"  # The ID of your Flotorch Knowledge Base                          || ex : knowledge-base\n",
        "agent_name = \"<your_agent_name>\"  # The name of your Flotorch ADK agent                                        || ex : flotorch-agent\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU  # The LLM model used for evaluation. Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "app_name = \"<your_app_name>\"  # The application name identifier                                                || ex : agent-evaluation-app-name_01\n",
        "user_id = \"<your_user_id>\"  # The user identifier                                                              || ex : agent-evaliation-user-01\n",
        "\n",
        "print(\"✓ Agent Configuration Parameter defined \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99963dc0",
      "metadata": {
        "id": "99963dc0"
      },
      "source": [
        "## 3. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating the Flotorch Support Agent latency using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`LatencyMetric`**: Flotorch Eval metric that extracts and summarizes latency information from execution traces\n",
        "- **`FlotorchADKAgent`**: Creates and configures Flotorch ADK agents with custom tools and tracing\n",
        "- **`FlotorchADKSession`**: Manages agent sessions for multi-turn conversations\n",
        "- **`FlotorchVectorStore`**: Connects to the Flotorch knowledge base for RAG-based context retrieval\n",
        "- **`extract_vectorstore_texts`**: Utility helper for extracting text from vector-store search results\n",
        "- **`Runner`**: Executes agent queries and coordinates the agent execution flow\n",
        "- **`FunctionTool`**: Wraps Python functions as tools that can be used by the agent\n",
        "- **`types`**: Google ADK types for creating message content and handling agent events\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5345ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5345ba",
        "outputId": "8b2d406c-8a00-4b9f-b370-191b01da8c19"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.latency_metrics import LatencyMetric\n",
        "\n",
        "# Flotorch ADK components\n",
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "from flotorch.sdk.memory import FlotorchVectorStore\n",
        "from flotorch.sdk.utils.memory_utils import extract_vectorstore_texts\n",
        "\n",
        "# Google ADK components\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.genai import types\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"✓ Imported necessary libraries successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d236c2",
      "metadata": {
        "id": "33d236c2"
      },
      "source": [
        "## 4. Flotorch Support Agent Setup\n",
        "\n",
        "### Purpose\n",
        "Set up the Flotorch Support Agent with OpenTelemetry tracing enabled to capture detailed execution data for latency evaluation.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Initializes the agent for providing context-aware responses using Flotorch documentation via RAG\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Essential for evaluation as traces contain complete latency information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "\n",
        "These components work together to run the Flotorch Support Agent and generate OpenTelemetry traces for latency analysis.\n",
        "\n",
        "### Custom Tool: RAG-based Knowledge Base Search\n",
        "\n",
        "The Flotorch Support Agent uses a custom tool (`kb_tool`) that integrates with the Flotorch knowledge base to retrieve relevant documentation context. This tool:\n",
        "- Accepts a query string as input\n",
        "- Performs semantic search in the vectorstore using FlotorchVectorStore\n",
        "- Extracts and returns relevant text chunks from Flotorch documentation\n",
        "- Handles errors gracefully with structured error responses\n",
        "\n",
        "The tool is wrapped as a `FunctionTool` that can be used by the agent to provide context-aware responses using Flotorch documentation via RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882c2118",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "882c2118",
        "outputId": "3127240a-84a6-4cff-d349-56736f00870a"
      },
      "outputs": [],
      "source": [
        "# Initialize the Flotorch Vector Store (Knowledge Base)\n",
        "kb = FlotorchVectorStore(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    vectorstore_id=KNOWLEDGE_BASE\n",
        ")\n",
        "\n",
        "def kb_tool(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Custom tool for searching the Flotorch knowledge base.\n",
        "    Performs semantic search and returns relevant text chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not query:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"results\": [],\n",
        "                \"error\": \"Empty query provided.\"\n",
        "            }\n",
        "\n",
        "        # Perform semantic search in vectorstore\n",
        "        context = kb.search(query)\n",
        "        results = extract_vectorstore_texts(context)\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"results\": results,\n",
        "            \"error\": None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"results\": [],\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "# Register the custom tool\n",
        "tools = [FunctionTool(kb_tool)]\n",
        "\n",
        "print(\"✓ Knowledge base tool setup completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c42916b",
      "metadata": {
        "id": "6c42916b"
      },
      "source": [
        "## 5. Agent and Runner Initialization\n",
        "\n",
        "### Purpose\n",
        "Set up the Flotorch ADK Agent and Runner with OpenTelemetry tracing enabled to capture detailed latency data for evaluation.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Initializes the agent with custom RAG tools\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Essential for latency measurement as traces contain timing information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "\n",
        "These components work together to run the Flotorch Support Agent and generate OpenTelemetry traces for latency analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ea47ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ea47ea",
        "outputId": "9feef86e-bbff-4357-ee79-9b84fc08d27f"
      },
      "outputs": [],
      "source": [
        "# Initialize Flotorch ADK Agent with tracing enabled\n",
        "agent_client = FlotorchADKAgent(\n",
        "    agent_name=agent_name,\n",
        "    custom_tools=tools,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    tracer_config={\n",
        "        \"enabled\": True,                                                   # Enable tracing for latency measurement\n",
        "        \"endpoint\": \"https://dev-observability.flotorch.cloud/v1/traces\",  # Dev observability OTLP HTTP endpoint (used by QA)\n",
        "        \"sampling_rate\": 1                                                 # Sample 100% of traces\n",
        "    }\n",
        ")\n",
        "agent = agent_client.get_agent()\n",
        "\n",
        "# Initialize session service\n",
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        ")\n",
        "\n",
        "# Create the ADK Runner to execute agent queries\n",
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=app_name,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "print(\"✓ Agent and runner and session setup completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed6bc1e",
      "metadata": {
        "id": "7ed6bc1e"
      },
      "source": [
        "## 6. Helper Function for Running a Query\n",
        "\n",
        "### Purpose\n",
        "Define a helper function that executes a single-turn query with the agent and extracts the final response. The agent execution is automatically traced for latency measurement.\n",
        "\n",
        "### Functionality\n",
        "The `run_single_turn` function:\n",
        "- Accepts a `Runner`, query string, session ID, and user ID as parameters\n",
        "- Creates a user message using Google ADK types\n",
        "- Executes the query through the runner\n",
        "- Iterates through events to find and return the final agent response\n",
        "- Returns a fallback message if no response is found\n",
        "\n",
        "This function simplifies the process of running queries and ensures trace generation during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51ef384",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51ef384",
        "outputId": "2116514d-5e66-41a0-99c4-fea0d4b1bad3"
      },
      "outputs": [],
      "source": [
        "def run_single_turn(runner: Runner, query: str, session_id: str, user_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute a single-turn query with the agent and return the final response.\n",
        "    The agent execution is traced automatically.\n",
        "    \"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Extract the final response\n",
        "    for event in events:\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            return event.content.parts[0].text\n",
        "    return \"No response from agent.\"\n",
        "\n",
        "print(\"✓ Helper function 'run_single_turn' defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2PvllyC5xljY",
      "metadata": {
        "id": "2PvllyC5xljY"
      },
      "source": [
        "## 7. Define Query\n",
        "\n",
        "### Purpose\n",
        "Define the sample query that will be executed by the Flotorch Support Agent to generate OpenTelemetry traces for latency evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`query`**: A sample question that will be processed by the agent\n",
        "  - This query will trigger the agent to use RAG (knowledge base search) and generate a response\n",
        "  - The execution will be automatically traced to capture latency information\n",
        "  - Example: \"How to create agents in flotorch console?\"\n",
        "\n",
        "The query can be modified to test different scenarios and measure latency for various types of questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-OaLR5izxjN2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OaLR5izxjN2",
        "outputId": "fc21abe0-a4f3-47a7-dee8-4572f367b55a"
      },
      "outputs": [],
      "source": [
        "# Execute the query to generate traces\n",
        "\n",
        "query = \"How to create agents in flotorch console?\"\n",
        "\n",
        "print(f\"✓ Query defined: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5742fdf",
      "metadata": {
        "id": "c5742fdf"
      },
      "source": [
        "## 8. Run the Query and Get Trace ID\n",
        "\n",
        "### Purpose\n",
        "Execute a sample query with the Flotorch Support Agent to generate OpenTelemetry traces that contain latency data for evaluation.\n",
        "\n",
        "### Process\n",
        "1. **Create Session**: Initialize a new session for the agent interaction\n",
        "2. **Execute Query**: Run a sample query (\"How to create agents in flotorch console?\") through the agent\n",
        "3. **Retrieve Trace IDs**: Extract the generated trace IDs from the agent client\n",
        "4. **Display Results**: Print the agent response and trace ID for verification\n",
        "\n",
        "The execution automatically generates OpenTelemetry traces that record timing information for all agent operations (LLM calls, tool executions, etc.), which will be used for latency evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98ad20c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98ad20c",
        "outputId": "ebe609d6-0ccb-4464-b27e-31f9203057ed"
      },
      "outputs": [],
      "source": [
        "# Create a new session\n",
        "session = await runner.session_service.create_session(\n",
        "    app_name=app_name,\n",
        "    user_id=user_id,\n",
        ")\n",
        "print(f\"Session created: {session.id}\")\n",
        "\n",
        "response = run_single_turn(\n",
        "    runner=runner,\n",
        "    query=query,\n",
        "    session_id=session.id,\n",
        "    user_id=user_id\n",
        ")\n",
        "\n",
        "# Retrieve the generated trace IDs\n",
        "trace_ids = agent_client.get_tracer_ids()\n",
        "print(\"Agent Response:\")\n",
        "print(response[:200] + \"...\" if len(response) > 200 else response)\n",
        "print(f\"Found {len(trace_ids)} trace(s). First trace ID: {trace_ids[0] if trace_ids else 'N/A'}\")\n",
        "\n",
        "print(f\"✓ Trace IDs retrieved: {len(trace_ids)} trace(s) generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812a9b9a",
      "metadata": {
        "id": "812a9b9a"
      },
      "source": [
        "## 9. Latency Evaluation with Flotorch Eval\n",
        "\n",
        "### Purpose\n",
        "Initialize the `AgentEvaluator`, fetch the OpenTelemetry trace, and run the `LatencyMetric` to compute the performance breakdown. The evaluation metric **latency_summary** provides detailed latency statistics for the Flotorch Support Agent.\n",
        "\n",
        "### Key Components\n",
        "1. **LatencyMetric**: Initializes the latency metric that will analyze trace data\n",
        "2. **AgentEvaluator** (`client`):\n",
        "   - Connects to the Flotorch gateway using API credentials\n",
        "   - Configured with a default evaluator model\n",
        "   - Provides methods to fetch and evaluate traces\n",
        "3. **Trace Fetching**: Retrieves the complete trace data using the trace ID generated during agent execution\n",
        "\n",
        "The fetched trace contains detailed timing information for all agent operations, which will be analyzed by the LatencyMetric to compute the latency_summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c963e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c963e4",
        "outputId": "be3b3005-dbba-4fd8-f7ed-c858307fe4a7"
      },
      "outputs": [],
      "source": [
        "# Initialize the LatencyMetric\n",
        "metrics = [LatencyMetric()]\n",
        "\n",
        "# Initialize the AgentEvaluator client\n",
        "client = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "print(\"✓ AgentEvaluator client initialized successfully\")\n",
        "\n",
        "traces = None\n",
        "if trace_ids:\n",
        "    # Fetch the trace data from the Flotorch gateway\n",
        "    traces = client.fetch_traces(trace_ids[0])\n",
        "else:\n",
        "    print(\"✗ No trace IDs found to fetch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218e8814",
      "metadata": {
        "id": "218e8814"
      },
      "source": [
        "## 10. Run Evaluation\n",
        "\n",
        "### Purpose\n",
        "Execute the latency evaluation by processing the fetched OpenTelemetry trace using the LatencyMetric to compute performance breakdown.\n",
        "\n",
        "### Process\n",
        "- Calls `client.evaluate()` with the trace data and LatencyMetric\n",
        "- The evaluator processes the trace to extract timing information for all agent steps\n",
        "- Computes the **latency_summary** metric which includes:\n",
        "  - Total latency for root steps\n",
        "  - Average step latency\n",
        "  - Detailed breakdown by operation type (LLM calls, tool execution, etc.)\n",
        "- Returns evaluation results with trajectory ID and metric scores\n",
        "\n",
        "This step generates the latency analysis that will be displayed in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d832fc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d832fc0",
        "outputId": "e28c1895-dc9e-4c85-ca9f-1620dd134b50"
      },
      "outputs": [],
      "source": [
        "if 'traces' in locals() and traces:\n",
        "    # Evaluate the trace using the LatencyMetric\n",
        "    results = await client.evaluate(\n",
        "        trace=traces,\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    print(\"✓ Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"Cannot evaluate: No traces were available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c436137c",
      "metadata": {
        "id": "c436137c"
      },
      "source": [
        "## 11. Display and Interpret Results\n",
        "\n",
        "### Purpose\n",
        "Define helper functions to format and display the evaluation output clearly, showing the latency_summary metric results in a readable format.\n",
        "### Functionality\n",
        "The `display_metrics` function:\n",
        "- Extracts the `latency_summary` metric from evaluation results\n",
        "- Formats the latency breakdown with step names and timing in milliseconds\n",
        "- Creates a structured display showing:\n",
        "  - Total Latency (Root Steps)\n",
        "  - Average Root Step Latency\n",
        "  - Detailed latency breakdown for each operation\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize latency performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a11561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a11561",
        "outputId": "e036f016-30b9-4c8f-9275-e51d029973b0"
      },
      "outputs": [],
      "source": [
        "def display_metrics(result):\n",
        "    \"\"\"\n",
        "    Display latency metrics in a formatted table, including total and step breakdown.\n",
        "    \"\"\"\n",
        "    # Find the latency_summary metric\n",
        "    metric = next((m for m in result.scores if m.name == \"latency_summary\"), None)\n",
        "    if not metric:\n",
        "        print(\"No latency_summary metric found.\")\n",
        "        return\n",
        "\n",
        "    # Extract metric details\n",
        "    d = metric.details\n",
        "\n",
        "    # Build the latency breakdown string\n",
        "    breakdown = \"\\n\".join(\n",
        "        f\"    - {s['step_name']}: {s['latency_ms']} ms\"\n",
        "        for s in d.get(\"latency_breakdown\", [])\n",
        "    )\n",
        "\n",
        "    # Format the details string\n",
        "    details = (\n",
        "        f\"Total Latency (Root Steps): {d.get('total_latency_ms')} ms\\n\"\n",
        "        f\"Average Root Step Latency: {d.get('average_step_latency_ms')} ms\\n\"\n",
        "        f\"Latency Breakdown:\\n{breakdown}\"\n",
        "    )\n",
        "\n",
        "    # Create DataFrame for display\n",
        "    df = pd.DataFrame([{\n",
        "        \"Metric\": metric.name,\n",
        "        \"Score\": f\"{metric.score:.2f}\",\n",
        "        \"Details\": details\n",
        "    }])\n",
        "\n",
        "    # Display DataFrame with multiline support\n",
        "    display(df.style.set_properties(\n",
        "        subset=['Details'],\n",
        "        **{'white-space': 'pre-wrap', 'text-align': 'left'}\n",
        "    ))\n",
        "\n",
        "\n",
        "print(\"✓ Display metrics function defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27de8dc3",
      "metadata": {
        "id": "27de8dc3"
      },
      "source": [
        "## 12. View Latency Results\n",
        "\n",
        "### Purpose\n",
        "Display the latency evaluation results in a formatted table showing the complete performance breakdown for the Flotorch Support Agent.\n",
        "\n",
        "### Output\n",
        "The displayed table includes:\n",
        "- **Metric**: The evaluation metric name (latency_summary)\n",
        "- **Score**: The overall latency score\n",
        "- **Details**: Comprehensive breakdown showing:\n",
        "  - Total Latency (Root Steps) in milliseconds\n",
        "  - Average Root Step Latency in milliseconds\n",
        "  - Individual step latencies (e.g., call_llm, execute_tool)\n",
        "\n",
        "This visualization helps identify performance bottlenecks and optimize the agent's response time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d055ba70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d055ba70",
        "outputId": "f559040b-aff5-400b-f65d-f15584d52624"
      },
      "outputs": [],
      "source": [
        "if 'results' in locals():\n",
        "    display_metrics(results)\n",
        "else:\n",
        "    print(\"No results object found. Please run sections 5 and 6 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3052630",
      "metadata": {
        "id": "f3052630"
      },
      "source": [
        "### Interpreting the Latency Breakdown\n",
        "\n",
        "The **latency_summary** metric is a vital tool for performance monitoring of the Flotorch Support Agent:\n",
        "\n",
        "* **Total Latency (Root Steps)**: The cumulative execution time for all root-level operations in milliseconds. This represents the end-to-end time from query initiation to final response.\n",
        "* **Average Root Step Latency**: The mean execution time across all root steps, providing a baseline for expected performance.\n",
        "* **Latency Breakdown**: Detailed timing information for individual operations:\n",
        "    * **invocation**: Total time for the agent invocation cycle\n",
        "    * **invoke_agent**: Time spent in the agent execution framework\n",
        "    * **call_llm**: Time taken for LLM API calls (can include multiple calls in a single turn)\n",
        "    * **execute_tool**: Time spent executing custom tools (e.g., RAG knowledge base search)\n",
        "    * Other operation-specific latencies as captured in the trace\n",
        "\n",
        "For a Flotorch Support Agent, understanding latency breakdown helps identify:\n",
        "- **LLM call bottlenecks**: If `call_llm` latencies are high, consider model optimization or caching strategies\n",
        "- **RAG search performance**: If `execute_tool` (knowledge base search) is slow, optimize vector store queries or indexing\n",
        "- **Overall response time**: Monitor total latency to ensure the agent delivers fast, context-aware responses using Flotorch documentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c3b6be",
      "metadata": {
        "id": "a3c3b6be"
      },
      "source": [
        "## 13. Summary of Agent Latency Evaluation Notebook\n",
        "\n",
        "This notebook demonstrates the professional methodology for evaluating the latency performance of a **Flotorch ADK Agent** (configured as a **Flotorch-Assistant use case** that provides context-aware responses using Flotorch documentation via RAG) using the **Flotorch Eval framework**.\n",
        "\n",
        "**Use Case**: Flotorch-Assistant  - Provides context-aware responses using Flotorch documentation via RAG.\n",
        "\n",
        "**Evaluation Metric**: latency_summary\n",
        "\n",
        "## Core Process\n",
        "\n",
        "### 1. Setup and Instrumentation\n",
        "- Configure a `FlotorchADKAgent` with custom RAG tools (e.g., a knowledge base search function for Flotorch documentation).\n",
        "- Enable **OpenTelemetry Tracing** via the `tracer_config`.\n",
        "- This instrumentation allows detailed capture of timing and internal operations.\n",
        "\n",
        "### 2. Execution and Data Generation\n",
        "- Run a sample query through the agent using the **Runner**.\n",
        "- This automatically generates an **Agent Trajectory** in the form of OpenTelemetry traces.\n",
        "- The trace records the timing of each component, including:\n",
        "  - LLM calls\n",
        "  - RAG tool executions (knowledge base search)\n",
        "  - Step-by-step agent operations\n",
        "\n",
        "### 3. Evaluation\n",
        "- Use the `AgentEvaluator` client along with the specialized **LatencyMetric** from `flotorch-eval`.\n",
        "- The evaluator processes the trace data to compute latency statistics using the **latency_summary** metric.\n",
        "\n",
        "### 4. Analysis\n",
        "- The notebook displays a thorough latency breakdown, including:\n",
        "  - **Total Latency (Root Steps)**\n",
        "  - **Average Root Step Latency**\n",
        "  - Timing for individual steps such as:\n",
        "    - `invocation`\n",
        "    - `call_llm`\n",
        "    - `execute_tool` (RAG search operations)\n",
        "\n",
        "## Purpose and Benefits\n",
        "\n",
        "This evaluation provides **actionable performance metrics** that help developers:\n",
        "\n",
        "- Identify latency bottlenecks in the Flotorch Support Agent  \n",
        "- Optimize slow operations, particularly RAG search performance  \n",
        "- Track performance trends over time  \n",
        "- Ensure the Flotorch Support Agent delivers **fast and reliable context-aware responses** using Flotorch documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27202ae9",
      "metadata": {
        "id": "27202ae9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
