{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NasFVbVegs7x"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/drive/folders/1IrwoNrb3AWLAhAqjlAkJNYa39p9eT9ui?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlQIMDZpgs7z"
      },
      "source": [
        "# Flotorch Agent Evaluation - Three Metrics Combined\n",
        "\n",
        "This notebook demonstrates how to measure and analyze **three key agent evaluation metrics** for **Flotorch ADK agents** using the **Flotorch Eval** framework.\n",
        "\n",
        "The evaluation relies on **OpenTelemetry Traces** generated during the agent's run to provide insights into agent performance.\n",
        "\n",
        "**Key Feature**: This notebook uses **ONE agent** and **ONE AgentEvaluator** to evaluate three metrics, making it efficient and easy to use.\n",
        "\n",
        "---\n",
        "\n",
        "## Metrics Covered\n",
        "\n",
        "This notebook includes evaluation for the following metrics:\n",
        "\n",
        "1. **Trajectory Evaluation with LLM** (`trajectory_evaluation_with_llm`) - Assesses overall trajectory quality using LLM-based evaluation\n",
        "2. **Trajectory Evaluation with Reference** (`trajectory_evaluation_with_reference`) - Compares trajectory against a reference trajectory\n",
        "3. **Tool Call Accuracy** (`toolcall_accuracy`) - Evaluates accuracy and appropriateness of tool usage decisions\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "![Workflow Diagram](diagrams/08_Three-metrics-evaluation-agent_Workflow_Diagram.drawio.png)\n",
        "*Figure 2: Detailed workflow diagram showing the step-by-step process of three-metrics evaluation from agent execution through trace collection to multiple metric computations.*\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Flotorch account with configured models\n",
        "* Valid Flotorch API key and gateway base URL\n",
        "* Agent configured with OpenTelemetry tracing enabled\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekpe-7-sgs70"
      },
      "source": [
        "## Agent Setup in Flotorch Console\n",
        "\n",
        "**Important**: Before running this notebook, you need to create an agent in the Flotorch Console. This section provides step-by-step instructions on how to set up the agent.\n",
        "\n",
        "### Step 1: Access Flotorch Console\n",
        "\n",
        "1. **Log in to Flotorch Console**:\n",
        "   - Navigate to your Flotorch Console (e.g., `https://dev-console.flotorch.cloud`)\n",
        "   - Ensure you have the necessary permissions to create agents\n",
        "\n",
        "2. **Navigate to Agents Section**:\n",
        "   - Click on **\"Agents\"** in the left sidebar\n",
        "   - You should see the \"Agent Builder\" option selected\n",
        "\n",
        "### Step 2: Create New Agent\n",
        "\n",
        "1. **Click \"Create FloTorch Agent\"**:\n",
        "   - Look for the blue **\"+ Create FloTorch Agent\"** button in the top right corner\n",
        "   - Click it to start creating a new agent\n",
        "\n",
        "2. **Agent Configuration**:\n",
        "   - **Agent Name**: Choose a unique name for your agent (e.g., `three-metrics-evaluation-agent`)\n",
        "     - **Important**: The name should only contain alphanumeric characters and dashes (a-z, A-Z, 0-9, -)\n",
        "     - **Note**: Copy this agent name - you'll need to use it in the `agent_name` variable later\n",
        "   - **Description** (Optional): Add a description if desired\n",
        "\n",
        "### Step 3: Configure Agent Details\n",
        "\n",
        "After creating the agent, you'll be directed to the agent configuration page. Configure the following:\n",
        "\n",
        "#### Required Configuration:\n",
        "\n",
        "1. **Model** (`* Model`):\n",
        "   - **Required**: Select a model from the available options\n",
        "   - Example: `gpt-model` or any available model from your Flotorch gateway\n",
        "   - Click the edit icon to configure\n",
        "\n",
        "2. **Agent Details** (`* Agent Details`):\n",
        "   - **Required**: Configure agent details\n",
        "   - **System Prompt**: Copy and paste the following system prompt:\n",
        "\n",
        "You are an intelligent assistant agent designed to help users with various tasks including:\n",
        "\n",
        "1. Web Research: Search the internet for information on topics, technologies, products, or any subject matter and provide comprehensive, well-synthesized answers.\n",
        "\n",
        "2. Weather Information: Retrieve and provide current weather data for any city worldwide, including temperature, wind speed, humidity, and location details.\n",
        "\n",
        "3. News Updates: Fetch the latest news articles from around the world, with the ability to filter by region or topic, especially focusing on India and global news.\n",
        "\n",
        "\n",
        "Your Approach:\n",
        "- Always use the appropriate tools for each task\n",
        "- When searching for information, use the web_search tool to gather comprehensive data\n",
        "- When asked about weather, use the get_weather tool with the city name\n",
        "- When requested for news, use the get_top_news tool\n",
        "- For code review tasks, analyze the code thoroughly and provide detailed feedback on functionality, style, and improvements\n",
        "\n",
        "Communication Style:\n",
        "- Be clear, concise, and helpful\n",
        "- Provide accurate information based on tool results\n",
        "- When reviewing code, be constructive and educational\n",
        "- Synthesize information from multiple sources when applicable\n",
        "- Always acknowledge when you're using tools to gather information\n",
        "\n",
        "Quality Standards:\n",
        "- Verify information accuracy before presenting\n",
        "- Provide context and explanations where helpful\n",
        "- Format responses in a clear, readable manner\n",
        "- Cite sources when possible (especially for news articles)\n",
        "\n",
        "   - **Goal**: Copy and paste the following goal:\n",
        "\n",
        "To be a versatile, reliable assistant that efficiently handles diverse user requests including web research, weather queries, news updates, and code analysis. The agent should intelligently select and use appropriate tools (web_search, get_weather, get_top_news) based on user needs, provide accurate and well-structured responses, and offer helpful insights for code review tasks. The agent must maintain high quality in information synthesis, accuracy, and user communication across all supported task types.\n",
        "\n",
        "#### Optional Configuration:\n",
        "\n",
        "1. **Tools**:\n",
        "   - Tools will be added programmatically via the notebook (see Section 8)\n",
        "   - You can leave this as \"Not Configured\" in the console\n",
        "\n",
        "2. **Input Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "3. **Output Schema**:\n",
        "   - Optional: Leave as \"Not Configured\" for this use case\n",
        "\n",
        "### Step 4: Publish the Agent\n",
        "\n",
        "1. **Review Configuration**:\n",
        "   - Ensure the Model and Agent Details are configured correctly\n",
        "   - Verify the System Prompt and Goal are set\n",
        "\n",
        "2. **Publish Agent**:\n",
        "   - After configuration, click **\"Publish\"** or **\"Make a revision\"** to publish the agent\n",
        "   - Once published, the agent will have a version number (e.g., v1)\n",
        "\n",
        "3. **Note the Agent Name**:\n",
        "   - **Important**: Copy the exact agent name you used when creating the agent\n",
        "   - You will need to replace `<your_agent_name>` in the `agent_name` variable in Section 2.1 (Global Provider Models and Agent Configuration)\n",
        "\n",
        "### Step 5: Update Notebook Configuration\n",
        "\n",
        "1. **Update Agent Name**:\n",
        "   - Navigate to Section 2.1 in this notebook\n",
        "   - Find the `agent_name` variable\n",
        "   - Replace `<your_agent_name>` with the exact agent name you created in the console\n",
        "\n",
        "**Example**:\n",
        "- If you created an agent named `three-metrics-evaluation-agent` in the console\n",
        "- Set `agent_name = \"four-metrics-evaluation-agent\"` in the notebook\n",
        "\n",
        "### Summary of Required vs Optional Settings\n",
        "\n",
        "| Setting | Required/Optional | Value |\n",
        "|---------|------------------|-------|\n",
        "| **Agent Name** | **Required** | Choose a unique name (copy it for notebook) |\n",
        "| **Model** | **Required** | Select from available models |\n",
        "| **System Prompt** | **Required** | Use the system prompt provided above |\n",
        "| **Goal** | **Required** | Use the goal provided above |\n",
        "| **Tools** | **Optional** | Will be added via notebook code |\n",
        "| **Input Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "| **Output Schema** | **Optional** | Can leave as \"Not Configured\" |\n",
        "\n",
        "**Note**: The tools (Knowledge Base, Web Search, Weather, News) will be added to the agent programmatically in the notebook code, so you don't need to configure them manually in the console.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9PJJEWGgs70"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Purpose\n",
        "Install the necessary packages for the Flotorch Evaluation framework required for agent evaluation metrics.\n",
        "\n",
        "### Key Components\n",
        "- **`flotorch-eval`**: Flotorch evaluation framework with all dependencies\n",
        "- **`flotorch[adk]`**: Flotorch ADK (Agent Development Kit) package with all dependencies for building and running Flotorch agents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGBX7Bygs71"
      },
      "outputs": [],
      "source": [
        "# Install Flotorch Eval packages\n",
        "# flotorch-eval: Flotorch evaluation framework with all dependencies\n",
        "# flotorch[adk]: Flotorch ADK package with all dependencies\n",
        "\n",
        "%pip install flotorch-eval==2.0.0b1 flotorch[adk]==3.1.0b1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGDlOoDCgs72"
      },
      "source": [
        "## 2. Authentication and Credentials\n",
        "\n",
        "### Purpose\n",
        "Configure your Flotorch API credentials and gateway URL for authentication.\n",
        "\n",
        "### Key Components\n",
        "This cell configures the essential authentication and connection parameters:\n",
        "\n",
        "**Authentication Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `FLOTORCH_API_KEY` | Your API authentication key (found in your Flotorch Console). Securely entered using `getpass` to avoid displaying in the notebook | `sk_...` |\n",
        "| `FLOTORCH_BASE_URL` | Your Flotorch gateway endpoint URL | `https://dev-console.flotorch.cloud` |\n",
        "\n",
        "**Note**: Use secure credential management in production environments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQR0rBWIgs73"
      },
      "outputs": [],
      "source": [
        "import getpass  # Securely prompt without echoing in Prefect/notebooks\n",
        "\n",
        "# authentication for Flotorch access\n",
        "try:\n",
        "    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  \n",
        "    print(f\"âœ“ FLOTORCH_API_KEY set successfully\")\n",
        "except getpass.GetPassWarning as e:\n",
        "    print(f\"Warning: {e}\")\n",
        "    FLOTORCH_API_KEY = \"\"\n",
        "    print(f\"âœ— FLOTORCH_API_KEY not set\")\n",
        "\n",
        "FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint          || https://dev-console.flotorch.cloud\n",
        "print(f\"âœ“ FLOTORCH_BASE_URL set: {FLOTORCH_BASE_URL}\")\n",
        "\n",
        "print(\"âœ“ All credentials configured successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzwTHIkAgs73"
      },
      "source": [
        "### 2.1. Global Provider Models and Agent Configuration\n",
        "\n",
        "### Purpose\n",
        "Define available models from the Flotorch gateway and configure agent-specific parameters.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Global Provider Models**: These are the available models from the Flotorch gateway that can be used for evaluation and agent operations:\n",
        "\n",
        "| Model Variable | Model Name | Description |\n",
        "|----------------|------------|-------------|\n",
        "| `MODEL_CLAUDE_HAIKU` | `flotorch/flotorch-claude-haiku-4-5` | Claude Haiku model via Flotorch gateway |\n",
        "| `MODEL_CLAUDE_SONNET` | `flotorch/flotorch-claude-sonnet-3-5-v2` | Claude Sonnet model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_PRO` | `flotorch/flotorch-aws-nova-pro` | AWS Nova Pro model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_LITE` | `flotorch/flotorch-aws-nova-lite` | AWS Nova Lite model via Flotorch gateway |\n",
        "| `MODEL_AWS_NOVA_MICRO` | `flotorch/flotorch-aws-nova-micro` | AWS Nova Micro model via Flotorch gateway |\n",
        "\n",
        "**Agent Configuration Parameters**:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| `default_evaluator` | The LLM model used for evaluation (can use MODEL_* variables above) | `MODEL_CLAUDE_SONNET` or `flotorch/flotorch-model` |\n",
        "| `agent_name` | The name of your Flotorch ADK agent | `flotorch-agent` |\n",
        "| `app_name` | The application name identifier | `agent-evaluation-app-name` |\n",
        "| `user_id` | The user identifier | `agent-evaluation-user` |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvtZavR6gs74"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Global Provider Models (Flotorch Gateway Models)\n",
        "# ============================================================================\n",
        "# These models are available from the Flotorch gateway and can be used\n",
        "# for evaluation, agent operations, and other tasks.\n",
        "\n",
        "MODEL_CLAUDE_HAIKU = \"flotorch/flotorch-claude-haiku-4-5\"\n",
        "MODEL_CLAUDE_SONNET = \"flotorch/flotorch-claude-sonnet-3-5-v2\"\n",
        "MODEL_AWS_NOVA_PRO = \"flotorch/flotorch-aws-nova-pro\"\n",
        "MODEL_AWS_NOVA_LITE = \"flotorch/flotorch-aws-nova-lite\"\n",
        "MODEL_AWS_NOVA_MICRO = \"flotorch/flotorch-aws-nova-micro\"\n",
        "\n",
        "print(\"âœ“ Global provider models defined\")\n",
        "\n",
        "# The LLM model used for evaluation.\n",
        "# Can be modified to use any MODEL_* constant above (e.g., MODEL_CLAUDE_SONNET, MODEL_AWS_NOVA_PRO)\n",
        "# You can use your own models from Flotorch Console as well\n",
        "default_evaluator = MODEL_CLAUDE_HAIKU\n",
        "\n",
        "# Agent configuration - can be customized per metric section\n",
        "agent_name = \"<your_agent_name>\"  # The name of your Flotorch ADK agent                                        || ex : four-metrics-evaluation-agent\n",
        "app_name = \"<your_app_name>\"  # The application name identifier                                                || ex : agent-evaluation-app-name\n",
        "user_id = \"<your_user_id>\"  # The user identifier                                                              || ex : agent-evaluation-user\n",
        "\n",
        "print(\"âœ“ Agent Configuration Parameter defined \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVS0KRLags74"
      },
      "outputs": [],
      "source": [
        "FLOTORCH_API_KEY = \"sk_KV9aENzBx0kVYzkfBmq10k61I91t29f+SvGuhQIHgbE=_MmU0MjBkZWYtNjdlYy00YWRmLTliMDktNDYyMjFhNTRhMzk5_MDFjZjJhYjctN2IwYS00M2FiLWE0MGEtMmNjZDAyM2ZmMzRk\"\n",
        "FLOTORCH_BASE_URL = \"https://dev-gateway.flotorch.cloud\"\n",
        "agent_name = \"four-metrics-evaluation-agent\"\n",
        "app_name = \"app_name_98\"\n",
        "user_id = \"user_id_983\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnvl7kx2gs75"
      },
      "source": [
        "## 3. Import Required Libraries\n",
        "\n",
        "### Purpose\n",
        "Import all required components for evaluating Flotorch ADK agents with three metrics using Flotorch Eval.\n",
        "\n",
        "### Key Components\n",
        "- **`AgentEvaluator`**: Core client for agent evaluation orchestration and trace fetching\n",
        "- **`TrajectoryEvalWithLLM`**: Flotorch Eval metric that assesses overall trajectory quality using LLM-based evaluation\n",
        "- **`TrajectoryEvalWithLLMWithReference`**: Flotorch Eval metric that compares trajectory against a reference trajectory using LLM-based evaluation\n",
        "- **`ToolCallAccuracy`**: Flotorch Eval metric that evaluates the accuracy and appropriateness of tool usage decisions\n",
        "- **`ReferenceTrajectory`**: Schema for defining the ideal \"golden path\" trajectory for comparison in TrajectoryEvalWithLLMWithReference\n",
        "- **`MetricConfig`**: Configuration class for customizing metric parameters and settings\n",
        "- **`FlotorchADKAgent`**: Creates and configures Flotorch ADK agents with custom tools and tracing\n",
        "- **`FlotorchADKSession`**: Manages agent sessions for multi-turn conversations\n",
        "- **`Runner`**: Executes agent queries and coordinates the agent execution flow\n",
        "- **`FunctionTool`**: Wraps Python functions as tools that can be used by the agent\n",
        "- **`types`**: Google ADK types for creating message content and handling agent events\n",
        "- **`pandas`**: Data manipulation and display for formatted results tables\n",
        "- **`display`**: IPython display utility for rendering formatted outputs in notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkJhddtrgs75"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "# Flotorch Eval components\n",
        "from flotorch_eval.agent_eval.core.client import AgentEvaluator\n",
        "from flotorch_eval.agent_eval.metrics.llm_evaluators import (\n",
        "    TrajectoryEvalWithLLM,\n",
        "    TrajectoryEvalWithLLMWithReference,\n",
        "    ToolCallAccuracy\n",
        ")\n",
        "\n",
        "from flotorch_eval.agent_eval.core.schemas import ReferenceTrajectory\n",
        "from flotorch_eval import MetricConfig\n",
        "\n",
        "# Flotorch ADK components\n",
        "from flotorch.adk.agent import FlotorchADKAgent\n",
        "from flotorch.adk.sessions import FlotorchADKSession\n",
        "\n",
        "# Google ADK components\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.tools import FunctionTool\n",
        "from google.genai import types\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"âœ“ Imported necessary libraries successfully\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUlgap5ngs75"
      },
      "source": [
        "## 4. Web Search Tool\n",
        "\n",
        "**Purpose**: This tool performs Google searches and returns top results. It's used for Trajectory Evaluation metrics to enable the agent to search for information.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a search query as input\n",
        "- Uses Google Custom Search API to retrieve results\n",
        "- Returns formatted search results with titles, snippets, and links\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI_MInHNgs75"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 1: Web Search Tool (for Trajectory Evaluation)\n",
        "# ============================================================================\n",
        "import requests\n",
        "\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Perform a Google search and return top results.\"\"\"\n",
        "    api_key = \"AIzaSyA_jSk0x7mubtDToisZjmPC9mC2n5Nvf80\"\n",
        "    cse_id = \"77ef93ba660804b07\"\n",
        "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\"key\": api_key, \"cx\": cse_id, \"q\": query, \"num\": 5}\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    if \"items\" not in data:\n",
        "        return \"No results found.\"\n",
        "    results = []\n",
        "    for item in data[\"items\"]:\n",
        "        title = item.get(\"title\", \"\")\n",
        "        snippet = item.get(\"snippet\", \"\")\n",
        "        link = item.get(\"link\", \"\")\n",
        "        results.append(f\"ðŸ”¹ {title}\\n{snippet}\\n{link}\")\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "print(\"âœ“ Web Search Tool (for Trajectory Evaluation)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PASyF2Yvgs75"
      },
      "source": [
        "## 5. Web Search Tool (for Trajectory Evaluation)\n",
        "\n",
        "**Purpose**: This tool performs Google searches and returns top results. It's used for Trajectory Evaluation metrics to enable the agent to search for information.\n",
        "\n",
        "**Functionality**:\n",
        "- Takes a search query as input\n",
        "- Uses Google Custom Search API to retrieve results\n",
        "- Returns formatted search results with titles, snippets, and links\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHxhVlggs75"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Tool 2: Weather Tool (for Tool Call Accuracy)\n",
        "# ============================================================================\n",
        "def get_weather(city_name: str) -> dict:\n",
        "    \"\"\"Return latitude, longitude, and current weather for a given city name.\"\"\"\n",
        "    geo_url = \"https://geocoding-api.open-meteo.com/v1/search\"\n",
        "    geo_params = {\"name\": city_name, \"count\": 1, \"language\": \"en\", \"format\": \"json\"}\n",
        "    geo_res = requests.get(geo_url, params=geo_params).json()\n",
        "    if \"results\" not in geo_res:\n",
        "        raise ValueError(f\"City '{city_name}' not found\")\n",
        "    city = geo_res[\"results\"][0]\n",
        "    lat, lon = city[\"latitude\"], city[\"longitude\"]\n",
        "    weather_url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    weather_params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"current\": \"temperature_2m,wind_speed_10m,relative_humidity_2m\"\n",
        "    }\n",
        "    weather_res = requests.get(weather_url, params=weather_params).json()\n",
        "    current_weather = weather_res.get(\"current\", {})\n",
        "    return {\n",
        "        \"city\": city[\"name\"],\n",
        "        \"country\": city.get(\"country\"),\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"weather\": current_weather\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# Combine All Tools\n",
        "# ============================================================================\n",
        "all_tools = [\n",
        "    FunctionTool(web_search),\n",
        "    FunctionTool(get_weather)\n",
        "]\n",
        "\n",
        "print(\"âœ“ Weather Tool (for Tool Call Accuracy)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd99mn7Xgs75"
      },
      "source": [
        "## 6. Agent, Session, and Evaluator Initialization\n",
        "\n",
        "### Purpose\n",
        "Initialize all the necessary components to run and evaluate the Flotorch ADK agent with OpenTelemetry tracing enabled.\n",
        "\n",
        "### Key Components\n",
        "1. **FlotorchADKAgent** (`agent_client`):\n",
        "   - Creates the agent client with custom tools (Web Search, Weather)\n",
        "   - Configures `tracer_config` with `enabled: True` and `sampling_rate: 1` to capture 100% of traces\n",
        "   - Sets tracing endpoint: `\"https://dev-observability.flotorch.cloud/v1/traces\"`\n",
        "   - Essential for evaluation as traces contain complete execution information\n",
        "2. **FlotorchADKSession** (`session_service`): Manages agent sessions for multi-turn conversations\n",
        "3. **Runner** (`runner`): Executes agent queries and coordinates the agent execution flow\n",
        "4. **AgentEvaluator** (`evaluator`):\n",
        "   - Initializes the evaluation client for metric assessment\n",
        "   - Configures the default evaluator model (LLM) for metric evaluation\n",
        "   - Used to evaluate agent performance across all three metrics\n",
        "\n",
        "These components work together to run the Flotorch ADK Agent and generate OpenTelemetry traces for evaluation across all three metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGv9-nQlgs76"
      },
      "outputs": [],
      "source": [
        "# Initialize ONE Flotorch ADK Agent with all tools\n",
        "agent_client = FlotorchADKAgent(\n",
        "    agent_name=agent_name,\n",
        "    custom_tools=all_tools,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    tracer_config={\n",
        "        \"enabled\": True,  # Enable tracing for metrics\n",
        "        \"endpoint\": \"https://dev-observability.flotorch.cloud/v1/traces\",\n",
        "        \"sampling_rate\": 1  # Sample 100% of traces\n",
        "    }\n",
        ")\n",
        "agent = agent_client.get_agent()\n",
        "\n",
        "# Initialize session service\n",
        "session_service = FlotorchADKSession(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        ")\n",
        "\n",
        "# Create the ADK Runner to execute agent queries\n",
        "runner = Runner(\n",
        "    agent=agent,\n",
        "    app_name=app_name,\n",
        "    session_service=session_service\n",
        ")\n",
        "\n",
        "# Initialize ONE AgentEvaluator for both metrics\n",
        "evaluator = AgentEvaluator(\n",
        "    api_key=FLOTORCH_API_KEY,\n",
        "    base_url=FLOTORCH_BASE_URL,\n",
        "    default_evaluator=default_evaluator\n",
        ")\n",
        "\n",
        "print(\"âœ“ This agent and evaluator will be used for all metrics\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKR0lRCgs76"
      },
      "source": [
        "## 7. Helper Function for Running a Query\n",
        "\n",
        "### Purpose\n",
        "Define a helper function that executes a single-turn query with the agent and extracts the final response. The agent execution is automatically traced for evaluation.\n",
        "\n",
        "### Functionality\n",
        "The `run_single_turn` function:\n",
        "- Accepts a `Runner`, query string, session ID, and user ID as parameters\n",
        "- Creates a user message using Google ADK types (`types.Content` and `types.Part`)\n",
        "- Executes the query through the runner which coordinates the agent execution flow\n",
        "- Iterates through events returned by the runner to find and return the final agent response\n",
        "- Returns a fallback message (\"No response from agent.\") if no response is found\n",
        "- Automatically generates OpenTelemetry traces during execution for metric evaluation\n",
        "\n",
        "This function simplifies the process of running queries and ensures trace generation during execution, which is essential for evaluating the three metrics (TrajectoryEvalWithLLM, TrajectoryEvalWithLLMWithReference, and ToolCallAccuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3rwSrA2gs76"
      },
      "outputs": [],
      "source": [
        "def run_single_turn(runner: Runner, query: str, session_id: str, user_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute a single-turn query with the agent and return the final response.\n",
        "    The agent execution is traced automatically.\n",
        "    \"\"\"\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
        "    events = runner.run(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Extract the final response\n",
        "    for event in events:\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "            return event.content.parts[0].text\n",
        "    return \"No response from agent.\"\n",
        "\n",
        "print(\"âœ“ Helper function 'run_single_turn' defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXPlTKd8gs76"
      },
      "source": [
        "## 8. Define Queries for Different Metrics\n",
        "\n",
        "### Purpose\n",
        "Define sample queries for different metrics that will be executed by the Flotorch ADK agent to generate OpenTelemetry traces for evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`queries`**: A dictionary containing sample questions for each metric type\n",
        "  - **`\"trajectory\"`**: A query for TrajectoryEvalWithLLM metric that triggers the agent to search and provide information (e.g., \"Tell me about Google ADK\")\n",
        "  - **`\"reference\"`**: A query for TrajectoryEvalWithLLMWithReference metric that requires code review/analysis (e.g., \"Review this Python function and tell me if it works and how good the code quality is\")\n",
        "  - **`\"toolcall\"`**: A query for ToolCallAccuracy metric that requires the agent to use a specific tool (e.g., \"what is the weather in the hyderabad\")\n",
        "  - Each query will trigger agent execution that is automatically traced to capture execution information\n",
        "  - Different queries are used to ensure each metric is evaluated with the most appropriate scenario\n",
        "\n",
        "The queries can be modified to test different scenarios and measure performance for various types of questions. Each query generates a separate trace that is used for evaluating its corresponding metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY3AHgf2gs76"
      },
      "outputs": [],
      "source": [
        "# Define queries for different metrics\n",
        "queries = {\n",
        "    \"trajectory\": \"Tell me about Google ADK\",\n",
        "    \"reference\": \"\"\"Review this Python function and tell me if it works and how good the code quality is.\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\"\"\",\n",
        "    \"toolcall\": \"what is the weather in the hyderabad\"\n",
        "}\n",
        "\n",
        "print(f\"âœ“ Queries defined for {len(queries)} metrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPX9ym3ugs76"
      },
      "source": [
        "### 9. Run Queries and Get Trace IDs\n",
        "\n",
        "### Purpose\n",
        "Execute sample queries for different metrics to generate OpenTelemetry traces. All queries use the same agent and each query execution generates a separate trace ID that will be used for metric evaluation.\n",
        "\n",
        "### Functionality\n",
        "This section:\n",
        "- Iterates through each query in the `queries` dictionary\n",
        "- Executes each query using the agent via `run_single_turn` function\n",
        "- Collects the trace ID generated for each query execution\n",
        "- Stores trace IDs in a dictionary mapping metric names to their corresponding trace IDs\n",
        "- Verifies that all required trace IDs are collected successfully\n",
        "\n",
        "The trace IDs are essential for evaluation as they contain the complete execution information needed by each metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKFQIDy3gs76"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Store trace IDs for each metric\n",
        "trace_ids = {}\n",
        "\n",
        "# Run queries and collect trace IDs\n",
        "for metric_name, query in queries.items():\n",
        "    print(f\"\\n--- Running query for {metric_name.upper()} metric ---\")\n",
        "\n",
        "    # Create session and execute query\n",
        "    session = await runner.session_service.create_session(\n",
        "        app_name=app_name,\n",
        "        user_id=user_id\n",
        "    )\n",
        "    tracer_ids_before = set(agent_client.get_tracer_ids() or [])\n",
        "\n",
        "    response = run_single_turn(\n",
        "        runner=runner,\n",
        "        query=query,\n",
        "        session_id=session.id,\n",
        "        user_id=user_id\n",
        "    )\n",
        "\n",
        "    # Wait for trace generation and collect trace ID\n",
        "    time.sleep(4)\n",
        "    tracer_ids_after = agent_client.get_tracer_ids() or []\n",
        "    new_trace_ids = [tid for tid in tracer_ids_after if tid not in tracer_ids_before]\n",
        "\n",
        "    if new_trace_ids:\n",
        "        trace_ids[metric_name] = new_trace_ids[-1]\n",
        "        print(f\"âœ“ {metric_name.upper()} trace ID: {trace_ids[metric_name]}\")\n",
        "    else:\n",
        "        print(f\"âœ— No trace ID found for {metric_name}\")\n",
        "\n",
        "# Verify all required trace IDs collected\n",
        "required_keys = [\"trajectory\", \"reference\", \"toolcall\"]\n",
        "missing = [k for k in required_keys if k not in trace_ids]\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n WARNING: Missing trace IDs: {missing}\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ All {len(trace_ids)} trace IDs collected successfully!\")\n",
        "    for name, tid in trace_ids.items():\n",
        "        print(f\"  - {name}: {tid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T727Jrfmgs76"
      },
      "source": [
        "## 10. Reference Trajectory Setup :\n",
        " `(For TrajectoryEvalWithLLMWithReference)`\n",
        "\n",
        "### Key Components\n",
        "- **`REFERENCE_TRAJECTORY`**: A dictionary that defines the ideal trajectory for the agent\n",
        "  - **`input`**: The user's query or prompt that will trigger the agent's response\n",
        "  - **`expected_steps`**: A list of expected steps the agent should take, where each step contains:\n",
        "    - **`thought`**: The agent's reasoning or thought process at this step\n",
        "    - **`final_response`**: The expected final response from the agent\n",
        "  - Example: A code review query with expected analysis covering correctness, code quality, readability, performance, and suggestions\n",
        "\n",
        "- **`ReferenceTrajectory`**: Schema validation class that ensures the reference trajectory structure is correct\n",
        "  - Validates that all required fields are present and properly formatted\n",
        "  - Used to ensure the reference trajectory meets the requirements for TrajectoryEvalWithLLMWithReference metric\n",
        "\n",
        "This reference trajectory will be compared against the actual agent execution to evaluate how closely the agent follows the expected behavior and reasoning path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abBfyDXVgs76"
      },
      "outputs": [],
      "source": [
        "# Define reference trajectory for TrajectoryEvalWithLLMWithReference metric\n",
        "REFERENCE_TRAJECTORY = {\n",
        "    \"input\": \"\"\"Review this Python function and tell me if it works and how good the code quality is.\n",
        "\n",
        "def add_items(items):\n",
        "    total = 0\n",
        "    for i in range(len(items)):\n",
        "        total = total + items[i]\n",
        "    return total\"\"\",\n",
        "    \"expected_steps\": [\n",
        "        {\n",
        "            \"thought\": \"The user wants a review of the Python function, focusing on whether it works and the quality of the implementation. I will analyze correctness, edge cases, style, performance, and then provide a structured review.\",\n",
        "            \"final_response\": \"\"\"### Summary\n",
        "\n",
        "- The function `add_items` sums all elements in the `items` list.\n",
        "- It likely works correctly when `items` is a list of numeric values.\n",
        "- The implementation is not very idiomatic Python and can be simplified.\n",
        "- There is no validation for invalid or unexpected input types.\n",
        "\n",
        "### Correctness\n",
        "\n",
        "- For a typical case like `add_items([1, 2, 3])`, the function will return `6`, which is correct.\n",
        "- If `items` contains non-numeric values (e.g., strings or `None`), it will raise a `TypeError` when trying to add them.\n",
        "- If `items` is not indexable or does not implement `__len__` (e.g., a generator), the function will also fail.\n",
        "- There is no explicit handling of `None` or other invalid inputs.\n",
        "\n",
        "### Code Quality & Readability\n",
        "\n",
        "- The logic is straightforward and easy to understand.\n",
        "- Using `range(len(items))` is not idiomatic Python for iterating over a list.\n",
        "- A more Pythonic version would iterate directly over the elements or use the built-in `sum` function.\n",
        "- Variable naming is acceptable, but the function could be shorter and clearer.\n",
        "\n",
        "### Performance\n",
        "\n",
        "- Time complexity is O(n), where n is the length of `items`, which is optimal for summing a list.\n",
        "- There is a minor overhead from indexing (`items[i]`) instead of iterating directly over the elements.\n",
        "- For most real-world cases, this overhead is negligible, but the idiomatic version is both clearer and slightly more efficient.\n",
        "\n",
        "### Improved Version\n",
        "\n",
        "A more Pythonic and concise implementation would be:\n",
        "\n",
        "```python\n",
        "def add_items(items):\n",
        "    return sum(items)\n",
        "```\n",
        "\n",
        "Ratings\n",
        "Correctness: 8/10 (works for standard numeric lists, no validation)\n",
        "Code Quality: 6/10 (clear but non-idiomatic and slightly verbose)\n",
        "Overall: 7/10\"\"\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "validated_ref = ReferenceTrajectory(**REFERENCE_TRAJECTORY)\n",
        "print(\"âœ“ Reference trajectory defined for TrajectoryEvalWithLLMWithReference metric\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqKDp8wags76"
      },
      "source": [
        "## 11. Initialize All Metrics\n",
        "\n",
        "### Purpose\n",
        "Initialize all three metrics that will be evaluated. Each metric is configured with its specific requirements and will be evaluated using the traces collected from the agent execution.\n",
        "\n",
        "### Key Components\n",
        "- **`all_metrics`**: A list containing instances of all three metrics\n",
        "  - **`TrajectoryEvalWithLLM()`**: Metric that assesses overall trajectory quality using LLM-based evaluation\n",
        "  - **`TrajectoryEvalWithLLMWithReference()`**: Metric that compares trajectory against a reference trajectory using LLM-based evaluation\n",
        "    - Requires `default_evaluator` (LLM model) for evaluation\n",
        "    - Requires `MetricConfig` with `reference` parameter set to the `REFERENCE_TRAJECTORY` defined earlier\n",
        "  - **`ToolCallAccuracy()`**: Metric that evaluates the accuracy and appropriateness of tool usage decisions\n",
        "\n",
        "All three metrics are initialized together so they can be evaluated in a unified manner using the same evaluator and traces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfZCjL34gs77"
      },
      "outputs": [],
      "source": [
        "# Initialize all three metrics at once\n",
        "all_metrics = [\n",
        "    TrajectoryEvalWithLLM(),\n",
        "    TrajectoryEvalWithLLMWithReference(\n",
        "        llm=default_evaluator,\n",
        "        config=MetricConfig(\n",
        "            metric_params={\"reference\": REFERENCE_TRAJECTORY}\n",
        "        )\n",
        "    ),\n",
        "    ToolCallAccuracy()\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Initialized {len(all_metrics)} metrics:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU1IaEWogs77"
      },
      "source": [
        "## 12. Configure Metric-to-Trace Mapping\n",
        "\n",
        "### Purpose\n",
        "Define the mapping between each metric and its corresponding trace. Each metric uses a different trace (from different queries) to ensure accurate evaluation.\n",
        "\n",
        "### Key Components\n",
        "- **`metric_configs`**: A list of dictionaries, each mapping a metric to its trace\n",
        "  - **`name`**: Human-readable metric name\n",
        "  - **`trace_key`**: Key from the `trace_ids` dictionary (e.g., \"trajectory\", \"reference\", \"toolcall\")\n",
        "  - **`metric`**: The metric instance initialized earlier\n",
        "  - **`reference`**: Reference trajectory (only for TrajectoryEvalWithLLMWithReference, otherwise None)\n",
        "\n",
        "This mapping ensures each metric is evaluated with the most appropriate trace for accurate assessment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-MscH0Wgs77"
      },
      "outputs": [],
      "source": [
        "\n",
        "from flotorch_eval.agent_eval.core.schemas import EvaluationResult\n",
        "\n",
        "# Define metric-to-trace mapping\n",
        "metric_configs = [\n",
        "    {\n",
        "        \"name\": \"Trajectory\",\n",
        "        \"trace_key\": \"trajectory\",\n",
        "        \"metric\": TrajectoryEvalWithLLM(),\n",
        "        \"reference\": None\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Reference\",\n",
        "        \"trace_key\": \"reference\",\n",
        "        \"metric\": TrajectoryEvalWithLLMWithReference(\n",
        "            llm=default_evaluator,\n",
        "            config=MetricConfig(metric_params={\"reference\": REFERENCE_TRAJECTORY})\n",
        "        ),\n",
        "        \"reference\": REFERENCE_TRAJECTORY\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ToolCall\",\n",
        "        \"trace_key\": \"toolcall\",\n",
        "        \"metric\": ToolCallAccuracy(),\n",
        "        \"reference\": None\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Configured {len(metric_configs)} metric-to-trace mappings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COBMQ713gs77"
      },
      "source": [
        "## 13. Run Evaluation for All Metrics\n",
        "\n",
        "### Purpose\n",
        "Evaluate all three metrics, each with its own trace (different question). The `evaluate()` method accepts ONE trace per call, so we loop through each metric and evaluate it separately with its corresponding trace, then combine all results.\n",
        "\n",
        "### Functionality\n",
        "This section:\n",
        "- Iterates through each metric configuration\n",
        "- Fetches the trace for each metric using the trace ID collected earlier\n",
        "- Evaluates each metric with its corresponding trace\n",
        "- Adds reference trajectory for metrics that require it (TrajectoryEvalWithLLMWithReference)\n",
        "- Collects all scores and combines them into a single `EvaluationResult` object\n",
        "- Handles errors gracefully, allowing other metrics to continue if one fails\n",
        "\n",
        "The evaluation results are stored in `all_results` for display in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gVBjN-bgs77"
      },
      "outputs": [],
      "source": [
        "all_scores = []\n",
        "all_trajectory_ids = []\n",
        "\n",
        "# Evaluate each metric with its own trace\n",
        "for i, config in enumerate(metric_configs, 1):\n",
        "    trace_key = config[\"trace_key\"]\n",
        "    metric_name = config[\"name\"]\n",
        "\n",
        "    if trace_key not in trace_ids:\n",
        "        print(f\" Skipping {metric_name}: No trace ID for '{trace_key}'\")\n",
        "        continue\n",
        "\n",
        "    trace_id = trace_ids[trace_key]\n",
        "    print(f\"{i}. Evaluating {metric_name} Metric...\")\n",
        "\n",
        "    # Fetch and evaluate\n",
        "    trace = evaluator.fetch_traces(trace_id)\n",
        "    eval_kwargs = {\"trace\": trace, \"metrics\": [config[\"metric\"]]}\n",
        "\n",
        "    if config[\"reference\"]:\n",
        "        eval_kwargs[\"reference\"] = config[\"reference\"]\n",
        "\n",
        "    try:\n",
        "        result = await evaluator.evaluate(**eval_kwargs)\n",
        "        all_scores.extend(result.scores)\n",
        "        all_trajectory_ids.append((metric_name, result.trajectory_id))\n",
        "        print(f\"   âœ“ {metric_name} Metric completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âœ— {metric_name} Metric failed: {str(e)[:100]}\")\n",
        "\n",
        "# Create combined results\n",
        "if all_scores:\n",
        "    from datetime import datetime\n",
        "    all_results = EvaluationResult(\n",
        "        trajectory_id=all_trajectory_ids[0][1] if all_trajectory_ids else \"combined\",\n",
        "        timestamp=datetime.utcnow(),\n",
        "        scores=all_scores\n",
        "    )\n",
        "    print(f\"\\nâœ“ All {len(all_scores)} metrics evaluated successfully!\")\n",
        "else:\n",
        "    print(\"âœ— No metrics evaluated. Please check trace IDs.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_2m8AmHgs77"
      },
      "source": [
        "## 14. Display All Metrics Results\n",
        "\n",
        "### Purpose\n",
        "Define helper function to format and display the evaluation output clearly, showing all three metric results in a readable format.\n",
        "\n",
        "### Functionality\n",
        "The `display_all_metrics` function:\n",
        "- Extracts all three metrics from the evaluation results (`trajectory_evaluation`, `trajectory_evaluation_with_reference`, `toolcall_accuracy`)\n",
        "- Formats the score and details for each metric type appropriately\n",
        "- Creates a structured display showing:\n",
        "  - Metric name\n",
        "  - Score (0.0 to 1.0 scale)\n",
        "  - Detailed evaluation results for each metric\n",
        "- Uses pandas DataFrame with styled formatting for clean presentation\n",
        "\n",
        "This function provides a user-friendly way to visualize all three metric evaluation results in a single formatted table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN3gh8J3gs77"
      },
      "outputs": [],
      "source": [
        "def display_all_metrics(result):\n",
        "    \"\"\"\n",
        "    Display all three metrics in a single formatted table.\n",
        "    This function handles 3 metrics and formats them appropriately.\n",
        "    \"\"\"\n",
        "    if not result or not result.scores:\n",
        "        print(\"No metrics found in results.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Trajectory ID: {result.trajectory_id}\")\n",
        "    print(f\"Timestamp: {result.timestamp}\\n\")\n",
        "\n",
        "    # Prepare data for all metrics\n",
        "    metrics_data = []\n",
        "\n",
        "    for metric in result.scores:\n",
        "        metric_name = metric.name\n",
        "        details_text = \"\"\n",
        "\n",
        "        # Format details based on metric type\n",
        "        if metric_name == \"trajectory_evaluation\":\n",
        "            d = metric.details\n",
        "            details_text = f\"Score: {metric.score:.2f} / 1.0\\n\\nDetails:\\n{d.get('details', 'No details available.')}\"\n",
        "\n",
        "        elif metric_name == \"trajectory_evaluation_with_reference\":\n",
        "            d = metric.details\n",
        "            details_text = f\"Score: {metric.score:.2f} / 1.0\\n\\nDetails:\\n{d.get('details', 'No details available.')}\"\n",
        "\n",
        "        elif metric_name == \"toolcall_accuracy\":\n",
        "            d = metric.details\n",
        "            details_text = f\"Score: {metric.score:.2f} / 1.0\\n\\nDetails:\\n{d.get('details', 'No details available.')}\"\n",
        "\n",
        "        else:\n",
        "            # Generic formatting for unknown metrics\n",
        "            details_text = str(metric.details)\n",
        "\n",
        "        # Format score\n",
        "        score_display = f\"{metric.score:.2f}\"\n",
        "\n",
        "        metrics_data.append({\n",
        "            \"Metric\": metric_name.replace(\"_\", \" \").title(),\n",
        "            \"Score\": score_display,\n",
        "            \"Details\": details_text\n",
        "        })\n",
        "\n",
        "    # Create DataFrame and display\n",
        "    df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    display(df.style.set_properties(\n",
        "        subset=['Details'],\n",
        "        **{'white-space': 'pre-wrap', 'text-align': 'left'}\n",
        "    ).set_table_styles([\n",
        "        {'selector': 'th', 'props': [('text-align', 'left')]}\n",
        "    ]))\n",
        "\n",
        "# Display all metrics\n",
        "if 'all_results' in locals():\n",
        "    display_all_metrics(all_results)\n",
        "else:\n",
        "    print(\"No results found. Please run the evaluation section first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DBgJBUXgs77"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "This notebook demonstrates how to evaluate Flotorch ADK agents across **3 key evaluation metrics** using a **unified approach**:\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Single Agent**: ONE `FlotorchADKAgent` with necessary tools (Web Search, Weather)\n",
        "2. **Single Evaluator**: ONE `AgentEvaluator` instance for all metrics\n",
        "3. **Unified Evaluation**: All metrics evaluated at once in a single call\n",
        "4. **Unified Display**: ONE display function shows all metric results together\n",
        "\n",
        "## Metrics Evaluated\n",
        "\n",
        "1. **Trajectory Evaluation with LLM** (`trajectory_evaluation`) - Overall trajectory quality assessment\n",
        "2. **Trajectory Evaluation with LLM and Reference** (`trajectory_evaluation_with_reference`) - Compares trajectory against reference trajectory\n",
        "3. **Tool Call Accuracy** (`toolcall_accuracy`) - Tool usage decision evaluation\n",
        "\n",
        "\n",
        "## Benefits\n",
        "\n",
        "- **Comprehensive**: Covers trajectory quality, reference comparison, and tool accuracy\n",
        "- **Efficiency**: Single agent and evaluator reduce setup overhead\n",
        "- **Consistency**: All metrics use the same agent configuration\n",
        "- **Complete Insights**: Get full evaluation coverage with trajectory analysis and tool performance\n",
        "\n",
        "The notebook provides comprehensive evaluation capabilities for monitoring and optimizing Flotorch ADK agents.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
