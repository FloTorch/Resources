{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1QfQOVsm_tN40R15AYL1KRrJ0D8bmYJXz/view?usp=sharing)\n","# Guide to FlotorchLLM: Synchronous & Asynchronous Usage"],"metadata":{"id":"Jo_VInsqtx1t"},"id":"Jo_VInsqtx1t"},{"cell_type":"markdown","id":"zrzCiBmn1IOe","metadata":{"id":"zrzCiBmn1IOe"},"source":["\n","\n","This notebook demonstrates how to interact with Large Language Models (LLMs) through the Flotorch gateway using the `FlotorchLLM` client.\n","\n","You will learn how to structure requests, perform synchronous (`invoke`) and asynchronous (`ainvoke`) calls, and request strictly-validated JSON outputs.\n","\n","### Prerequisites\n","You will need an active Flotorch model and the corresponding credentials from the Flotorch Console (https://console.flotorch.cloud/).\n","\n","### Viewing Logs\n","Runtime logs for your model invocations are available in the Flotorch Console.\n","\n","### Key Objectives\n","- Initialize the `FlotorchLLM` client.\n","- Perform a basic `invoke()` call for text-to-text generation.\n","- Retrieve response headers to monitor cost, tokens, and latency.\n","- Enforce strictly-validated JSON output using `response_format`.\n","- Use `ainvoke()` for asynchronous operations."]},{"cell_type":"markdown","id":"Ilttz44P1IOj","metadata":{"id":"Ilttz44P1IOj"},"source":["## 1. Setup and Initialization\n","\n","First, let's install the required libraries and configure our credentials."]},{"cell_type":"code","execution_count":null,"id":"4618f9d6","metadata":{"id":"4618f9d6"},"outputs":[],"source":["%pip install flotorch[sdk]"]},{"cell_type":"markdown","id":"a9e6b6d2","metadata":{"id":"a9e6b6d2"},"source":["### Configure Credentials\n","\n","Replace the placeholder values in the cell below with your credentials from the Flotorch console."]},{"cell_type":"code","execution_count":null,"id":"55edfc92","metadata":{"id":"55edfc92"},"outputs":[],"source":["from getpass import getpass\n","\n","FLOTORCH_API_KEY = getpass(\"Enter your API Key: \")\n","FLOTORCH_BASE_URL = input(\"Enter Base URL: \")         # Example., https://gateway.flotorch.cloud\n","FLOTORCH_MODEL = \"<your_flotorch_model_id_here>\"\n","\n","print(\"Credentials and configuration set.\")"]},{"cell_type":"code","source":["# @title\n","from getpass import getpass\n","# FLOTORCH_API_KEY = \"sk_dYhYNVEkIkfBBYogHXP8L9ac5kctRzs810gFxcKPbM4=_MDMwNmI2NTYtMTZkOS00Njg5LThjN2MtNTg0MWRhMzE1Yjk1_NTg2MDBkMWEtZmFmMi00MTY2LTk1YTAtY2VjNjFlY2FkM2Q5\"\n","FLOTORCH_API_KEY = \"sk_xiHVC7JKWxyuzW7WkpuwqPCfkIcc5T3PSdiTkjhw748=_ZjczYmJiZWYtMDQ4MC00MzQxLWFiYWMtN2E2ZTIzNGU4MzBm_NmRkMzA0NWItZjU0NC00ZmVlLWI2YjItYmYxZTZlMGM5NDc3\"\n","FLOTORCH_BASE_URL = \"https://gateway.flotorch.cloud\"        # Example., https://gateway.flotorch.cloud\n","# FLOTORCH_MODEL = \"flotorch/flotorch-aws-nova-pro\"\n","# FLOTORCH_MODEL = \"flotorch/flotorch-aws-nova-micro\"     # Model used for scoring\n","FLOTORCH_MODEL = \"flotorch/flotorch-aws-nova-pro\"     # Model used for scoring\n","\n","print(\"Credentials and configuration set.\")"],"metadata":{"cellView":"form","id":"tdEiCM_v3k0N"},"id":"tdEiCM_v3k0N","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"qRSMIUWn1IOm","metadata":{"id":"qRSMIUWn1IOm"},"outputs":[],"source":["# Import necessary libraries\n","import json\n","from pydantic import BaseModel, Field\n","\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.utils.llm_utils import (\n","    # ResponseFormatSchema,\n","    convert_pydantic_to_custom_json_schema\n",")\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","source":["## Initialize the LLM Client\n","\n","This step creates the `FlotorchLLM` client using the configured model and credentials.  \n","The `llm` object will be used to send prompts and receive responses from the selected Flotorch model.\n"],"metadata":{"id":"e29GYkFovQ8-"},"id":"e29GYkFovQ8-"},{"cell_type":"code","execution_count":null,"id":"I_lcuul21IOm","metadata":{"id":"I_lcuul21IOm"},"outputs":[],"source":["# Initialize the LLM client\n","llm = FlotorchLLM(\n","    model_id = FLOTORCH_MODEL,\n","    api_key = FLOTORCH_API_KEY,\n","    base_url = FLOTORCH_BASE_URL\n",")\n","\n","print(f\"FlotorchLLM client initialized for model: {FLOTORCH_MODEL}\")"]},{"cell_type":"markdown","id":"b3f0c11f","metadata":{"id":"b3f0c11f"},"source":["## 2. Define Reusable Schemas and Helpers\n","\n","Before we make calls, let's define the Pydantic model we'll use for structured output and a helper function to print headers."]},{"cell_type":"code","execution_count":null,"id":"364b4f7d","metadata":{"id":"364b4f7d"},"outputs":[],"source":["class QuestionResponseModel(BaseModel):\n","    \"\"\"Structured schema for question and response pairs.\"\"\"\n","    question: str = Field(description=\"User's query\")\n","    response: str = Field(description=\"Final response returned to the user\")\n","\n","print(f\"Pydantic model '{QuestionResponseModel.__name__}' defined.\")"]},{"cell_type":"code","execution_count":null,"id":"b9c2f5d8","metadata":{"id":"b9c2f5d8"},"outputs":[],"source":["def print_flotorch_headers(headers: dict):\n","    \"\"\"Pretty-prints key headers from a Flotorch response.\"\"\"\n","\n","    header_labels = {\n","        \"x-request-id\": \"Request ID\",\n","        \"x-session-id\": \"Session ID\",\n","        \"x-total-tokens\": \"Total tokens\",\n","        \"x-input-tokens\": \"Input tokens\",\n","        \"x-completion-tokens\": \"Completion tokens\",\n","        \"x-total-cost\": \"Total cost ($)\",\n","        \"x-input-cost\": \"Input cost ($)\",\n","        \"x-completion-cost\": \"Completion cost ($)\",\n","        \"x-gateway-total-latency\": \"Gateway latency (ms)\",\n","        \"x-provider-latency\": \"Provider latency (ms)\",\n","    }\n","\n","    print(\"--- Key Response Headers ---\")\n","    for key, label in header_labels.items():\n","        value = headers.get(key)\n","        if value is not None:\n","            print(f\"  {label}: {value}\")\n","\n","print(\"Helper function 'print_flotorch_headers' defined.\")"]},{"cell_type":"markdown","id":"J1-I-6Yr1IOn","metadata":{"id":"J1-I-6Yr1IOn"},"source":["## 3. Synchronous Operations (`invoke`)\n","\n","Use the blocking `invoke()` method for sequential tasks. We will demonstrate three different ways to use this method."]},{"cell_type":"markdown","id":"ex1_markdown","metadata":{"id":"ex1_markdown"},"source":["### Example 1: Basic Invocation (Text-to-Text)\n","\n","This is the simplest use case: sending text and receiving text. No structured output is requested, and no headers are returned."]},{"cell_type":"code","execution_count":null,"id":"ex1_code","metadata":{"id":"ex1_code"},"outputs":[],"source":["messages = [\n","    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n","]\n","\n","# Make the basic call\n","response = llm.invoke(messages=messages)\n","\n","print(\"--- Basic LLM Response (String) ---\")\n","print(response.content)"]},{"cell_type":"markdown","id":"ex2_markdown","metadata":{"id":"ex2_markdown"},"source":["### Example 2: Invocation with Headers\n","\n","By adding `return_headers=True`, you receive a tuple: `(response, headers)`. This is useful for monitoring cost, tokens, and latency."]},{"cell_type":"code","execution_count":null,"id":"ex2_code","metadata":{"id":"ex2_code"},"outputs":[],"source":["messages = [\n","    {\"role\": \"user\", \"content\": \"what is the capital of india give it in one word?\"}\n","]\n","\n","# Make the call and request headers\n","response, headers = llm.invoke(\n","    messages=messages,\n","    return_headers=True\n",")\n","\n","print(\"--- LLM Response (String) ---\")\n","print(response.content)\n","\n","# Print the metadata\n","print_flotorch_headers(headers)"]},{"cell_type":"markdown","id":"ex3_markdown","metadata":{"id":"ex3_markdown"},"source":["### Example 3: Invocation with Structured Output (JSON)\n","\n","This is the most powerful feature. By providing a `response_format`, you force the LLM to return a JSON object that strictly matches your schema."]},{"cell_type":"markdown","id":"ex3_pydantic_markdown","metadata":{"id":"ex3_pydantic_markdown"},"source":["#### Method 1: Using Pydantic (Recommended)\n","\n","The easiest way to create a schema is to use the `convert_pydantic_to_custom_json_schema` utility with the `QuestionResponseModel` we defined earlier."]},{"cell_type":"code","execution_count":null,"id":"f5g1h2i3","metadata":{"id":"f5g1h2i3"},"outputs":[],"source":["# Step 1: Generate the schema from our Pydantic model\n","schema_wrapper = convert_pydantic_to_custom_json_schema(QuestionResponseModel)\n","# pydantic_response_format: ResponseFormatSchema = schema_wrapper[\"response_format\"]\n","pydantic_response_format = schema_wrapper[\"response_format\"]\n","\n","print(\"--- Generated ResponseFormatSchema ---\")\n","print(json.dumps(pydantic_response_format, indent=2))"]},{"cell_type":"code","execution_count":null,"id":"ex3_pydantic_code","metadata":{"id":"ex3_pydantic_code"},"outputs":[],"source":["# Step 2: Define a new prompt\n","messages = [\n","    {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n","]\n","\n","# Step 3: Make the call using the generated schema\n","response = llm.invoke(\n","    messages=messages,\n","    response_format=pydantic_response_format\n",")\n","\n","print(\"\\n--- Raw LLM Response (JSON String) ---\")\n","print(response.content)"]},{"cell_type":"code","execution_count":null,"id":"ex3_pydantic_validation","metadata":{"id":"ex3_pydantic_validation"},"outputs":[],"source":["# Step 4: Validate the JSON string into a Pydantic object\n","structured_output = QuestionResponseModel.model_validate_json(response.content)\n","\n","print(\"--- Parsed and Validated Pydantic Object ---\")\n","print(structured_output.model_dump_json(indent=2))"]},{"cell_type":"markdown","id":"ex3_manual_markdown","metadata":{"id":"ex3_manual_markdown"},"source":["#### Method 2: Manual JSON Schema (Alternative)\n","\n","You can also define the schema manually as a dictionary. This is more verbose but provides full control."]},{"cell_type":"code","execution_count":null,"id":"a2b0e9d4","metadata":{"id":"a2b0e9d4"},"outputs":[],"source":["# Step 1: Define the manual JSON schema\n","manual_json_schema = {\n","    \"type\": \"json_schema\",\n","    \"json_schema\": {\n","        \"name\": \"QuestionResponseJson\",\n","        \"strict\": True,\n","        \"schema\": {\n","            \"type\": \"object\",\n","            \"required\": [\"question\", \"response\"],\n","            \"properties\": {\n","                \"question\": {\n","                    \"type\": \"string\",\n","                    \"description\": \"User's query\"\n","                },\n","                \"response\": {\n","                    \"type\": \"string\",\n","                    \"description\": \"Final response returned to the user\"\n","                }\n","            },\n","            \"additionalProperties\": False\n","        }\n","    }\n","}\n","\n","print(\"Manual JSON schema defined.\")"]},{"cell_type":"code","execution_count":null,"id":"ex3_manual_code","metadata":{"id":"ex3_manual_code"},"outputs":[],"source":["# Step 2: Make the call with the manual schema\n","messages = [\n","    {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n","]\n","\n","response = llm.invoke(\n","    messages=messages,\n","    response_format=manual_json_schema\n",")\n","\n","print(\"--- Raw LLM Response (JSON String) ---\")\n","print(response.content)"]},{"cell_type":"markdown","id":"E2wCMjsr1IOo","metadata":{"id":"E2wCMjsr1IOo"},"source":["## 4. Asynchronous Operations (`ainvoke`)\n","\n","For concurrent applications (like a web backend), use the non-blocking `ainvoke()` method. It accepts the same arguments as `invoke`, including `response_format`."]},{"cell_type":"code","execution_count":null,"id":"KnkN_UTy1IOo","metadata":{"id":"KnkN_UTy1IOo"},"outputs":[],"source":["# We can re-use the 'pydantic_response_format' from the previous section\n","async def single_async_example():\n","    messages = [\n","        {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}\n","    ]\n","\n","    flotorch_response = await llm.ainvoke(\n","        messages=messages,\n","        response_format=pydantic_response_format  # Using the generated schema\n","    )\n","\n","    print(\"--- Async Response (Raw JSON) ---\")\n","    print(flotorch_response.content)\n","\n","    # Validate the output\n","    parsed = QuestionResponseModel.model_validate_json(flotorch_response.content)\n","    print(\"\\n--- Parsed Pydantic Object ---\")\n","    print(parsed.model_dump_json(indent=2))\n","\n","    print(f\"\\nTokens used: {flotorch_response.metadata.get('totalTokens', 'N/A')}\")\n","\n","\n","# Run the async function\n","await single_async_example()"]},{"cell_type":"markdown","id":"32s4DsJ11IOo","metadata":{"id":"32s4DsJ11IOo"},"source":["## 5. Summary\n","\n","This notebook demonstrated the key patterns for using `FlotorchLLM`, progressing from simple to advanced:\n","\n","- **Basic Invocation**: Use `invoke()` for simple text-to-text generation.\n","- **Monitoring**: Pass `return_headers=True` to get valuable metadata on cost, tokens, and latency.\n","- **Structured Outputs**: Pass a `response_format` to enforce strict, validated JSON output. The recommended method is to use Pydantic and the `convert_pydantic_to_custom_json_schema` utility.\n","- **Async Operations**: Use `ainvoke()` for non-blocking, concurrent applications."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}