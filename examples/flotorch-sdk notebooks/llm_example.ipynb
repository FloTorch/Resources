{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flotorch SDK - LLM User Guide\n",
        "\n",
        "This notebook provides a comprehensive guide for using the **FlotorchLLM** class from the Flotorch SDK. The FlotorchLLM class enables you to interact with Large Language Models through the Flotorch gateway with both synchronous and asynchronous operations.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Installation & Setup](#installation--setup)\n",
        "2. [Basic Usage](#basic-usage)\n",
        "3. [Synchronous Operations](#synchronous-operations)\n",
        "4. [Asynchronous Operations](#asynchronous-operations)\n",
        "5. [Advanced Parameters](#advanced-parameters)\n",
        "6. [Best Practices](#best-practices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation & Setup\n",
        "\n",
        "Before using the FlotorchLLM class, ensure you have the required environment variables configured:\n",
        "\n",
        "- `FLOTORCH_API_KEY`: Your Flotorch API key\n",
        "- `FLOTORCH_BASE_URL`: The base URL for your Flotorch gateway\n",
        "\n",
        "You can set these in a `.env` file or as environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Environment variables loaded successfully\n",
            "Base URL: https://qa-gateway.flotorch.cloud\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from dotenv import load_dotenv\n",
        "from flotorch.sdk.llm import FlotorchLLM\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify environment variables\n",
        "api_key = os.getenv(\"FLOTORCH_API_KEY\")\n",
        "base_url = os.getenv(\"FLOTORCH_BASE_URL\")\n",
        "\n",
        "if not api_key or not base_url:\n",
        "    raise ValueError(\"Please set FLOTORCH_API_KEY and FLOTORCH_BASE_URL environment variables\")\n",
        "\n",
        "print(\"✓ Environment variables loaded successfully\")\n",
        "print(f\"Base URL: {base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Usage\n",
        "\n",
        "The FlotorchLLM class is initialized with a model ID, API key, and base URL. Here's how to create an instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-03 22:16:40 - flotorch.sdk.llm - INFO - FlotorchLLM initialized (model_id=openai/gpt-4o-mini, base_url=https://qa-gateway.flotorch.cloud)\n",
            "✓ FlotorchLLM client initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize the LLM client\n",
        "llm = FlotorchLLM(\n",
        "    model_id=\"openai/gpt-4o-mini\",  # Replace with your actual model ID\n",
        "    api_key=api_key,\n",
        "    base_url=base_url\n",
        ")\n",
        "\n",
        "print(\"✓ FlotorchLLM client initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synchronous Operations\n",
        "\n",
        "Use the `invoke()` method for synchronous LLM calls. This is suitable for simple, sequential operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "Artificial intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (the acquisition of information and rules for using it), reasoning (the ability to solve problems through logical deduction), and self-correction. AI can be categorized into two main types:\n",
            "\n",
            "1. **Narrow AI (Weak AI)**: This type of AI is designed and trained for a specific task. Examples include virtual assistants like Siri and Alexa, recommendation systems on streaming services, and image recognition software. Narrow AI operates under a limited set of constraints and is not capable of generalizing its knowledge to other tasks.\n",
            "\n",
            "2. **General AI (Strong AI)**: This is a theoretical form of AI that possesses the ability to understand, learn, and apply intelligence across a wide range of tasks, similar to a human being. General AI would be able to reason, solve problems, and understand complex concepts in a way that is indistinguishable from human intelligence. As of now, general AI remains largely a concept and has not yet been realized.\n",
            "\n",
            "AI technologies encompass various subfields, including machine learning (where algorithms improve through experience), natural language processing (enabling machines to understand and generate human language), computer vision (allowing machines to interpret and process visual information), and robotics.\n",
            "\n",
            "AI has applications across numerous industries, including healthcare, finance, transportation, entertainment, and more, and continues to evolve rapidly, raising both opportunities and ethical considerations regarding its use.\n",
            "\n",
            "Tokens used: 306\n"
          ]
        }
      ],
      "source": [
        "# Simple single-turn conversation\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(response.content)\n",
        "print(f\"\\nTokens used: {response.metadata.get('totalTokens', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Asynchronous Operations\n",
        "\n",
        "Use the `ainvoke()` method for asynchronous operations. This is ideal for handling multiple requests concurrently or when integrating with async applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Async Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Async Response:\n",
            "Machine learning is a way for computers to learn from data and improve their performance on a task without being explicitly programmed for that task. \n",
            "\n",
            "Think of it like teaching a child. Instead of giving them a detailed set of instructions on how to recognize animals, you show them many pictures of cats and dogs. Over time, they learn to identify which is which based on the examples you've provided. \n",
            "\n",
            "In machine learning, we feed a computer lots of data (like pictures, text, or numbers), and it uses that data to find patterns and make predictions or decisions. For example, it might learn to recognize spam emails by analyzing many examples of both spam and non-spam messages. \n",
            "\n",
            "So, in short, machine learning is about teaching computers to learn from experience and improve over time!\n",
            "\n",
            "Tokens used: 170\n"
          ]
        }
      ],
      "source": [
        "async def single_async_example():\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}\n",
        "    ]\n",
        "    \n",
        "    response = await llm.ainvoke(messages)\n",
        "    \n",
        "    print(\"Async Response:\")\n",
        "    print(response.content)\n",
        "    print(f\"\\nTokens used: {response.metadata.get('totalTokens', 'N/A')}\")\n",
        "\n",
        "# Run the async function\n",
        "await single_async_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "### 1. **Environment Management**\n",
        "- Always use environment variables for API keys\n",
        "- Never hardcode sensitive information in your code\n",
        "\n",
        "### 2. **Async vs Sync**\n",
        "- Use **sync** (`invoke()`) for simple, sequential operations\n",
        "- Use **async** (`ainvoke()`) for concurrent operations or async applications\n",
        "\n",
        "### 3. **Parameter Optimization**\n",
        "- **Temperature**: 0.1-0.3 for focused responses, 0.7-0.9 for creative responses\n",
        "- **Max Tokens**: Set appropriate limits to control response length and costs\n",
        "\n",
        "### 4. **Error Handling**\n",
        "- Always wrap LLM calls in try-except blocks\n",
        "- Handle network timeouts and API rate limits gracefully\n",
        "\n",
        "### 5. **Token Management**\n",
        "- Monitor token usage through `response.metadata`\n",
        "- Keep track of costs and usage patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The FlotorchLLM class provides a powerful and flexible interface for working with Large Language Models:\n",
        "\n",
        "- **Simple initialization** with model ID, API key, and base URL\n",
        "- **Synchronous operations** using `invoke()` for straightforward use cases\n",
        "- **Asynchronous operations** using `ainvoke()` for better performance and concurrency\n",
        "- **Advanced parameter control** for fine-tuning model behavior\n",
        "- **Comprehensive metadata** for monitoring token usage and costs\n",
        "\n",
        "This SDK enables you to build robust LLM-powered applications with both simple and advanced use cases."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}