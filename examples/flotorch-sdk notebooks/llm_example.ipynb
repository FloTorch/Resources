{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zrzCiBmn1IOe",
   "metadata": {
    "id": "zrzCiBmn1IOe"
   },
   "source": [
    "# Guide to FlotorchLLM: Synchronous and Asynchronous Usage\n",
    "\n",
    "This notebook provides a clear and practical guide to using the `FlotorchLLM` class from the Flotorch SDK. It demonstrates how to interact with Large Language Models (LLMs) through the Flotorch gateway, covering both simple blocking (synchronous) calls and concurrent (asynchronous) calls.\n",
    "\n",
    "### Prerequesit\n",
    "Configure model, API key in Flotroch console (https://console.flotorch.cloud/)\n",
    "\n",
    "### Viewing logs\n",
    "Logs can be viewed in logs tab in Flotroch console (https://console.flotorch.cloud/)\n",
    "\n",
    "### Key Objectives:\n",
    "- Initialize the `FlotorchLLM` client.\n",
    "- Make a standard synchronous request using the `invoke()` method.\n",
    "- Make a non-blocking asynchronous request using the `ainvoke()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ilttz44P1IOj",
   "metadata": {
    "id": "Ilttz44P1IOj"
   },
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "The first step is to configure the environment and initialize the `FlotorchLLM` client. The following cells will load your `FLOTORCH_API_KEY` and `FLOTORCH_BASE_URL` from an environment file and then use them to create an LLM client instance connected to a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618f9d6",
   "metadata": {
    "id": "4618f9d6"
   },
   "outputs": [],
   "source": [
    "%pip install --pre flotorch[sdk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfc92",
   "metadata": {
    "id": "55edfc92"
   },
   "outputs": [],
   "source": [
    "FLOTORCH_API_KEY = \"<flotorch api key>\"\n",
    "FLOTORCH_BASE_URL = \"<flotroch gateway base url>\" # eg: https://gateway.flotorch.cloud\"\n",
    "FLOTORCH_MODEL = \"<flotorch model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qRSMIUWn1IOm",
   "metadata": {
    "id": "qRSMIUWn1IOm"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from flotorch.sdk.llm import FlotorchLLM\n",
    "\n",
    "print(\"Imported necessary libraries successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I_lcuul21IOm",
   "metadata": {
    "id": "I_lcuul21IOm"
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM client\n",
    "llm = FlotorchLLM(\n",
    "    model_id = FLOTORCH_MODEL,\n",
    "    api_key = FLOTORCH_API_KEY,\n",
    "    base_url = FLOTORCH_BASE_URL\n",
    ")\n",
    "\n",
    "print(\"FlotorchLLM client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J1-I-6Yr1IOn",
   "metadata": {
    "id": "J1-I-6Yr1IOn"
   },
   "source": [
    "## 2. Synchronous Operations\n",
    "\n",
    "For simple, sequential tasks, use the `invoke()` method. This is a **blocking** call, meaning your program will pause and wait for the LLM to complete its response before executing the next line of code. It's the most straightforward way to get a response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qCYDpkWP1IOn",
   "metadata": {
    "id": "qCYDpkWP1IOn"
   },
   "outputs": [],
   "source": [
    "# Simple single-turn conversation using a synchronous call\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.content)\n",
    "print(f\"\\nTokens used: {response.metadata.get('totalTokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E2wCMjsr1IOo",
   "metadata": {
    "id": "E2wCMjsr1IOo"
   },
   "source": [
    "## 3. Asynchronous Operations\n",
    "\n",
    "For applications requiring higher performance and concurrency (like web backends), use the `ainvoke()` method. This is a **non-blocking** call that must be used with the `await` keyword inside an `async` function. It allows your application to handle other tasks while waiting for the LLM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KnkN_UTy1IOo",
   "metadata": {
    "id": "KnkN_UTy1IOo"
   },
   "outputs": [],
   "source": [
    "async def single_async_example():\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}\n",
    "    ]\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "\n",
    "    print(\"Async Response:\")\n",
    "    print(response.content)\n",
    "    print(f\"\\nTokens used: {response.metadata.get('totalTokens', 'N/A')}\")\n",
    "\n",
    "# Run the async function\n",
    "await single_async_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32s4DsJ11IOo",
   "metadata": {
    "id": "32s4DsJ11IOo"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The `FlotorchLLM` class provides a robust and flexible interface for interacting with Large Language Models. This guide has demonstrated its core functionalities.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Initialization**: The client is easily initialized with your `model_id`, `api_key`, and `base_url`.\n",
    "- **Synchronous Usage**: Use `invoke()` for straightforward, blocking operations suitable for scripts and sequential tasks.\n",
    "- **Asynchronous Usage**: Use `ainvoke()` within an `async` function for high-performance, non-blocking operations ideal for concurrent applications.\n",
    "- **Metadata**: The response object contains valuable metadata, including token usage, which is essential for monitoring costs and performance.\n",
    "- **Best Practices**: For production code, always manage API keys securely using environment variables and wrap LLM calls in `try...except` blocks to handle potential network or API errors gracefully."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
